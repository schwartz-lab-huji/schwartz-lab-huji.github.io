[{"authors":["noah"],"categories":null,"content":"","date":1600041600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600041600,"objectID":"26229e118c8ed67fdf60140d2c027cf4","permalink":"/author/a-hrefhttp/homes.cs.washington.edu/~nasmith/-targetblanknoah-a.-smith/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/homes.cs.washington.edu/~nasmith/-targetblanknoah-a.-smith/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://homes.cs.washington.edu/~nasmith/' target='blank'\u003eNoah A. Smith\u003c/a\u003e","type":"authors"},{"authors":["yejin"],"categories":null,"content":"","date":1600041600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600041600,"objectID":"7ba101def5f150cbbd01c9959aedd8ec","permalink":"/author/a-hrefhttp/homes.cs.washington.edu/~yejin/-targetblankyejin-choi/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/homes.cs.washington.edu/~yejin/-targetblankyejin-choi/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://homes.cs.washington.edu/~yejin/' target='blank'\u003eYejin Choi\u003c/a\u003e","type":"authors"},{"authors":["swabha"],"categories":null,"content":"","date":1600041600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600041600,"objectID":"03408aa46022610095573e9111d359b2","permalink":"/author/a-hrefhttp/www.cs.cmu.edu/~sswayamd/-targetblankswabha-swayamdipta/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/www.cs.cmu.edu/~sswayamd/-targetblankswabha-swayamdipta/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://www.cs.cmu.edu/~sswayamd/' target='blank'\u003eSwabha Swayamdipta\u003c/a\u003e","type":"authors"},{"authors":["hannaneh"],"categories":null,"content":"","date":1600041600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600041600,"objectID":"3d30ca823fc3dad847c76e8f4b82f589","permalink":"/author/a-hrefhttps/homes.cs.washington.edu/~hannaneh/-targetblankhannaneh-hajishirzi/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/homes.cs.washington.edu/~hannaneh/-targetblankhannaneh-hajishirzi/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://homes.cs.washington.edu/~hannaneh/' target='blank'\u003eHannaneh Hajishirzi\u003c/a\u003e","type":"authors"},{"authors":["yizhong"],"categories":null,"content":"","date":1600041600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600041600,"objectID":"a03196afe27a59d9a6e6b6a9e15b68c6","permalink":"/author/a-hrefhttps/yizhong-wang.com/-targetblankyizhong-wang/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/yizhong-wang.com/-targetblankyizhong-wang/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://yizhong-wang.com/' target='blank'\u003eYizhong Wang\u003c/a\u003e","type":"authors"},{"authors":["me"],"categories":null,"content":"","date":1600041600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600041600,"objectID":"f8f9a3384bdb64736380a1b633f3701b","permalink":"/author/broy-schwartz/b/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/broy-schwartz/b/","section":"authors","summary":"","tags":null,"title":"\u003cb\u003eRoy Schwartz\u003c/b\u003e","type":"authors"},{"authors":["nicholas"],"categories":null,"content":"","date":1600041600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600041600,"objectID":"d6baee863e6a9befe871365b5a4886b4","permalink":"/author/nicholas-lourie/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/nicholas-lourie/","section":"authors","summary":"","tags":null,"title":"Nicholas Lourie","type":"authors"},{"authors":["aidastar"],"categories":null,"content":"","date":1592524800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1592524800,"objectID":"89628711a6421968b4a8cbe394cf31cc","permalink":"/author/a-hrefhttps/aidaamini.github.io/-targetblankaida-amini/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/aidaamini.github.io/-targetblankaida-amini/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://aidaamini.github.io/' target='blank'\u003eAida Amini\u003c/a\u003e*","type":"authors"},{"authors":["davidwadden"],"categories":null,"content":"","date":1592524800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1592524800,"objectID":"0345d6e594f8aa189a716c16f941d944","permalink":"/author/david-wadden/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/david-wadden/","section":"authors","summary":"","tags":null,"title":"David Wadden","type":"authors"},{"authors":["tomstar"],"categories":null,"content":"","date":1592524800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1592524800,"objectID":"62876c8b8bf56d332eefb5bff515fed1","permalink":"/author/tom-hope/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tom-hope/","section":"authors","summary":"","tags":null,"title":"Tom Hope*","type":"authors"},{"authors":["hao"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1585699200,"objectID":"216cc62ed48c13143e8a57e65b3b4bae","permalink":"/author/a-hrefhttp/homes.cs.washington.edu/~hapeng/-targetblanka-hrefhttp/homes.cs.washington.edu/~hapeng/hao-peng/a/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/homes.cs.washington.edu/~hapeng/-targetblanka-hrefhttp/homes.cs.washington.edu/~hapeng/hao-peng/a/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://homes.cs.washington.edu/~hapeng/' target='blank'\u003e\u003ca href='http://homes.cs.washington.edu/~hapeng/'\u003eHao Peng\u003c/a\u003e\u003c/a\u003e","type":"authors"},{"authors":["jesse"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1585699200,"objectID":"20b9fcb3e499237078dd28b702108e90","permalink":"/author/a-hrefhttp/www.cs.cmu.edu/~jessed/-targetblankjesse-dodge/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/www.cs.cmu.edu/~jessed/-targetblankjesse-dodge/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://www.cs.cmu.edu/~jessed/' target='blank'\u003eJesse Dodge\u003c/a\u003e","type":"authors"},{"authors":["erany"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1585699200,"objectID":"a3c3393543d270a8475ebbce22b95ebb","permalink":"/author/a-hrefhttp/www.cs.technion.ac.il/~yahave/-targetblankeran-yahav/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/www.cs.technion.ac.il/~yahave/-targetblankeran-yahav/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://www.cs.technion.ac.il/~yahave/' target='blank'\u003eEran Yahav\u003c/a\u003e","type":"authors"},{"authors":["gabi"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1585699200,"objectID":"712728ba8e850c1cb1ff551e8939735c","permalink":"/author/a-hrefhttps/gabrielstanovsky.github.io-targetblankgabi-stanovsky/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/gabrielstanovsky.github.io-targetblankgabi-stanovsky/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://gabrielstanovsky.github.io' target='blank'\u003eGabi Stanovsky\u003c/a\u003e","type":"authors"},{"authors":["will"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1585699200,"objectID":"7c4ddaa28ed01844e37fcd570383bed9","permalink":"/author/a-hrefhttps/lambdaviking.com-targetblankwill-merrill/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/lambdaviking.com-targetblankwill-merrill/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://lambdaviking.com' target='blank'\u003eWill Merrill\u003c/a\u003e","type":"authors"},{"authors":["gail"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1585699200,"objectID":"9dc7ab2bee66877a4d70ef1735699b7a","permalink":"/author/a-hrefhttps/sgailw.cswp.cs.technion.ac.il-targetblankgail-weiss/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/sgailw.cswp.cs.technion.ac.il-targetblankgail-weiss/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://sgailw.cswp.cs.technion.ac.il' target='blank'\u003eGail Weiss\u003c/a\u003e","type":"authors"},{"authors":["yoav"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1585699200,"objectID":"f29984c7f0aea115651c62c115d16a98","permalink":"/author/a-hrefhttps/u.cs.biu.ac.il/~yogo/-targetblankyoav-goldberg/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/u.cs.biu.ac.il/~yogo/-targetblankyoav-goldberg/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://u.cs.biu.ac.il/~yogo/' target='blank'\u003eYoav Goldberg\u003c/a\u003e","type":"authors"},{"authors":["dianqi"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1585699200,"objectID":"79a0915003a3ad061ccd8700817acf39","permalink":"/author/dianqi-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/dianqi-li/","section":"authors","summary":"","tags":null,"title":"Dianqi Li","type":"authors"},{"authors":["gabrielh"],"categories":null,"content":"","date":1580515200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1580515200,"objectID":"4891830e203f10bfb7350b059c02a20c","permalink":"/author/a-hrefhttp/gabrielilharco.com-targetblankgabriel-ilharco/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/gabrielilharco.com-targetblankgabriel-ilharco/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://gabrielilharco.com' target='blank'\u003eGabriel Ilharco\u003c/a\u003e","type":"authors"},{"authors":["ali"],"categories":null,"content":"","date":1580515200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1580515200,"objectID":"e2ca724dc1a3617350c4ef4bd06a6829","permalink":"/author/a-hrefhttps/homes.cs.washington.edu/~ali/-targetblankali-farhadi/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/homes.cs.washington.edu/~ali/-targetblankali-farhadi/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://homes.cs.washington.edu/~ali/' target='blank'\u003eAli Farhadi\u003c/a\u003e","type":"authors"},{"authors":["orene"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1561939200,"objectID":"541ff9e257368958f152de67922a7f8d","permalink":"/author/a-hrefhttp/allenai.org/team/orene/-targetblankoren-etzioni/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/allenai.org/team/orene/-targetblankoren-etzioni/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://allenai.org/team/orene/' target='blank'\u003eOren Etzioni\u003c/a\u003e","type":"authors"},{"authors":["markn="],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1561939200,"objectID":"82a5029d7d9b639f505e157787bf1d3d","permalink":"/author/a-hrefhttp/markneumann.xyz-targetblankmark-neumann/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/markneumann.xyz-targetblankmark-neumann/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://markneumann.xyz' target='blank'\u003eMark Neumann\u003c/a\u003e","type":"authors"},{"authors":["sameer="],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1561939200,"objectID":"6e07ef181703393f8b4b03a1b035f665","permalink":"/author/a-hrefhttp/sameersingh.org-targetblanksameer-singh/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/sameersingh.org-targetblanksameer-singh/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://sameersingh.org' target='blank'\u003eSameer Singh\u003c/a\u003e","type":"authors"},{"authors":["dallas="],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1561939200,"objectID":"654bc7dc4aeffca2894e972e22a9b648","permalink":"/author/a-hrefhttp/web.stanford.edu/~dcard/-targetblankdallas-card/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/web.stanford.edu/~dcard/-targetblankdallas-card/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://web.stanford.edu/~dcard/' target='blank'\u003eDallas Card\u003c/a\u003e","type":"authors"},{"authors":["jessestar"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1561939200,"objectID":"edf9fbb542bfe92b197fa1b0108104e9","permalink":"/author/a-hrefhttp/www.cs.cmu.edu/~jessed/-targetblankjesse-dodge/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/www.cs.cmu.edu/~jessed/-targetblankjesse-dodge/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://www.cs.cmu.edu/~jessed/' target='blank'\u003eJesse Dodge\u003c/a\u003e*","type":"authors"},{"authors":["rob="],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1561939200,"objectID":"1e69b33059cc9342e33da675bf1c651a","permalink":"/author/a-hrefhttps/rloganiv.github.io-targetblankrobert-logan/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/rloganiv.github.io-targetblankrobert-logan/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://rloganiv.github.io' target='blank'\u003eRobert Logan\u003c/a\u003e","type":"authors"},{"authors":["mattp="],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1561939200,"objectID":"6f2b7dd59575de35d40e0a23533cc5ea","permalink":"/author/a-hrefhttps/scholar.google.com/citationsuserk5ncpzwaaaajhttps/gabrielstanovsky.github.io/hl-targetblankmatthew-e.-peters/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/scholar.google.com/citationsuserk5ncpzwaaaajhttps/gabrielstanovsky.github.io/hl-targetblankmatthew-e.-peters/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://scholar.google.com/citations?user=K5nCPZwAAAAJhttps://gabrielstanovsky.github.io/hl' target='blank'\u003eMatthew E. Peters\u003c/a\u003e","type":"authors"},{"authors":["vidur="],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1561939200,"objectID":"d0262167f06a23b14f0fd7fc2a1d9257","permalink":"/author/a-hrefhttps/scholar.google.com/citationsusernobf_heaaaaj-targetblankvidur-joshi/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/scholar.google.com/citationsusernobf_heaaaaj-targetblankvidur-joshi/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://scholar.google.com/citations?user=NobF_hEAAAAJ' target='blank'\u003eVidur Joshi\u003c/a\u003e","type":"authors"},{"authors":["suchin"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1561939200,"objectID":"ead8ad11b4996c92654d577e09edc581","permalink":"/author/a-hrefhttps/suchin.io/-targetblanksuchin-gururangan/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/suchin.io/-targetblanksuchin-gururangan/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://suchin.io/' target='blank'\u003eSuchin Gururangan\u003c/a\u003e","type":"authors"},{"authors":["mestar"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1561939200,"objectID":"c0b934b66b419af8b3397aee48eb893f","permalink":"/author/broy-schwartz/b/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/broy-schwartz/b/","section":"authors","summary":"","tags":null,"title":"\u003cb\u003eRoy Schwartz\u003c/b\u003e*","type":"authors"},{"authors":["nelson"],"categories":null,"content":"","date":1554076800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1554076800,"objectID":"7184fd1ca11da8b5027f7b05f2b44d86","permalink":"/author/a-hrefhttps/cs.stanford.edu/~nfliu/-targetblanknelson-f.-liu/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/cs.stanford.edu/~nfliu/-targetblanknelson-f.-liu/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://cs.stanford.edu/~nfliu/' target='blank'\u003eNelson F. Liu\u003c/a\u003e","type":"authors"},{"authors":["rowan"],"categories":null,"content":"","date":1533081600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1533081600,"objectID":"d3750635a3b8746ef003d5679b039abb","permalink":"/author/a-hrefhttp/rowanzellers.com/-targetblankrowan-zellers/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/rowanzellers.com/-targetblankrowan-zellers/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://rowanzellers.com/' target='blank'\u003eRowan Zellers\u003c/a\u003e","type":"authors"},{"authors":["sam"],"categories":null,"content":"","date":1533081600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1533081600,"objectID":"e4e71fbcd126139d55a6bddb1c8f65b1","permalink":"/author/a-hrefhttp/samthomson.com/-targetblanksam-thomson/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/samthomson.com/-targetblanksam-thomson/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://samthomson.com/' target='blank'\u003eSam Thomson\u003c/a\u003e","type":"authors"},{"authors":["yonatan"],"categories":null,"content":"","date":1533081600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1533081600,"objectID":"464fa4fe4322425e77d3928c6b48ed77","permalink":"/author/a-hrefhttp/yonatanbisk.com/-targetblankyonatan-bisk/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/yonatanbisk.com/-targetblankyonatan-bisk/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://yonatanbisk.com/' target='blank'\u003eYonatan Bisk\u003c/a\u003e","type":"authors"},{"authors":["samstar"],"categories":null,"content":"","date":1525132800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1525132800,"objectID":"06cde61cbe9f1c0c7d3a6c111cd6dab7","permalink":"/author/a-hrefhttp/samthomson.com/-targetblanksam-thomson/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/samthomson.com/-targetblanksam-thomson/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://samthomson.com/' target='blank'\u003eSam Thomson\u003c/a\u003e*","type":"authors"},{"authors":["chenhao"],"categories":null,"content":"","date":1525132800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1525132800,"objectID":"ce8099c085eb4f38ff57db08388e4a12","permalink":"/author/a-hrefhttps/chenhaot.com/-targetblankchenhao-tan/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/chenhaot.com/-targetblankchenhao-tan/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://chenhaot.com/' target='blank'\u003eChenhao Tan\u003c/a\u003e","type":"authors"},{"authors":["omer"],"categories":null,"content":"","date":1525132800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1525132800,"objectID":"bc4bcb920a0d31cd4b7f0eff18fd6517","permalink":"/author/a-hrefhttps/levyomer.wordpress.com/-targetblankomer-levy/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/levyomer.wordpress.com/-targetblankomer-levy/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://levyomer.wordpress.com/' target='blank'\u003eOmer Levy\u003c/a\u003e","type":"authors"},{"authors":["bhavana"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1522540800,"objectID":"3a640323d6060c987b5c6cf2096872aa","permalink":"/author/a-hrefhttp/allenai.org/team/bhavanad/-targetblankbhavana-dalvi/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/allenai.org/team/bhavanad/-targetblankbhavana-dalvi/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://allenai.org/team/bhavanad/' target='blank'\u003eBhavana Dalvi\u003c/a\u003e","type":"authors"},{"authors":["dongyeop"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1522540800,"objectID":"2b09f815c215a56588b131014a9ed27a","permalink":"/author/a-hrefhttp/www.cs.cmu.edu/~dongyeok/-targetblankdongyeop-kang/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/www.cs.cmu.edu/~dongyeok/-targetblankdongyeop-kang/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://www.cs.cmu.edu/~dongyeok/' target='blank'\u003eDongyeop Kang\u003c/a\u003e","type":"authors"},{"authors":["ed"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1522540800,"objectID":"839353db5a8f9e9e14bbaddf22173466","permalink":"/author/a-hrefhttp/www.cs.cmu.edu/~hovy/-targetblankeduard-hovy/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/www.cs.cmu.edu/~hovy/-targetblankeduard-hovy/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://www.cs.cmu.edu/~hovy/' target='blank'\u003eEduard Hovy\u003c/a\u003e","type":"authors"},{"authors":["swabhastar"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1522540800,"objectID":"b5d4592c8b6c16e2f1ecfff660cfc067","permalink":"/author/a-hrefhttp/www.cs.cmu.edu/~sswayamd/-targetblankswabha-swayamdipta/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/www.cs.cmu.edu/~sswayamd/-targetblankswabha-swayamdipta/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://www.cs.cmu.edu/~sswayamd/' target='blank'\u003eSwabha Swayamdipta\u003c/a\u003e*","type":"authors"},{"authors":["suchinstar"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1522540800,"objectID":"fa34c25f6acfb0b83697c1a85ebf65b6","permalink":"/author/a-hrefhttps/suchin.io/-targetblanksuchin-gururangan/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/suchin.io/-targetblanksuchin-gururangan/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://suchin.io/' target='blank'\u003eSuchin Gururangan\u003c/a\u003e*","type":"authors"},{"authors":["waleed"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1522540800,"objectID":"9c68b4bb6737210793b2c8d1024633c2","permalink":"/author/a-hrefhttps/wammar.github.io/-targetblankwaleed-ammar/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/wammar.github.io/-targetblankwaleed-ammar/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://wammar.github.io/' target='blank'\u003eWaleed Ammar\u003c/a\u003e","type":"authors"},{"authors":["samb"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1522540800,"objectID":"318404fcdcf96a6ffb449c2ac0ab0500","permalink":"/author/a-hrefhttps/www.nyu.edu/projects/bowman/-targetblanksam-bowman/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/www.nyu.edu/projects/bowman/-targetblanksam-bowman/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://www.nyu.edu/projects/bowman/' target='blank'\u003eSam Bowman\u003c/a\u003e","type":"authors"},{"authors":["madeleine"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1522540800,"objectID":"2090b9f54db72db756e71cc95965a518","permalink":"/author/madeleine-van-zuylen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/madeleine-van-zuylen/","section":"authors","summary":"","tags":null,"title":"Madeleine van Zuylen","type":"authors"},{"authors":["sebastian"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1522540800,"objectID":"d68d186b5437c0054f698afb12d818b3","permalink":"/author/sebastian-kohlmeier/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/sebastian-kohlmeier/","section":"authors","summary":"","tags":null,"title":"Sebastian Kohlmeier","type":"authors"},{"authors":["maarten"],"categories":null,"content":"","date":1485907200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1485907200,"objectID":"09aa5d76f9c7c34a3cb468bb08cf92f9","permalink":"/author/a-hrefhttp/homes.cs.washington.edu/~msap/-targetblankmaarten-sap/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/homes.cs.washington.edu/~msap/-targetblankmaarten-sap/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://homes.cs.washington.edu/~msap/' target='blank'\u003eMaarten Sap\u003c/a\u003e","type":"authors"},{"authors":["roi"],"categories":null,"content":"","date":1485907200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1485907200,"objectID":"53b2c6a46ac812374e24a6a8bac0ec65","permalink":"/author/a-hrefhttp/ie.technion.ac.il/~roiri/-targetblankroi-reichart/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/ie.technion.ac.il/~roiri/-targetblankroi-reichart/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://ie.technion.ac.il/~roiri/' target='blank'\u003eRoi Reichart\u003c/a\u003e","type":"authors"},{"authors":["annak"],"categories":null,"content":"","date":1485907200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1485907200,"objectID":"8e70e0f286c4d498511c6e2df1dde685","permalink":"/author/a-hrefhttp/www.cl.cam.ac.uk/~alk23/-targetblankanna-korhonen/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/www.cl.cam.ac.uk/~alk23/-targetblankanna-korhonen/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://www.cl.cam.ac.uk/~alk23/' target='blank'\u003eAnna Korhonen\u003c/a\u003e","type":"authors"},{"authors":["ari"],"categories":null,"content":"","date":1485907200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1485907200,"objectID":"aa7925a00cffabb64a80be8f9b2b7a95","permalink":"/author/a-hrefhttp/www.cs.huji.ac.il/~arir/-targetblankari-rappoport/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/www.cs.huji.ac.il/~arir/-targetblankari-rappoport/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://www.cs.huji.ac.il/~arir/' target='blank'\u003eAri Rappoport\u003c/a\u003e","type":"authors"},{"authors":["yannis"],"categories":null,"content":"","date":1485907200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1485907200,"objectID":"1f98fb23db447d97730d8a349b745bdb","permalink":"/author/a-hrefhttp/www.ikonstas.net/-targetblankyannis-konstas/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/www.ikonstas.net/-targetblankyannis-konstas/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://www.ikonstas.net/' target='blank'\u003eYannis Konstas\u003c/a\u003e","type":"authors"},{"authors":["ivan"],"categories":null,"content":"","date":1485907200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1485907200,"objectID":"d083a01ef83d2c90e2ff0e0436ad5c23","permalink":"/author/a-hrefhttps/sites.google.com/site/ivanvulic/-targetblankivan-vulic/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/sites.google.com/site/ivanvulic/-targetblankivan-vulic/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://sites.google.com/site/ivanvulic/' target='blank'\u003eIvan Vulić\u003c/a\u003e","type":"authors"},{"authors":["lizilles"],"categories":null,"content":"","date":1485907200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1485907200,"objectID":"4ed9a47c39a25ac68ec93dc12ad71a2d","permalink":"/author/li-zilles/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/li-zilles/","section":"authors","summary":"","tags":null,"title":"Li Zilles","type":"authors"},{"authors":["dana"],"categories":null,"content":"","date":1435708800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1435708800,"objectID":"d62f511fb676ebed5efc144c0efe502b","permalink":"/author/dana-rubinstein/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/dana-rubinstein/","section":"authors","summary":"","tags":null,"title":"Dana Rubinstein","type":"authors"},{"authors":["effi"],"categories":null,"content":"","date":1435708800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1435708800,"objectID":"5211333d848210d45e42b0d089d2edb4","permalink":"/author/effi-levi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/effi-levi/","section":"authors","summary":"","tags":null,"title":"Effi Levi","type":"authors"},{"authors":["koppel"],"categories":null,"content":"","date":1372636800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1372636800,"objectID":"ccc402d91e3e8dd0c6118f5b88a0727f","permalink":"/author/a-hrefhttp/u.cs.biu.ac.il/~koppel/-targetblankmoshe-koppel/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/u.cs.biu.ac.il/~koppel/-targetblankmoshe-koppel/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://u.cs.biu.ac.il/~koppel/' target='blank'\u003eMoshe Koppel\u003c/a\u003e","type":"authors"},{"authors":["orent"],"categories":null,"content":"","date":1372636800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1372636800,"objectID":"67b1ba841b9254d03f776cee66088505","permalink":"/author/a-hrefhttp/www.ise.bgu.ac.il/orentsur/-targetblankoren-tsur/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/www.ise.bgu.ac.il/orentsur/-targetblankoren-tsur/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://www.ise.bgu.ac.il/OrenTsur/' target='blank'\u003eOren Tsur\u003c/a\u003e","type":"authors"},{"authors":["omri"],"categories":null,"content":"","date":1341100800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1341100800,"objectID":"9c2eae572b3ce680465db88ea3063b64","permalink":"/author/a-hrefhttp/www.cs.huji.ac.il/~oabend/-targetblankomri-abend/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/www.cs.huji.ac.il/~oabend/-targetblankomri-abend/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://www.cs.huji.ac.il/~oabend/' target='blank'\u003eOmri Abend\u003c/a\u003e","type":"authors"},{"authors":["lab"],"categories":null,"content":"Roy Schwartz's lab at the the School of Computer Science and Engineering at the The Hebrew University of Jerusalem studies Natural Language Processing (NLP). Our research is driven towards making text understanding technology widely accessible - to doctors, to teachers, to researchers or even to curious teenagers. To be broadly adopted, NLP technology needs to not only be accurate, but also reliable; models should provide explanations for their outputs; and the methods we use to evaluate them need to be convincing. Our lab also studies methods to make NLP technology more efficient and green, in order to decrease the environmental impact of the field, as well as lower the cost of AI research in order to broaden participation in it.  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"fe1f51c425d0e91980707c007b293282","permalink":"/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"Roy Schwartz's lab at the the School of Computer Science and Engineering at the The Hebrew University of Jerusalem studies Natural Language Processing (NLP). Our research is driven towards making text understanding technology widely accessible - to doctors, to teachers, to researchers or even to curious teenagers.","tags":null,"title":"","type":"authors"},{"authors":["home"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ad5e25f4be46d7ea52b4406d1eccc2e1","permalink":"/author/a-hrefhttp/www.cs.huji.ac.il/~roys02/-targetblank/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/www.cs.huji.ac.il/~roys02/-targetblank/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://www.cs.huji.ac.il/~roys02/' target='blank'\u003e\u003c/a\u003e","type":"authors"},{"authors":["gabis"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6316f613fc7d879429c05fee6af329a5","permalink":"/author/a-hrefhttps/gabrielstanovsky.github.io/-targetblankgabi-stanovsky/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/gabrielstanovsky.github.io/-targetblankgabi-stanovsky/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://gabrielstanovsky.github.io/' target='blank'\u003eGabi Stanovsky\u003c/a\u003e","type":"authors"},{"authors":["deborah"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3d25a7fa13df6502340d16df765bde29","permalink":"/author/deborah-wolhandler/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/deborah-wolhandler/","section":"authors","summary":"","tags":null,"title":"Deborah Wolhandler","type":"authors"},{"authors":["inbal"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"c975119c378996fc8132674ae733ae53","permalink":"/author/inbal-magar/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/inbal-magar/","section":"authors","summary":"","tags":null,"title":"Inbal Magar","type":"authors"},{"authors":["admin"],"categories":null,"content":"Bio Roy Schwartz is a senior lecturer at the School of Computer Science and Engineering at The Hebrew University of Jerusalem. Prior to that Roy was a postdoc (2016-2019) and then a research scientist (2019-2020) at the AllenNLP team at the Allen institute for AI and at the school of computer science and engineering at The University of Washington, where he was working with Noah A. Smith. Roy completed his Ph.D. in 2016 at the School of Computer Science and Engineering at The Hebrew University of Jerusalem, where he worked with Ari Rappoport. In 2011, he graduated his masters degree (magna cum laude) in computer science, under the supervision of Prof. Ari Rappoport. Prior to that, he studied computer science and cognitive sciences at the Hebrew University, and completed his B.Sc. (magna cum laude) in 2008. He was a member of the Amirim program for outstanding undergraduate students. In 2004-2005, he was a software engineer at Check Point Ltd.  Teaching  Lecturer, Object Oriented Programming; Israeli Council of Higher Education Program for Online Digital Learning (2018-2020) Guest talk on distributional semantics at UW NLP course (spring 2017, slides, video) UW-NLP RNN Reading Group (spring 2017) UW-NLP Discourse Reading Group (winter 2017) Lecturer of the huji coursera online version of Introduction to Object Oriented Programming Lecturer of Introduction to Object Oriented Programming (13/14, 11/12 [Ranked #1 in student survey!], 10/11, 09/10) Lecturer of Computer Laboratory in Data Structures (12/13)  Lecturer of Introduction to Programming in the Perl Language (07/08)  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/roy-schwartz/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/roy-schwartz/","section":"authors","summary":"Bio Roy Schwartz is a senior lecturer at the School of Computer Science and Engineering at The Hebrew University of Jerusalem. Prior to that Roy was a postdoc (2016-2019) and then a research scientist (2019-2020) at the AllenNLP team at the Allen institute for AI and at the school of computer science and engineering at The University of Washington, where he was working with Noah A.","tags":null,"title":"Roy Schwartz","type":"authors"},{"authors":["yarden"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2b1935f562e2cb2dd7c383a1f9b5496a","permalink":"/author/yarden-tal/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yarden-tal/","section":"authors","summary":"","tags":null,"title":"Yarden Tal","type":"authors"},{"authors":["yonatanbitton"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b0c7080f22198ed0c7d2341bfe4774af","permalink":"/author/yonatan-bitton-co-advised-with-a-hrefhttps/gabrielstanovsky.github.io/-target_blankgabriel-stanovsky/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yonatan-bitton-co-advised-with-a-hrefhttps/gabrielstanovsky.github.io/-target_blankgabriel-stanovsky/a/","section":"authors","summary":"","tags":null,"title":"Yonatan Bitton (Co-advised with \u003ca href=\"https://gabrielstanovsky.github.io/\" target=\"_blank\"\u003eGabriel Stanovsky\u003c/a\u003e)","type":"authors"},{"authors":["yuval_reif"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"8ac5d742ac1117a3fe406055777c288e","permalink":"/author/yuval-reif/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yuval-reif/","section":"authors","summary":"","tags":null,"title":"Yuval Reif","type":"authors"},{"authors":["swabha","me","nicholas","yizhong","hannaneh","noah","yejin"],"categories":null,"content":"","date":1600041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600041600,"objectID":"983549b2dfb87b817ebfe9e61790d7a9","permalink":"/publication/datamaps/","publishdate":"2020-09-01T00:00:00Z","relpermalink":"/publication/datamaps/","section":"publication","summary":"Large datasets have become commonplace in NLP research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce \"Data Maps\"---a model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: the behavior of the model on individual instances during training (training dynamics) for building data maps. This yields two intuitive measures for each example---the model's confidence in the true class, and the variability of this confidence across epochs, in a single run of training. Experiments on four datasets show that these model-dependent measures reveal three distinct regions in the data map, each with pronounced characteristics. First, our data maps show the presence of \"ambiguous\" regions with respect to the model, which contribute the most towards out-of-distribution generalization. Second, the most populous regions in the data are \"easy to learn\" for the model, and play an important role in model optimization. Finally, data maps uncover a region with instances that the model finds \"hard to learn\"; these often correspond to labeling errors. Our results indicate that a shift in focus from quantity to quality of data could lead to robust models and improved out-of-distribution generalization.","tags":["improved_evaluation","green_ai"],"title":"Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics","type":"publication"},{"authors":["aidastar","tomstar","davidwadden","me","hannaneh"],"categories":null,"content":"","date":1592524800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592524800,"objectID":"5898c87fc36fdb5a7f94308ca27e91da","permalink":"/publication/covid_mechanisms/","publishdate":"2020-06-19T00:00:00Z","relpermalink":"/publication/covid_mechanisms/","section":"publication","summary":"The COVID-19 pandemic has sparked an influx of research by scientists worldwide, leading to a rapidly evolving corpus of interdisciplinary papers. At the time of this writing the COVID-19 Open Research Dataset (CORD-19) has amassed over 128K relevant papers, both historical and cutting-edge. In this emergency scenario, there is a need for automatic information extraction (IE) to provide scientists with structured knowledge, and to accelerate exploration and discovery.\nIn this work we extract relations capturing a broad notion of mechanisms in CORD-19 papers \u0026ndash; Spanning a range of mechanisms as diverse as psychological intervention techniques, computational algorithms, and molecular mechanisms of viral cell entry. This unified view of natural and artificial mechanisms can help generalize across the CORD-19 corpus and is designed to help scale the study of the many different types of processes, activities and functions described in the dataset.\nWe collect a set of annotations from domain experts for direct mechanisms (operations and functions explicitly described in the text) and indirect mechanisms (observed effects and interactions without an explicit description of a direct functional relation). For example, descriptions of the mechanism by which the SARS-CoV-2 virus binds to cells, or of a diagnostic procedure based on computer vision \u0026ndash; are considered direct mechanisms. Conversely, descriptions of indirect mechanisms can for example be of observed links between COVID-19 and certain symptoms, with no explicit mention of the functional process leading from the disease to the symptoms. This distinction between direct and indirect relations is inspired by a review of biomedical and scientific ontologies (e.g., direct and indirect regulation of proteins by chemicals).\nWe allow annotators to select free-form text spans as the arguments in our mechanism relations; this is in contrast to many existing datasets of annotated scientific relations which are often entity-centric (e.g., protein-chemical interactions). We do so in order to capture the complexity and diversity of the many concepts and ideas described in the corpus, in a scalable approach. To address the challenging nature of the annotation task with multiple \"correct\" annotations of complex and diverse spans, we conduct a multi-round annotation process with final adjudication by a domain expert experienced in bioNLP annotations.\nOur annotations are used in combination with existing datasets from different domains to train a relation extraction model, using a mapping schema for previously introduced scientific datasets,selecting only direct and indirect mechanisms (e.g.,DIRECT UP-REGULATION in the chemprot dataset) and unifying relation labels into our typology using a domain expert. Our results indicate we outperform baselines including openIE and SRL, and also supervised models trained on related science IE datasets in the biomedical and computer science domains. We use a biomedical language model that we fine-tune to capture semantic similarity, build a graph of similar mechanisms and induce concepts by finding cliques. To support search over our KB, we use the same language model for retrieving relations similar to the query. To help boost community efforts we release our curated data and models as well as a large-scale knowledge graph of extracted mechanisms.","tags":null,"title":"Extracting a knowledge base of mechanisms from COVID-19 papers","type":"publication"},{"authors":["will","gail","yoav","me","noah","erany"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"d1aecc8e2f36bd4b4d385f0754f9971b","permalink":"/publication/rnn_hierarchy/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/rnn_hierarchy/","section":"publication","summary":"We develop a formal hierarchy of the expressive capacity of RNN architectures. The hierarchy is based around two formal properties: space complexity, which is a measure of the RNN's memory, and rational recurrence, defined as whether the recurrent update can be described by a weighted finite-state machine. We place several RNN variants within this hierarchy. For example, we prove that the LSTM is not rational, which formally separates it from the related QRNN (Bradbury et al., 2016). We also show how the expressive capacity of these models is expanded by stacking multiple layers or composing them with different pooling functions.  Our results build on the theory of 'saturated' RNNs (Merrill, 2019). While formally extending these findings to unsaturated RNNs is left to future work, we hypothesize that the practical learnable capacity of unsaturated RNNs obeys a similar hierarchy. Experimental findings from training unsaturated networks on formal languages support this conjecture.","tags":["understanding_models"],"title":"A Formal Hierarchy of RNN Architectures","type":"publication"},{"authors":["hao","me","dianqi","noah"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"68b89e8a4be4f02b881e0c079a4c279d","permalink":"/publication/mae/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/mae/","section":"publication","summary":"Multi-head attentive neural architectures have achieved state-of-the-art results on a variety of natural language processing tasks. Evidence has shown that they are overparameterized; attention heads can be pruned without significant performance loss. In this work, we instead 'reallocate' them-the model learns to activate different heads on different inputs. Drawing connections between multi-head attention and mixture of experts, we propose the mixture of attentive experts model (MAE). MAE is trained using a block coordinate descent algorithm that alternates between updating (1) the responsibilities of the experts and (2) their parameters. Experiments on machine translation and language modeling show that MAE outperforms strong baselines on both tasks. Particularly, on the WMT14 English to German translation dataset, MAE improves over 'transformer-base' by 0.8 BLEU, with a comparable number of parameters. Our analysis shows that our model learns to specialize different experts to different inputs.","tags":null,"title":"A Mixture of \u003ci\u003eh-1\u003c/i\u003e Heads is Better than \u003ci\u003eh\u003c/i\u003e Heads","type":"publication"},{"authors":null,"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"f582d6148313863e66c2c65bef36af19","permalink":"/talk/green_ai/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/talk/green_ai/","section":"talk","summary":"Microsoft, Machine Learning Seminar","tags":null,"title":"Green AI","type":"talk"},{"authors":["me","gabi","swabha","jesse","noah"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"09980fdceb6f0763d23789d3185f905c","permalink":"/publication/sledgehammer/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/sledgehammer/","section":"publication","summary":"As NLP models become larger, executing a trained model requires significant computational resources incurring monetary and environmental costs. To better respect a given inference budget, we propose a modification to contextual representation fine-tuning which, during inference, allows for an early (and fast) 'exit' from neural network calculations for simple instances, and late (and accurate) exit for hard instances. To achieve this, we add classifiers to different layers of BERT and use their calibrated confidence scores to make early exit decisions. We test our proposed modification on five different datasets in two tasks: three text classification datasets and two natural language inference benchmarks. Our method presents a favorable speed/accuracy tradeoff in almost all cases, producing models which are up to five times faster than the state of the art, while preserving their accuracy. Our method also requires almost no additional training resources (in either time or parameters) compared to the baseline BERT model. Finally, our method alleviates the need for costly retraining of multiple models at different levels of efficiency; we allow users to control the inference speed/accuracy tradeoff using a single trained model, by setting a single variable at inference time. We publicly release our code.","tags":["greenai"],"title":"The Right Tool for the Job: Matching Model and Instance Complexities","type":"publication"},{"authors":null,"categories":null,"content":"","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"bbd95ffd24c789dc5276ef0fa897188e","permalink":"/talk/green_nlp/","publishdate":"2020-03-01T00:00:00Z","relpermalink":"/talk/green_nlp/","section":"talk","summary":"Berkeley, Natural Language Processing Group Seminar\nStanford, Natural Language Processing Group Seminar\nGoogle Brain, Natural Language Processing Group Seminar","tags":null,"title":"Green NLP","type":"talk"},{"authors":["jesse","gabrielh","me","ali","hannaneh","noah"],"categories":null,"content":"","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580515200,"objectID":"e20b7646b10e3955a01b8e92a2233a27","permalink":"/publication/finetune/","publishdate":"2020-02-01T00:00:00Z","relpermalink":"/publication/finetune/","section":"publication","summary":"Fine-tuning pretrained contextual word embedding models to supervised downstream tasks has become commonplace in natural language processing. This process, however, is often brittle: even with the same hyperparameter values, distinct random seeds can lead to substantially different results. To better understand this phenomenon, we experiment with four datasets from the GLUE benchmark, fine-tuning BERT hundreds of times on each while varying only the random seeds. We find substantial performance increases compared to previously reported results, and we quantify how the performance of the best-found model varies as a function of the number of fine-tuning trials. Further, we examine two factors influenced by the choice of random seed: weight initialization and training data order. We find that both contribute comparably to the variance of out-of-sample performance, and that some weight initializations perform well across all tasks explored. On small datasets, we observe that many fine-tuning trials diverge part of the way through training, and we offer best practices for practitioners to stop training less promising runs early. We publicly release all of our experimental data, including training and validation scores for 2,100 trials, to encourage further analysis of training dynamics during fine-tuning.","tags":["greenai"],"title":"Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping","type":"publication"},{"authors":["mestar","jessestar","noah","orene"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"a34fddc456f6c9c067029157009f758d","permalink":"/publication/greenai/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/publication/greenai/","section":"publication","summary":"The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018 [2]. These computations have a surprisingly large carbon footprint [38]. Ironically, deep learning was inspired by the human brain, which is remarkably energy efficient. Moreover, the financial cost of the computations can make it difficult for academics, students, and researchers, in particular those from emerging economies, to engage in deep learning research.\nThis position paper advocates a practical solution by making efficiency an evaluation criterion for research alongside accuracy and related measures. In addition, we propose reporting the financial cost or 'price tag' of developing, training, and running models to provide baselines for the investigation of increasingly efficient methods. Our goal is to make AI both greener and more inclusive---enabling any inspired undergraduate with a laptop to write high-quality research papers. Green AI is an emerging focus at the Allen Institute for AI.","tags":["greenai"],"title":"Green AI","type":"publication"},{"authors":["mattp","markn","rob","me","vidur","sameer","noah"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"5241c7fc13c8a5c9e0a46ab2ae255360","permalink":"/publication/kermit/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/kermit/","section":"publication","summary":"Contextual word representations, typically trained on unstructured, unlabeled text, do not contain any explicit grounding to real world entities and are often unable to remember facts about those entities. We propose a general method to embed multiple knowledge bases(KBs) into large scale models, and there by enhance their representations with structured, human-curated knowledge. For each KB, we first use an integrated entity linker to retrieve relevant entity embeddings, then update contextual word representations via a form of word-to-entity attention. In contrast to previous approaches, the entity linkers and self-supervised language modeling objective are jointly trained end-to-end in a multitask setting that combines a small amount of entity linking supervision with a large amount of raw text. After integrating WordNet and a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation. KnowBert’s runtime is comparable to BERT’sand it scales to large KBs.","tags":["word_representations"],"title":"Knowledge Enhanced Contextual Word Representations","type":"publication"},{"authors":["hao","me","noah"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"2e5dcdc75a12ddd89f89063f1e760e1f","permalink":"/publication/palm/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/palm/","section":"publication","summary":"We present PaLM, a hybrid parser and neural language model. Building on an RNN language model, PaLM adds an attention layer over text spans in the left context. An unsupervised constituency parser can be derived from its attention weights, using a greedy decoding algorithm. We evaluate PaLM on language modeling, and empirically show that it outperforms strong baselines. If syntactic annotations are available, the attention component can be trained in a supervised manner, providing syntactically-informed representations of the context, and further improving language modeling performance.","tags":["understanding_models"],"title":"PaLM: A Hybrid Parser and Language Model","type":"publication"},{"authors":["jesse","me","hao","noah"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"bc1eea8e54580e8d974265f69ea149db","permalink":"/publication/sparsifying/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/sparsifying/","section":"publication","summary":"Neural models for NLP typically use large numbers of parameters to reach state-of-the- art performance, which can lead to excessive memory usage and increased runtime. We present a structure learning method for learning sparse, parameter-efficient NLP models. Our method applies group lasso to rational RNNs (Peng et al., 2018), a family of models that is closely connected to weighted finite-state automata (WFSAs). We take advantage of rational RNNs’ natural grouping of the weights, so the group lasso penalty directly removes WFSA states, substantially reducing the number of parameters in the model. Our experiments on a number of sentiment analysis datasets, using both GloVe and BERT embeddings, show that our approach learns neural structures which have fewer parameters without sacrificing performance relative to parameter-rich baselines. Our method also highlights the interpretable properties of rational RNNs. We show that sparsifying such models makes them easier to visualize, and we present models that rely exclusively on as few as three WFSAs after pruning more than 90% of the weights. We publicly release our code.","tags":["greenai","understanding_models"],"title":"RNN Architecture Learning with Sparse Regularization ","type":"publication"},{"authors":["jesse","suchin","dallas","me","noah"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"37be21390a243324f4cc1fb45554898a","permalink":"/publication/showyourwork/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/showyourwork/","section":"publication","summary":"Research in natural language processing proceeds, in part, by demonstrating that new models achieve superior performance (e.g., accuracy) on held-out test data, compared to previous results. In this paper, we demonstrate that test-set performance scores alone are insufficient for drawing accurate conclusions about which model performs best. We argue for reporting additional details, especially performance on validation data obtained during model development. We present a novel technique for doing so: expected validation performance of the best-found model as a function of computation budget (i.e., the number of hyperparameter search trials or the overall training time). Using our approach, we find multiple recent model comparisons where authors would have reached a different conclusion if they had used more (or less) computation. Our approach also allows us to estimate the amount of computation required to obtain a given accuracy; applying it to several recently published results yields massive variation across papers, from hours to weeks. We conclude with a set of best practices for reporting experimental results which allow for robust future comparisons, and provide code to allow researchers to use our technique.","tags":["greenai"],"title":"Show Your Work: Improved Reporting of Experimental Results ","type":"publication"},{"authors":["nelson","me","noah"],"categories":null,"content":"","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554076800,"objectID":"458917737964753b4534a68d01b748ad","permalink":"/publication/inoculation/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/publication/inoculation/","section":"publication","summary":"Several datasets have recently been constructed to expose brittleness in models trained on existing benchmarks. While model performance on these challenge datasets is significantly lower compared to the original benchmark, it is unclear what particular weaknesses they reveal. For example, a challenge dataset may be difficult because it targets phenomena that current models cannot capture, or because it simply exploits blind spots in a model's specific training set. We introduce inoculation by fine-tuning, a new analysis method for studying challenge datasets by exposing models (the metaphorical patient) to a small amount of data from the challenge dataset (a metaphorical pathogen) and assessing how well they can adapt. We apply our method to analyze the NLI 'stress tests' (Naik et al., 2018) and the Adversarial SQuAD dataset (Jia and Liang,2017). We show that after slight exposure, some of these datasets are no longer challenging, while others remain difficult. Our results indicate that failures on challenge datasets may lead to very different conclusions about models, training datasets, and the challenge datasets themselves.","tags":["improved_evaluation"],"title":"Inoculation by Fine-Tuning: A Method for Analyzing Challenge Datasets","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"2080fb38cd10bc24f698ab039e3694bc","permalink":"/talk/towards_interpretable/","publishdate":"2018-12-01T00:00:00Z","relpermalink":"/talk/towards_interpretable/","section":"talk","summary":"Tel Aviv University, Computer Science and Electrical Engineering Colloquia\nWeizmann Institute, Machine Learning Seminar\nThe Hebrew University, Computer Science Colloquium\nTechnion, Computer Science, Electrical Engineering, and Industrial Engineering Colloquia","tags":null,"title":"Towards Interpretable Deep Learning for Natural Language Processing","type":"talk"},{"authors":["hao","me","sam","noah"],"categories":null,"content":"","date":1533081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533081600,"objectID":"c037b7b164ba30b860de47f247a01c0c","permalink":"/publication/rr/","publishdate":"2018-10-31T00:00:00Z","relpermalink":"/publication/rr/","section":"publication","summary":"Despite the tremendous empirical success of neural models in natural language processing, many of them lack the strong intuitions that accompany classical machine learning approaches. Recently, connections have been shown between convolutional neural networks (CNNs) and weighted finite state automata (WFSAs), leading to new interpretations and insights. In this work, we show that some recurrent neural networks also share this connection to WFSAs. We characterize this connection formally, defining rational recurrences to be recurrent hidden state update functions that can be written as the Forward calculation of a finite set of WFSAs. We show that several recent neural models use rational recurrences. Our analysis provides a fresh view of these models and facilitates devising new neural architectures that draw inspiration from WFSAs. We present one such model, which performs better than two recent baselines on language modeling and text classification. Our results demonstrate that transferring intuitions from classical models like WFSAs can be an effective approach to designing and understanding neural models.","tags":["understanding_models"],"title":"Rational Recurrences","type":"publication"},{"authors":["rowan","yonatanbisk","me","yejin"],"categories":null,"content":"","date":1533081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533081600,"objectID":"eb52c057620dbb0cd289f1918dce57c4","permalink":"/publication/swag/","publishdate":"2018-10-31T00:00:00Z","relpermalink":"/publication/swag/","section":"publication","summary":"Given a partial description like 'she opened the hood of the car', humans can reason about the situation and anticipate what might come next ('then, she examined the engine'). In this paper, we introduce the task of grounded commonsense inference, unifying natural language inference and commonsense reasoning.\nWe present SWAG, a new dataset with 113k multiple choice questions about a rich spectrum of grounded situations. To address the recurring challenges of the annotation artifacts and human biases found in many existing datasets, we propose Adversarial Filtering (AF), a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers, and using them to filter the data. To account for the aggressive adversarial filtering, we use state-of-the-art language models to massively oversample a diverse set of potential counterfactuals. Empirical results demonstrate that while humans can solve the resulting inference problems with high accuracy (88%), various competitive models struggle on our task. We provide comprehensive analysis that indicates significant opportunities for future research.","tags":["improved_evaluation"],"title":"SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference","type":"publication"},{"authors":null,"categories":null,"content":" Guest talk on distributional semantics at UW NLP course (spring 2017, slides, video) UW-NLP RNN Reading Group (spring 2017) UW-NLP Discourse Reading Group (winter 2017) Lecturer of the huji coursera online version of Introduction to Object Oriented Programming Lecturer of Introduction to Object Oriented Programming (13/14, 11/12 [Ranked #1 in student survey!], 10/11, 09/10) Lecturer of Computer Laboratory in Data Structures (12/13)  Lecturer of Introduction to Programming in the Perl Language (07/08)  ","date":1530144000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530144000,"objectID":"322dbaccf72a6d71f827fdb2866be935","permalink":"/teaching/","publishdate":"2018-06-28T00:00:00Z","relpermalink":"/teaching/","section":"","summary":"Here we describe how to add a page to your site.","tags":null,"title":"Teaching","type":"page"},{"authors":["nelson","omer","me","chenhao","noah"],"categories":null,"content":"","date":1525132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525132800,"objectID":"6f1cac49532ac4818369595c415f0b2d","permalink":"/publication/lstm/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/publication/lstm/","section":"publication","summary":"While recurrent neural networks have found success in a variety of natural language processing applications, they are general models of sequential data. We investigate how the properties of natural language data affect an LSTM’s ability to learn a nonlinguistic task: recalling elements from its input. We find that models trained on natural language data are able to recall tokens from much longer sequences than models trained on non-language sequential data. Furthermore, we show that the LSTM learns to solve the memorization task by explicitly using a subset of its neurons to count timesteps in the input. We hypothesize that the patterns and structure in natural language data enable LSTMs to learn by providing approximate ways of reducing loss, but understanding the effect of different training data on the learnability of LSTMs remains an open question.","tags":null,"title":"LSTMs Exploit Linguistic Attributes of Data","type":"publication"},{"authors":["mestar","samstar","noah"],"categories":null,"content":"","date":1525132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525132800,"objectID":"b29b498a1f0f35b3881ba70697daa94e","permalink":"/publication/sopa/","publishdate":"2018-07-15T00:00:00Z","relpermalink":"/publication/sopa/","section":"publication","summary":"Recurrent and convolutional neural networks comprise two distinct families of models that have proven to be useful for encoding natural language utterances. In this paper we present SoPa, a new model that aims to bridge these two approaches. SoPa combines neural representation learning with weighted finite-state automata (WFSAs) to learn a soft version of traditional surface patterns. We show that SoPa is an extension of a one-layer CNN, and that such CNNs are equivalent to a restricted version of SoPa, and accordingly, to a restricted form of WFSA. Empirically, on three text classification tasks, SoPa is comparable or better than both a BiLSTM (RNN) baseline and a CNN baseline, and is particularly useful in small data settings.","tags":["understanding_models"],"title":"SoPa: Bridging CNNs, RNNs, and Weighted Finite-State Machines","type":"publication"},{"authors":["dongyeop","waleed","bhavana","madeleine","sebastian","ed","me"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522540800,"objectID":"e432881c7bb49b824679cd87674d65e8","permalink":"/publication/peeread/","publishdate":"2018-06-02T00:00:00Z","relpermalink":"/publication/peeread/","section":"publication","summary":"Peer reviewing is a central component in the scientific publishing process. We present the first public dataset of scientific peer reviews available for research purposes (PeerRead v1) providing an opportunity to study this important artifact. The dataset consists of 14K paper drafts and the corresponding accept/reject decisions in top-tier venues including ACL, NIPS and ICLR.  The dataset also includes 8.7K textual peer reviews written by experts for a subset of the papers. We describe the data collection process and report interesting observed phenomena in the peer reviews. We also propose two novel NLP tasks based on this dataset and provide simple baseline models. In the first task, we show that simple models can predict whether a paper is accepted with up to 21% error reduction compared to the majority baseline. In the second task, we predict the numerical scores of review aspects and show that simple models can outperform the mean baseline for aspects with high variance such as 'originality' and 'impact'.","tags":null,"title":"A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications","type":"publication"},{"authors":["suchinstar","swabhastar","omer","me","samb","noah"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522540800,"objectID":"0e1a342b11782311ddc5e4b260ff7a88","permalink":"/publication/artifacts/","publishdate":"2018-06-02T00:00:00Z","relpermalink":"/publication/artifacts/","section":"publication","summary":"Large-scale datasets for natural language inference are created by presenting crowd workers with a sentence (premise), and asking them to generate three new sentences (hypotheses) that it entails, contradicts, or is logically neutral with respect to. We show that, in a significant portion of such data, this protocol leaves clues that make it possible to identify the label by looking only at the hypothesis, without observing the premise. Specifically, we show that a simple text categorization model can correctly classify the hypothesis alone in about 67% of SNLI (Bowman et al., 2015) and 53% of MultiNLI (Williams et al., 2018). Our analysis reveals that specific linguistic phenomena such as negation and vagueness are highly correlated with certain inference classes. Our findings suggest that the success of natural language inference models to date has been overestimated, and that the task remains a hard open problem.","tags":["improved_evaluation"],"title":"Annotation Artifacts in Natural Language Inference Data","type":"publication"},{"authors":null,"categories":null,"content":"","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512086400,"objectID":"dd3f0ca395d5bb85ba9ad24f4215d910","permalink":"/talk/inductive_bias/","publishdate":"2017-12-01T00:00:00Z","relpermalink":"/talk/inductive_bias/","section":"talk","summary":"Google Research Tel Aviv, Machine Learning Seminar","tags":null,"title":"Inductive Bias of Deep Networks through Language Patterns","type":"talk"},{"authors":["ivan","me","ari","roi","annak"],"categories":null,"content":"","date":1485907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1485907200,"objectID":"139d8ad21d31bbe1c3529101948656ab","permalink":"/publication/context_config/","publishdate":"2017-07-01T00:00:00Z","relpermalink":"/publication/context_config/","section":"publication","summary":"This paper is concerned with identifying contexts useful for training word representation models for different word classes such as adjectives (A), verbs (V), and nouns (N). We introduce a simple yet effective framework for an automatic selection of class-specific context configurations. We construct a context configuration space based on universal dependency relations between words, and efficiently search this space with an adapted beam search algorithm. In word similarity tasks for each word class, we show that our framework is both effective and efficient. Particularly, it improves the Spearman's rho correlation with human scores on SimLex-999 over the best previously proposed class-specific contexts by 6 (A), 6 (V) and 5 (N) rho points. With our selected context configurations, we train on only 14% (A), 26.2% (V), and 33.6% (N) of all dependency-based contexts, resulting in a reduced training time. Our results generalise: we show that the configurations our algorithm learns for one English training setup outperform previously proposed context types in another training setup for English. Moreover, basing the configuration space on universal dependencies, it is possible to transfer the learned configurations to German and Italian. We also demonstrate improved per-class results over other context types in these two languages.","tags":["word_representations"],"title":"Automatic selection of context configurations for improved (and fast) class-specific word representations","type":"publication"},{"authors":["me","maarten","yannis","lizilles","yejin","noah"],"categories":null,"content":"","date":1485907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1485907200,"objectID":"578928aec6c1656fa51b778566bdeabf","permalink":"/publication/language_constraint/","publishdate":"2017-07-01T00:00:00Z","relpermalink":"/publication/language_constraint/","section":"publication","summary":"A writer's style depends not just on personal traits but also on her intent and mental state. In this paper, we show how variants of the same writing task can lead to measurable differences in writing style. We present a case study based on the story cloze task (Mostafazadeh et al., 2016a), where annotators were assigned similar writing tasks with different constraints: (1) writing an entire story, (2) adding a story ending for a given story context, and (3) adding an incoherent ending to a story. We show that a simple linear classifier informed by stylistic features is able to successfully distinguish among the three cases, without even looking at the story context. In addition, combining our stylistic features with language model predictions reaches state of the art performance on the story cloze challenge. Our results demonstrate that different task framings can dramatically affect the way people write.","tags":["improved_evaluation"],"title":"The Effect of Different Writing Tasks on Linguistic Style: A Case Study of the ROC Story Cloze Task","type":"publication"},{"authors":["me","maarten","yannis","lizilles","yejin","noah"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"16f511c2c1e2d28c9173df867d8d4aa6","permalink":"/publication/lsdsem_uw_nlp/","publishdate":"2017-04-01T00:00:00Z","relpermalink":"/publication/lsdsem_uw_nlp/","section":"publication","summary":"This paper describes University of Washington NLP's submission for the Linking Models of Lexical, Sentential and Discourse-level Semantics (LSDSem 2017) shared task\u0026mdash;the Story Cloze Task. Our system is a linear classifier with a variety of features, including both the scores of a neural language model and style features. We report 75.2% accuracy on the task. A further discussion of our results can be found in Schwartz et al. (2017).","tags":null,"title":"Story Cloze Task: UW NLP System","type":"publication"},{"authors":["me","roi","ari"],"categories":null,"content":"","date":1467331200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1467331200,"objectID":"38a984ae3fcee16dc7bee710c452d723","permalink":"/publication/sp_sg/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/sp_sg/","section":"publication","summary":"State-of-the-art word embeddings, which are often trained on bag-of-words (BOW) contexts, provide a high quality representation of aspects of the semantics of nouns. However, their quality decreases substantially for the task of verb similarity prediction. In this paper we show that using symmetric pattern contexts (SPs, e.g., ``X and Y'') improves word2vec verb similarity performance by up to 15% and is also instrumental in adjective similarity prediction. The unsupervised SP contexts are even superior to a variety of dependency contexts extracted using a supervised dependency parser. Moreover, we observe that SPs and dependency coordination contexts (Coor) capture a similar type of information, and demonstrate that Coor contexts are superior to other dependency contexts including the set of all dependency contexts, although they are still inferior to SPs. Finally, there are substantially fewer SP contexts compared to alternative representations, leading to a massive reduction in training time. On an 8G words corpus and a 32 core machine, the SP model trains in 11 minutes, compared to 5 and 11 hours with BOW and all dependency contexts, respectively.","tags":["word_representations"],"title":"Symmetric Patterns and Coordinations: Fast and Enhanced Representations of Verbs and Adjectives","type":"publication"},{"authors":["me"],"categories":null,"content":"","date":1464739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464739200,"objectID":"0f40cf545adf79432be77d9037b7794b","permalink":"/publication/thesis/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/thesis/","section":"publication","summary":"","tags":["word_representations"],"title":"Pattern-based methods for Improved Lexical Semantics and Word Embeddings","type":"publication"},{"authors":null,"categories":null,"content":"","date":1454284800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1454284800,"objectID":"efb082ee0a992cc8565a3b08a0d42f23","permalink":"/talk/pattern-based_solutions/","publishdate":"2016-02-01T00:00:00Z","relpermalink":"/talk/pattern-based_solutions/","section":"talk","summary":"Intel Inc. Yakum, Research talk\nUniversity of Pennsylvania, CLunch computational linguistics seminar\nJohns Hopkins University, NLP seminar\nUniversity of Washington, NLP seminar","tags":null,"title":"Pattern-based Solutions to Limitations of Leading Word Embeddings","type":"talk"},{"authors":null,"categories":null,"content":"","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"f248d7679f3df615b94f5411aef8dc1c","permalink":"/talk/word_similarity/","publishdate":"2015-09-01T00:00:00Z","relpermalink":"/talk/word_similarity/","section":"talk","summary":"IBM Research Tel Aviv, Machine Learning and Data Mining Group Seminar","tags":null,"title":"Word Similarity via Symmetric Patterns","type":"talk"},{"authors":["dana","effi","","me","ari"],"categories":null,"content":"","date":1435708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435708800,"objectID":"12f578dd53839cc61ab7c9b18ee14675","permalink":"/publication/semantics_from_text/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/semantics_from_text/","section":"publication","summary":"In recent years, distributional models (DMs) have shown great success in representing lexical semantics. In this work we show that the extent to which DMs represent semantic knowledge is highly dependent on the type of knowledge. We pose the task of predicting properties of concrete nouns in a supervised setting, and compare between learning taxonomic properties (e.g., animacy) and attributive properties (e.g., size, color). We employ four state-of-the-art DMs as sources of feature representation for this task, and show that they all yield poor results when tested on attributive properties, achieving no more than an average F-score of 0.37 in the binary property prediction task, compared to 0.73 on taxonomic properties. Our results suggest that the distributional hypothesis may not be equally applicable to all types of semantic information.","tags":["word_representations"],"title":"How Well Do Distributional Models Capture Different Types of Semantic Knowledge?","type":"publication"},{"authors":["me","roi","ari"],"categories":null,"content":"","date":1435708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435708800,"objectID":"8e3bd2cdf0982208a5caab7cfdf9f0bf","permalink":"/publication/sp_embeddings/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/sp_embeddings/","section":"publication","summary":"We present a novel word level vector representation based on symmetric patterns (SPs). For this aim we automatically acquire SPs (e.g., 'X and Y') from a large corpus of plain text, and generate vectors where each coordinate represents the co-occurrence in SPs of the represented word with another word of the vocabulary. Our representation has three advantages over existing alternatives: First, being based on symmetric word relationships, it is highly suitable for word similarity prediction. Particularly, on the SimLex999 word similarity dataset, our model achieves a Spearman's \u0026#961; score of 0.517, compared to 0.462 of the state-of-the-art word2vec model. Interestingly, our model performs exceptionally well on verbs, outperforming state-of-the-art baselines by 20.2\u0026#x2012;41.5%. Second, pattern features can be adapted to the needs of a target NLP application. For example, we show that we can easily control whether the embeddings derived from SPs deem antonym pairs (e.g. (big,small)) as similar or dissimilar, an important distinction for tasks such as word classification and sentiment analysis. Finally, we show that a simple combination of the word similarity scores generated by our method and by word2vec results in a superior predictive power over that of each individual model, scoring as high as 0.563 in Spearman's \u0026#961; on SimLex999. This emphasizes the differences between the signals captured by each of the models.","tags":["word_representations"],"title":"Symmetric Pattern Based Word Embeddings for Improved Word Similarity Prediction","type":"publication"},{"authors":null,"categories":null,"content":"","date":1422748800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1422748800,"objectID":"0fc9851e88f6a985877336c58da6b7cb","permalink":"/talk/semantic_knowledge/","publishdate":"2015-02-01T00:00:00Z","relpermalink":"/talk/semantic_knowledge/","section":"talk","summary":"The Catalonia-Israel Symposium on Lexical Semantics and Grammatical Structure","tags":null,"title":"Semantic Knowledge Acquisition using Frequency Based Patterns","type":"talk"},{"authors":null,"categories":null,"content":"","date":1417392000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1417392000,"objectID":"efd0c4db9e70a844e3002aa090a475ea","permalink":"/talk/acquiring_semantic/","publishdate":"2014-12-01T00:00:00Z","relpermalink":"/talk/acquiring_semantic/","section":"talk","summary":"Hebrew University, CS Learning Semanir","tags":null,"title":"Acquiring Semantic Knowledge using Patterns","type":"talk"},{"authors":["me","roi","ari"],"categories":null,"content":"","date":1404172800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1404172800,"objectID":"d9563020ab84f7e55132ec38685a3b71","permalink":"/publication/coarse_grained/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/coarse_grained/","section":"publication","summary":"Classifying nouns into semantic categories (e.g., animals, food) is an important line of research in both cognitive science and natural language processing. We present a minimally supervised model for noun classification, which uses symmetric patterns (e.g., 'X and Y') and an iterative variant of the k-Nearest Neighbors algorithm. Unlike most previous works, we do not use a predefined set of symmetric patterns, but extract them automatically from plain text, in an unsupervised manner. We experiment with four semantic categories and show that symmetric patterns constitute much better classification features compared to leading word embedding methods. We further demonstrate that our simple k-Nearest Neighbors algorithm outperforms two state-of-the-art label propagation alternatives for this task. In experiments, our model obtains 82%-94% accuracy using as few as four labeled examples per category, emphasizing the effectiveness of simple search and representation techniques for this task.","tags":null,"title":"Minimally Supervised Classification to Semantic Categories using Automatically Acquired Symmetric Patterns","type":"publication"},{"authors":null,"categories":null,"content":"","date":1398902400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1398902400,"objectID":"948bd5778ebb1e6f2fc974738736383b","permalink":"/talk/identifying_authorships/","publishdate":"2014-05-01T00:00:00Z","relpermalink":"/talk/identifying_authorships/","section":"talk","summary":"Intel Inc. Haifa, ICRI-CI Retreat","tags":null,"title":"Identifying Authorships of very Short Texts using Flexible Patterns","type":"talk"},{"authors":null,"categories":null,"content":"","date":1380585600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1380585600,"objectID":"3047206d7b79a10b7743f23d4bcdd619","permalink":"/talk/semantic_representation/","publishdate":"2013-10-01T00:00:00Z","relpermalink":"/talk/semantic_representation/","section":"talk","summary":"Berkeley, Natural Language Processing Group Seminar\nStanford, Natural Language Processing Group Seminar\nUSC Information Sciences Institute, Natural Language Processing Group Seminar\nTwitter Inc., Technological Talk\nIntel Inc. Santa Clara, Natural Language Processing Group Seminar\nIBM Research Tel Aviv, Machine Learning and Data Mining Group Seminar","tags":null,"title":"Semantic Representation using Flexible Patterns","type":"talk"},{"authors":["me","orent","ari","koppel"],"categories":null,"content":"","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"e61824aa977036cb12cae2e0cc68ad76","permalink":"/publication/twitter_authorship/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/twitter_authorship/","section":"publication","summary":"Work on authorship attribution has traditionally focused on long texts. In this work, we tackle the question of whether the author of a very short text can be successfully identified. We use Twitter as an experimental testbed. We introduce the concept of an author's unique 'signature', and show that such signatures are typical of many authors when writing very short texts. We also present a new authorship attribution feature ('flexible patterns') and demonstrate a significant improvement over our baselines. Our results show that the author of a single tweet can be identified with good accuracy in an array of flavors of the authorship attribution task.","tags":null,"title":"Authorship Attribution of Micro-Messages","type":"publication"},{"authors":["me","omri","ari"],"categories":null,"content":"","date":1341100800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1341100800,"objectID":"9b25dda30b8c75cbba3971cccd528e1d","permalink":"/publication/learnable/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/learnable/","section":"publication","summary":"There is often more than one way to represent syntactic structures, even within a given formalism. Selecting one representation over another may affect parsing performance. Therefore, selecting between alternative syntactic representations (henceforth, syntactic selection) is an essential step in designing an annotation scheme. We present a methodology for syntactic selection and apply it to six central dependency structures. Our methodology compares pairs of annotation schemes that differ in the annotation of a single structure. It selects the more learnable scheme, namely the one that can be better learned using statistical parsers. We find that in three of the structures, one annotation is unequivocally better than the alternatives. Our results are consistent over various settings involving five parsers and two definitions of learnability. Furthermore, we show that the learnability gains incurred by our selections are both considerable (error reductions of up to 19.8%) and additive. The contribution of this work is in demonstrating that syntactic selection has a substantial and predictable effect on parsing performance, and showing that this effect can be effectively used in designing syntactic annotation schemes.","tags":null,"title":"Learnability-based Syntactic Annotation Design","type":"publication"},{"authors":["me","omri","roi","ari"],"categories":null,"content":"","date":1309478400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1309478400,"objectID":"130d4fc50cfb78d36946926efa60f0bd","permalink":"/publication/ned/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/ned/","section":"publication","summary":"Dependency parsing is a central NLP task. In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations. We show that for three leading unsupervised parsers (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures. These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon.","tags":["improved_evaluation"],"title":"Neutralizing Linguistically Problematic Annotations in Unsupervised Dependency Parsing Evaluation","type":"publication"},{"authors":null,"categories":null,"content":"","date":1306886400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1306886400,"objectID":"12b9b21ce1c6bfc2bb3d349959d3e8d7","permalink":"/talk/neutralizing_linguistically/","publishdate":"2011-06-01T00:00:00Z","relpermalink":"/talk/neutralizing_linguistically/","section":"talk","summary":"The Israeli Seminar on Computational Linguistics, ISCOL 2011","tags":null,"title":"Neutralizing Linguistically Problematic Annotations in Unsupervised Dependency Parsing Evaluation","type":"talk"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0985a49259cc27ed0412019929cef063","permalink":"/project/biases/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/biases/","section":"project","summary":"We analyze the datasets on which NLP models are trained. Looking carefully into these datasets, we uncover limitations and biases in the data collection process as well as the evaluation process. Our findings indicate that the recent success of neural models on many NLP tasks has been overestimated, and pave the way for the development of more reliable methods of evaluation.","tags":null,"title":"Biases in Datasets","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d686074f00ba84d4757f75524bbc0c89","permalink":"/project/green_nlp/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/green_nlp/","section":"project","summary":"The computations required for deep learning research have been doubling every few months. These computations have a surprisingly large carbon footprint. Moreover, the financial cost of the computations can make it difficult for academics, students, and researchers, in particular those from emerging economies, to engage in deep learning research. Our lab studies tools to make NLP technology more efficient, and to enhance the reporting of computational budgets.","tags":null,"title":"Green NLP","type":"project"},{"authors":null,"categories":null,"content":"We are looking for curious and motivated students and postdocs from diverse backgrounds, who are interested in natural language processing. If you are looking for a postdoc, Ph.D., Masters, undergrad project or summer internship, please email Roy, attach a 2-page CV, and briefly describe your research interests.  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fba6c149775da15c6a5b2adae893dd1c","permalink":"/joinus/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/joinus/","section":"","summary":"We are looking for curious and motivated students and postdocs from diverse backgrounds, who are interested in natural language processing. If you are looking for a postdoc, Ph.D., Masters, undergrad project or summer internship, please email Roy, attach a 2-page CV, and briefly describe your research interests.","tags":null,"title":"Join Us!","type":"page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"People","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b51ae6a884dde0b7b74cd413e8b1590d","permalink":"/teachinga/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/teachinga/","section":"","summary":"","tags":null,"title":"Teaching","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3081f18a5769f6c057e102ad89f9a122","permalink":"/project/understanding_nlp/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/understanding_nlp/","section":"project","summary":"In recent years, deep learning became the leading machine learning technology in NLP. Despite its wide adoption in NLP, the theory of deep learning lags behind its empirical success, as many engineered systems are in commercial use without a solid scientific basis for their operation. Our research aims to bridge the gap between theory and practice. We devise mathematical theories that link deep neural models to classical NLP models, such as weighted finite-state automata.","tags":null,"title":"Understanding NLP","type":"project"}]