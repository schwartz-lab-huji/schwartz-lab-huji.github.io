[{"authors":["amit_msalum"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"621caab26da2e99efcd97aabc75f0635","permalink":"/author/amit-ben-artzy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/amit-ben-artzy/","section":"authors","summary":"","tags":null,"title":"Amit Ben Artzy","type":"authors"},{"authors":["haya"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"adcebb065719c862bdc8a357c9f5432c","permalink":"/author/haya-riesel/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/haya-riesel/","section":"authors","summary":"","tags":null,"title":"Haya Riesel","type":"authors"},{"authors":["netta"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"54069fe3e2fdb586d324260389505d36","permalink":"/author/netta-madvil/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/netta-madvil/","section":"authors","summary":"","tags":null,"title":"Netta Madvil","type":"authors"},{"authors":["daniel_rotem"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"56649b16302305a68645ea606f0ac58c","permalink":"/author/daniel-rotem/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/daniel-rotem/","section":"authors","summary":"","tags":null,"title":"Daniel Rotem","type":"authors"},{"authors":["yuval_reif_msalum"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2dc3ebc9ba7b490a29f0d99bfff51d3d","permalink":"/author/yuval-reif/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yuval-reif/","section":"authors","summary":"","tags":null,"title":"Yuval Reif","type":"authors"},{"authors":["aviad"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"dacb9909672288cac15084794f711b99","permalink":"/author/aviad-sar-shalom/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/aviad-sar-shalom/","section":"authors","summary":"","tags":null,"title":"Aviad Sar-Shalom","type":"authors"},{"authors":["boaz"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"42aff9df25185b9d3cde950d7e2e8891","permalink":"/author/boaz-beldinger/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/boaz-beldinger/","section":"authors","summary":"","tags":null,"title":"Boaz Beldinger","type":"authors"},{"authors":["lotem_rozner_bcalum"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a7c8c310cbc2c1d27712e5c9bfd034de","permalink":"/author/lotem-rozner/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/lotem-rozner/","section":"authors","summary":"","tags":null,"title":"Lotem Rozner","type":"authors"},{"authors":["michael_hassid_msalum"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5bc527ddd018eaa6ea90a1b4fd314598","permalink":"/author/michael-hassid/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/michael-hassid/","section":"authors","summary":"","tags":null,"title":"Michael Hassid","type":"authors"},{"authors":["Nir Yarden"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3183a3c7e6c1947c43d2a50f6e3b4579","permalink":"/author/nir-yarden/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/nir-yarden/","section":"authors","summary":"","tags":null,"title":"Nir Yarden","type":"authors"},{"authors":["inbal"],"categories":null,"content":"","date":1651708800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1651708800,"objectID":"c975119c378996fc8132674ae733ae53","permalink":"/author/inbal-magar/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/inbal-magar/","section":"authors","summary":"","tags":null,"title":"Inbal Magar","type":"authors"},{"authors":["yuval_arbel"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f9481ebf568782ad895b98d541b168a9","permalink":"/author/yuval-arbel/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yuval-arbel/","section":"authors","summary":"","tags":null,"title":"Yuval Arbel","type":"authors"},{"authors":["yonatanbittonstar"],"categories":null,"content":"","date":1689292800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1689292800,"objectID":"5ed7f6350b56703263054864f12a2f2f","permalink":"/author/a-hrefhttps/yonatanbitton.github.io/-target_blank-relnoopener-noreferreryonatan-bitton/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/yonatanbitton.github.io/-target_blank-relnoopener-noreferreryonatan-bitton/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://yonatanbitton.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eYonatan Bitton\u003c/a\u003e*","type":"authors"},{"authors":["yonatanbitton"],"categories":null,"content":"","date":1689033600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1689033600,"objectID":"b0c7080f22198ed0c7d2341bfe4774af","permalink":"/author/a-hrefhttps/yonatanbitton.github.io/-target_blank-relnoopener-noreferreryonatan-bitton/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/yonatanbitton.github.io/-target_blank-relnoopener-noreferreryonatan-bitton/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://yonatanbitton.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eYonatan Bitton\u003c/a\u003e","type":"authors"},{"authors":["yarden"],"categories":null,"content":"","date":1651708800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1651708800,"objectID":"2b1935f562e2cb2dd7c383a1f9b5496a","permalink":"/author/yarden-tal/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yarden-tal/","section":"authors","summary":"","tags":null,"title":"Yarden Tal","type":"authors"},{"authors":["roital"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"4f94460a2a96a51b0bd55d1d14c46b10","permalink":"/author/roi-tal/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/roi-tal/","section":"authors","summary":"","tags":null,"title":"Roi Tal","type":"authors"},{"authors":["deborah"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3d25a7fa13df6502340d16df765bde29","permalink":"/author/deborah-wolhandler/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/deborah-wolhandler/","section":"authors","summary":"","tags":null,"title":"Deborah Wolhandler","type":"authors"},{"authors":["michael_hassidstar"],"categories":null,"content":"","date":1712016000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1712016000,"objectID":"601de294187138ae097bf8fa559d0dab","permalink":"/author/michael-hassid/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/michael-hassid/","section":"authors","summary":"","tags":null,"title":"Michael Hassid*","type":"authors"},{"authors":["michael_hassid"],"categories":null,"content":"","date":1684540800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1684540800,"objectID":"60214d1bbef393b500513e38ad68c5c2","permalink":"/author/michael-hassid/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/michael-hassid/","section":"authors","summary":"","tags":null,"title":"Michael Hassid","type":"authors"},{"authors":["yuval_reif"],"categories":null,"content":"","date":1737504000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1737504000,"objectID":"8ac5d742ac1117a3fe406055777c288e","permalink":"/author/yuval-reif/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yuval-reif/","section":"authors","summary":"","tags":null,"title":"Yuval Reif","type":"authors"},{"authors":["amit"],"categories":null,"content":"","date":1725753600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1725753600,"objectID":"591850d0d6a4f0d13e6df8ce96b8e76d","permalink":"/author/amit-ben-artzy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/amit-ben-artzy/","section":"authors","summary":"","tags":null,"title":"Amit Ben Artzy","type":"authors"},{"authors":["ido_amos"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"4a10cabd05e343e84242d1977374014b","permalink":"/author/ido-amos/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ido-amos/","section":"authors","summary":"","tags":null,"title":"Ido Amos","type":"authors"},{"authors":["omer_benshahar"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"47430a8d228d40120c8f412dc7800b6a","permalink":"/author/omer-ben-shahar/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/omer-ben-shahar/","section":"authors","summary":"","tags":null,"title":"Omer Ben Shahar","type":"authors"},{"authors":["erez_badash"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3510a03aa4d153ecc6e2c1a70ecbcef2","permalink":"/author/erez-badash/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/erez-badash/","section":"authors","summary":"","tags":null,"title":"Erez Badash","type":"authors"},{"authors":["matanel"],"categories":null,"content":"","date":1737504000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1737504000,"objectID":"2d2a83e6d5ebb1a4acf0d327b5efbf99","permalink":"/author/matanel-oren/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/matanel-oren/","section":"authors","summary":"","tags":null,"title":"Matanel Oren","type":"authors"},{"authors":["matanelstar"],"categories":null,"content":"","date":1705017600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1705017600,"objectID":"18d55d434c2b806f2f4aeed8b65ef264","permalink":"/author/matanel-oren/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/matanel-oren/","section":"authors","summary":"","tags":null,"title":"Matanel Oren*","type":"authors"},{"authors":["maoz"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"49c8e30a7af3b22d71e12490b8fa0f5f","permalink":"/author/maoz-nahum/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/maoz-nahum/","section":"authors","summary":"","tags":null,"title":"Maoz Nahum","type":"authors"},{"authors":["tamer"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3e0f6298e2a534e19d429daa9adbb193","permalink":"/author/tamer-ghattas/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tamer-ghattas/","section":"authors","summary":"","tags":null,"title":"Tamer Ghattas","type":"authors"},{"authors":["guy_kaplan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"68cc0bbf9c756b9f3bf161571d128f87","permalink":"/author/guy-kaplan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/guy-kaplan/","section":"authors","summary":"","tags":null,"title":"Guy Kaplan","type":"authors"},{"authors":["Nir Yarden"],"categories":null,"content":"","date":1705017600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1705017600,"objectID":"a4f7ac7f7b273d0e25148e12f86d84f3","permalink":"/author/nir-yarden/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/nir-yarden/","section":"authors","summary":"","tags":null,"title":"Nir Yarden","type":"authors"},{"authors":["lotem_rozner"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f24a37d11589a6cdad92572b917de9ca","permalink":"/author/lotem-rozner/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/lotem-rozner/","section":"authors","summary":"","tags":null,"title":"Lotem Rozner","type":"authors"},{"authors":["me"],"categories":null,"content":"","date":1740614400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1740614400,"objectID":"f8f9a3384bdb64736380a1b633f3701b","permalink":"/author/broy-schwartz/b/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/broy-schwartz/b/","section":"authors","summary":"","tags":null,"title":"\u003cb\u003eRoy Schwartz\u003c/b\u003e","type":"authors"},{"authors":["noah"],"categories":null,"content":"","date":1715558400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1715558400,"objectID":"26229e118c8ed67fdf60140d2c027cf4","permalink":"/author/a-hrefhttp/homes.cs.washington.edu/~nasmith/-target_blank-relnoopener-noreferrernoah-a.-smith/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/homes.cs.washington.edu/~nasmith/-target_blank-relnoopener-noreferrernoah-a.-smith/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://homes.cs.washington.edu/~nasmith/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eNoah A. Smith\u003c/a\u003e","type":"authors"},{"authors":["tomstar"],"categories":null,"content":"","date":1715558400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1715558400,"objectID":"44935861f5ea371a028c571374ac3ebb","permalink":"/author/a-hrefhttps/tomhoper.github.io/-target_blank-relnoopener-noreferrertom-hope/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/tomhoper.github.io/-target_blank-relnoopener-noreferrertom-hope/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://tomhoper.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eTom Hope\u003c/a\u003e","type":"authors"},{"authors":["iryna"],"categories":null,"content":"","date":1715558400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1715558400,"objectID":"37356973201d1ddb7ed0e0d43c3a1257","permalink":"/author/a-hrefhttps/www.informatik.tu-darmstadt.de/ukp/ukp_home/head_ukp/index.en.jsp-target_blank-relnoopener-noreferreriryna-gurevych/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/www.informatik.tu-darmstadt.de/ukp/ukp_home/head_ukp/index.en.jsp-target_blank-relnoopener-noreferreriryna-gurevych/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://www.informatik.tu-darmstadt.de/ukp/ukp_home/head_ukp/index.en.jsp' target=\"_blank\" rel=\"noopener noreferrer\"\u003eIryna Gurevych\u003c/a\u003e","type":"authors"},{"authors":["orenp"],"categories":null,"content":"","date":1715126400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1715126400,"objectID":"5b359c93c909aa29ead503c665b610fb","permalink":"/author/a-hrefhttps/scholar.google.co.il/citationsuser96pr-j0aaaajhlen-target_blank-relnoopener-noreferrerjonathan-mamou/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/scholar.google.co.il/citationsuser96pr-j0aaaajhlen-target_blank-relnoopener-noreferrerjonathan-mamou/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://scholar.google.co.il/citations?user=96pR-j0AAAAJ\u0026hl=en' target=\"_blank\" rel=\"noopener noreferrer\"\u003eJonathan Mamou\u003c/a\u003e","type":"authors"},{"authors":["orenp"],"categories":null,"content":"","date":1715126400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1715126400,"objectID":"05b80ca510fc6f89b072e81bed8db222","permalink":"/author/a-hrefhttps/www.intel.com/content/www/us/en/artificial-intelligence/bios/oren-pereg.html-target_blank-relnoopener-noreferreroren-pereg/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/www.intel.com/content/www/us/en/artificial-intelligence/bios/oren-pereg.html-target_blank-relnoopener-noreferreroren-pereg/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://www.intel.com/content/www/us/en/artificial-intelligence/bios/oren-pereg.html' target=\"_blank\" rel=\"noopener noreferrer\"\u003eOren Pereg\u003c/a\u003e","type":"authors"},{"authors":["moshew"],"categories":null,"content":"","date":1715126400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1715126400,"objectID":"248a30a7b1c431815d8dbde529cec5e0","permalink":"/author/a-hrefhttps/www.linkedin.com/in/moshe-wasserblat-8977632/originalsubdomainil-target_blank-relnoopener-noreferrermoshe-wasserblat/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/www.linkedin.com/in/moshe-wasserblat-8977632/originalsubdomainil-target_blank-relnoopener-noreferrermoshe-wasserblat/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://www.linkedin.com/in/moshe-wasserblat-8977632/?originalSubdomain=il' target=\"_blank\" rel=\"noopener noreferrer\"\u003eMoshe Wasserblat\u003c/a\u003e","type":"authors"},{"authors":["yossi_adi"],"categories":null,"content":"","date":1712016000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1712016000,"objectID":"c267c3190028d73c22c83cfecbdfdb8c","permalink":"/author/a-hrefhttps/www.cs.huji.ac.il/~adiyoss/-target_blank-relnoopener-noreferreryossi-adi/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/www.cs.huji.ac.il/~adiyoss/-target_blank-relnoopener-noreferreryossi-adi/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://www.cs.huji.ac.il/~adiyoss/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eYossi Adi\u003c/a\u003e","type":"authors"},{"authors":["yossi_adistar"],"categories":null,"content":"","date":1695340800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1695340800,"objectID":"2e738e1955dbd52d59f4644dec4ec962","permalink":"/author/a-hrefhttps/www.cs.huji.ac.il/~adiyoss/-target_blank-relnoopener-noreferreryossi-adi/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/www.cs.huji.ac.il/~adiyoss/-target_blank-relnoopener-noreferreryossi-adi/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://www.cs.huji.ac.il/~adiyoss/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eYossi Adi\u003c/a\u003e*","type":"authors"},{"authors":["mestar"],"categories":null,"content":"","date":1695340800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1695340800,"objectID":"c0b934b66b419af8b3397aee48eb893f","permalink":"/author/broy-schwartz/b/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/broy-schwartz/b/","section":"authors","summary":"","tags":null,"title":"\u003cb\u003eRoy Schwartz\u003c/b\u003e*","type":"authors"},{"authors":["gabi"],"categories":null,"content":"","date":1689292800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1689292800,"objectID":"712728ba8e850c1cb1ff551e8939735c","permalink":"/author/a-hrefhttps/gabrielstanovsky.github.io-target_blank-relnoopener-noreferrergabi-stanovsky/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/gabrielstanovsky.github.io-target_blank-relnoopener-noreferrergabi-stanovsky/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://gabrielstanovsky.github.io' target=\"_blank\" rel=\"noopener noreferrer\"\u003eGabi Stanovsky\u003c/a\u003e","type":"authors"},{"authors":["jesse"],"categories":null,"content":"","date":1687996800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1687996800,"objectID":"20b9fcb3e499237078dd28b702108e90","permalink":"/author/a-hrefhttp/www.cs.cmu.edu/~jessed/-target_blank-relnoopener-noreferrerjesse-dodge/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/www.cs.cmu.edu/~jessed/-target_blank-relnoopener-noreferrerjesse-dodge/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://www.cs.cmu.edu/~jessed/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eJesse Dodge\u003c/a\u003e","type":"authors"},{"authors":["emma"],"categories":null,"content":"","date":1687996800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1687996800,"objectID":"8ace8de30cb258bb197bc16920449706","permalink":"/author/a-hrefhttps/strubell.github.io/-target_blank-relnoopener-noreferreremma-strubell/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/strubell.github.io/-target_blank-relnoopener-noreferreremma-strubell/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://strubell.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eEmma Strubell\u003c/a\u003e","type":"authors"},{"authors":["dafna"],"categories":null,"content":"","date":1668816000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1668816000,"objectID":"8f3a30dbcacdef8ef30c2b8ef811255c","permalink":"/author/a-hrefhttp/www.hyadatalab.com/-target_blank-relnoopener-noreferrerdafna-shahaf/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/www.hyadatalab.com/-target_blank-relnoopener-noreferrerdafna-shahaf/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://www.hyadatalab.com/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eDafna Shahaf\u003c/a\u003e","type":"authors"},{"authors":["hao"],"categories":null,"content":"","date":1665014400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1665014400,"objectID":"216cc62ed48c13143e8a57e65b3b4bae","permalink":"/author/a-hrefhttp/homes.cs.washington.edu/~hapeng/-target_blank-relnoopener-noreferrerhao-peng/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/homes.cs.washington.edu/~hapeng/-target_blank-relnoopener-noreferrerhao-peng/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://homes.cs.washington.edu/~hapeng/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eHao Peng\u003c/a\u003e","type":"authors"},{"authors":["jungo"],"categories":null,"content":"","date":1665014400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1665014400,"objectID":"337ff57af2bc5dcd5f5442fa45006a1d","permalink":"/author/a-hrefhttps/homes.cs.washington.edu/~jkasai/-target_blank-relnoopener-noreferrerjungo-kasai/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/homes.cs.washington.edu/~jkasai/-target_blank-relnoopener-noreferrerjungo-kasai/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://homes.cs.washington.edu/~jkasai/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eJungo Kasai\u003c/a\u003e","type":"authors"},{"authors":["mohitb"],"categories":null,"content":"","date":1658793600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1658793600,"objectID":"96b6c8ac2c904fe7bf2b4fd076afea3a","permalink":"/author/a-hrefhttps/www.cs.unc.edu/~mbansal/-target_blank-relnoopener-noreferrermohit-bansal/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/www.cs.unc.edu/~mbansal/-target_blank-relnoopener-noreferrermohit-bansal/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://www.cs.unc.edu/~mbansal/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eMohit Bansal\u003c/a\u003e","type":"authors"},{"authors":["jesse"],"categories":null,"content":"","date":1649203200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1649203200,"objectID":"c57b5e2ab54bf02ea8d144cf86903a71","permalink":"/author/a-hrefhttps/www.sashaluccioni.com/-target_blank-relnoopener-noreferrersasha-luccioni/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/www.sashaluccioni.com/-target_blank-relnoopener-noreferrersasha-luccioni/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://www.sashaluccioni.com/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eSasha Luccioni\u003c/a\u003e","type":"authors"},{"authors":["dani"],"categories":null,"content":"","date":1633910400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1633910400,"objectID":"bda0dac152a6d360494280ff27e00394","permalink":"/author/a-hrefhttps/dyogatama.github.io/-target_blank-relnoopener-noreferrerdani-yogatama/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/dyogatama.github.io/-target_blank-relnoopener-noreferrerdani-yogatama/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://dyogatama.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eDani Yogatama\u003c/a\u003e","type":"authors"},{"authors":["lingpeng"],"categories":null,"content":"","date":1633910400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1633910400,"objectID":"609631dedfa3e5f65f5b0cff65339144","permalink":"/author/a-hrefhttps/ikekonglp.github.io/-target_blank-relnoopener-noreferrerlingpeng-kong/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/ikekonglp.github.io/-target_blank-relnoopener-noreferrerlingpeng-kong/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://ikekonglp.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eLingpeng Kong\u003c/a\u003e","type":"authors"},{"authors":["nikos"],"categories":null,"content":"","date":1633910400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1633910400,"objectID":"f9d56a04da68ec7e37e0fbf143da0514","permalink":"/author/a-hrefhttps/nik0spapp.github.io/-target_blank-relnoopener-noreferrernikolaos-pappas/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/nik0spapp.github.io/-target_blank-relnoopener-noreferrernikolaos-pappas/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://nik0spapp.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eNikolaos Pappas\u003c/a\u003e","type":"authors"},{"authors":["zhaofeng"],"categories":null,"content":"","date":1633910400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1633910400,"objectID":"9e630d248e9b67e1f44e4bfb3c10161a","permalink":"/author/a-hrefhttps/zhaofengwu.github.io/-target_blank-relnoopener-noreferrerzhaofeng-wu/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/zhaofengwu.github.io/-target_blank-relnoopener-noreferrerzhaofeng-wu/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://zhaofengwu.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eZhaofeng Wu\u003c/a\u003e","type":"authors"},{"authors":["dallas="],"categories":null,"content":"","date":1631145600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1631145600,"objectID":"654bc7dc4aeffca2894e972e22a9b648","permalink":"/author/a-hrefhttp/web.stanford.edu/~dcard/-target_blank-relnoopener-noreferrerdallas-card/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/web.stanford.edu/~dcard/-target_blank-relnoopener-noreferrerdallas-card/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://web.stanford.edu/~dcard/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eDallas Card\u003c/a\u003e","type":"authors"},{"authors":["will"],"categories":null,"content":"","date":1631145600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1631145600,"objectID":"7c4ddaa28ed01844e37fcd570383bed9","permalink":"/author/a-hrefhttps/lambdaviking.com-target_blank-relnoopener-noreferrerwill-merrill/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/lambdaviking.com-target_blank-relnoopener-noreferrerwill-merrill/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://lambdaviking.com' target=\"_blank\" rel=\"noopener noreferrer\"\u003eWill Merrill\u003c/a\u003e","type":"authors"},{"authors":["suchin"],"categories":null,"content":"","date":1631145600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1631145600,"objectID":"ead8ad11b4996c92654d577e09edc581","permalink":"/author/a-hrefhttps/suchin.io/-target_blank-relnoopener-noreferrersuchin-gururangan/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/suchin.io/-target_blank-relnoopener-noreferrersuchin-gururangan/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://suchin.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eSuchin Gururangan\u003c/a\u003e","type":"authors"},{"authors":["yoav"],"categories":null,"content":"","date":1631145600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1631145600,"objectID":"f29984c7f0aea115651c62c115d16a98","permalink":"/author/a-hrefhttps/u.cs.biu.ac.il/~yogo/-target_blank-relnoopener-noreferreryoav-goldberg/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/u.cs.biu.ac.il/~yogo/-target_blank-relnoopener-noreferreryoav-goldberg/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://u.cs.biu.ac.il/~yogo/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eYoav Goldberg\u003c/a\u003e","type":"authors"},{"authors":["vivek"],"categories":null,"content":"","date":1631145600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1631145600,"objectID":"2568dc170cfb7011694508efd2f98f31","permalink":"/author/a-hrefhttps/vkramanuj.github.io/-target_blank-relnoopener-noreferrervivek-ramanujan/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/vkramanuj.github.io/-target_blank-relnoopener-noreferrervivek-ramanujan/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://vkramanuj.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eVivek Ramanujan\u003c/a\u003e","type":"authors"},{"authors":["elhadad"],"categories":null,"content":"","date":1630800000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1630800000,"objectID":"e9ca3ab89b463d6c8bd53a9cb2e21cc6","permalink":"/author/a-hrefhttps/www.cs.bgu.ac.il/~elhadad/-target_blank-relnoopener-noreferrermichael-elhadad/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/www.cs.bgu.ac.il/~elhadad/-target_blank-relnoopener-noreferrermichael-elhadad/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://www.cs.bgu.ac.il/~elhadad/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eMichael Elhadad\u003c/a\u003e","type":"authors"},{"authors":["erich"],"categories":null,"content":"","date":1615507200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1615507200,"objectID":"7edc12ee299954d7ad2934e2f4361d64","permalink":"/author/a-hrefhttp/erichorvitz.com/-target_blank-relnoopener-noreferrereric-horvitz/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/erichorvitz.com/-target_blank-relnoopener-noreferrereric-horvitz/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://erichorvitz.com/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eEric Horvitz\u003c/a\u003e","type":"authors"},{"authors":["aidastar"],"categories":null,"content":"","date":1615507200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1615507200,"objectID":"89628711a6421968b4a8cbe394cf31cc","permalink":"/author/a-hrefhttps/aidaamini.github.io/-target_blank-relnoopener-noreferreraida-amini/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/aidaamini.github.io/-target_blank-relnoopener-noreferreraida-amini/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://aidaamini.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eAida Amini\u003c/a\u003e*","type":"authors"},{"authors":["hannaneh"],"categories":null,"content":"","date":1615507200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1615507200,"objectID":"3d30ca823fc3dad847c76e8f4b82f589","permalink":"/author/a-hrefhttps/homes.cs.washington.edu/~hannaneh/-target_blank-relnoopener-noreferrerhannaneh-hajishirzi/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/homes.cs.washington.edu/~hannaneh/-target_blank-relnoopener-noreferrerhannaneh-hajishirzi/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://homes.cs.washington.edu/~hannaneh/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eHannaneh Hajishirzi\u003c/a\u003e","type":"authors"},{"authors":["tomstar"],"categories":null,"content":"","date":1615507200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1615507200,"objectID":"62876c8b8bf56d332eefb5bff515fed1","permalink":"/author/a-hrefhttps/tomhoper.github.io/-target_blank-relnoopener-noreferrertom-hope/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/tomhoper.github.io/-target_blank-relnoopener-noreferrertom-hope/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://tomhoper.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eTom Hope\u003c/a\u003e*","type":"authors"},{"authors":["danw"],"categories":null,"content":"","date":1615507200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1615507200,"objectID":"91ec39454e5da8074486d833add5f9a9","permalink":"/author/a-hrefhttps/www.cs.washington.edu/people/faculty/weld-target_blank-relnoopener-noreferrerdan-weld/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/www.cs.washington.edu/people/faculty/weld-target_blank-relnoopener-noreferrerdan-weld/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://www.cs.washington.edu/people/faculty/weld' target=\"_blank\" rel=\"noopener noreferrer\"\u003eDan Weld\u003c/a\u003e","type":"authors"},{"authors":["davidwadden"],"categories":null,"content":"","date":1615507200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1615507200,"objectID":"0345d6e594f8aa189a716c16f941d944","permalink":"/author/david-wadden/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/david-wadden/","section":"authors","summary":"","tags":null,"title":"David Wadden","type":"authors"},{"authors":["madeleine"],"categories":null,"content":"","date":1615507200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1615507200,"objectID":"2090b9f54db72db756e71cc95965a518","permalink":"/author/madeleine-van-zuylen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/madeleine-van-zuylen/","section":"authors","summary":"","tags":null,"title":"Madeleine van Zuylen","type":"authors"},{"authors":["yejin"],"categories":null,"content":"","date":1600041600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600041600,"objectID":"7ba101def5f150cbbd01c9959aedd8ec","permalink":"/author/a-hrefhttp/homes.cs.washington.edu/~yejin/-target_blank-relnoopener-noreferreryejin-choi/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/homes.cs.washington.edu/~yejin/-target_blank-relnoopener-noreferreryejin-choi/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://homes.cs.washington.edu/~yejin/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eYejin Choi\u003c/a\u003e","type":"authors"},{"authors":["swabha"],"categories":null,"content":"","date":1600041600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600041600,"objectID":"03408aa46022610095573e9111d359b2","permalink":"/author/a-hrefhttp/www.cs.cmu.edu/~sswayamd/-target_blank-relnoopener-noreferrerswabha-swayamdipta/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/www.cs.cmu.edu/~sswayamd/-target_blank-relnoopener-noreferrerswabha-swayamdipta/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://www.cs.cmu.edu/~sswayamd/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eSwabha Swayamdipta\u003c/a\u003e","type":"authors"},{"authors":["yizhong"],"categories":null,"content":"","date":1600041600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600041600,"objectID":"a03196afe27a59d9a6e6b6a9e15b68c6","permalink":"/author/a-hrefhttps/yizhong-wang.com/-target_blank-relnoopener-noreferreryizhong-wang/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/yizhong-wang.com/-target_blank-relnoopener-noreferreryizhong-wang/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://yizhong-wang.com/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eYizhong Wang\u003c/a\u003e","type":"authors"},{"authors":["nicholas"],"categories":null,"content":"","date":1600041600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1600041600,"objectID":"d6baee863e6a9befe871365b5a4886b4","permalink":"/author/nicholas-lourie/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/nicholas-lourie/","section":"authors","summary":"","tags":null,"title":"Nicholas Lourie","type":"authors"},{"authors":["erany"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1585699200,"objectID":"a3c3393543d270a8475ebbce22b95ebb","permalink":"/author/a-hrefhttp/www.cs.technion.ac.il/~yahave/-target_blank-relnoopener-noreferrereran-yahav/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/www.cs.technion.ac.il/~yahave/-target_blank-relnoopener-noreferrereran-yahav/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://www.cs.technion.ac.il/~yahave/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eEran Yahav\u003c/a\u003e","type":"authors"},{"authors":["gail"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1585699200,"objectID":"9dc7ab2bee66877a4d70ef1735699b7a","permalink":"/author/a-hrefhttps/sgailw.cswp.cs.technion.ac.il-target_blank-relnoopener-noreferrergail-weiss/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/sgailw.cswp.cs.technion.ac.il-target_blank-relnoopener-noreferrergail-weiss/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://sgailw.cswp.cs.technion.ac.il' target=\"_blank\" rel=\"noopener noreferrer\"\u003eGail Weiss\u003c/a\u003e","type":"authors"},{"authors":["dianqi"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1585699200,"objectID":"79a0915003a3ad061ccd8700817acf39","permalink":"/author/dianqi-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/dianqi-li/","section":"authors","summary":"","tags":null,"title":"Dianqi Li","type":"authors"},{"authors":["gabrielh"],"categories":null,"content":"","date":1580515200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1580515200,"objectID":"4891830e203f10bfb7350b059c02a20c","permalink":"/author/a-hrefhttp/gabrielilharco.com-target_blank-relnoopener-noreferrergabriel-ilharco/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/gabrielilharco.com-target_blank-relnoopener-noreferrergabriel-ilharco/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://gabrielilharco.com' target=\"_blank\" rel=\"noopener noreferrer\"\u003eGabriel Ilharco\u003c/a\u003e","type":"authors"},{"authors":["ali"],"categories":null,"content":"","date":1580515200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1580515200,"objectID":"e2ca724dc1a3617350c4ef4bd06a6829","permalink":"/author/a-hrefhttps/homes.cs.washington.edu/~ali/-target_blank-relnoopener-noreferrerali-farhadi/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/homes.cs.washington.edu/~ali/-target_blank-relnoopener-noreferrerali-farhadi/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://homes.cs.washington.edu/~ali/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eAli Farhadi\u003c/a\u003e","type":"authors"},{"authors":["orene"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1561939200,"objectID":"541ff9e257368958f152de67922a7f8d","permalink":"/author/a-hrefhttp/allenai.org/team/orene/-target_blank-relnoopener-noreferreroren-etzioni/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/allenai.org/team/orene/-target_blank-relnoopener-noreferreroren-etzioni/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://allenai.org/team/orene/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eOren Etzioni\u003c/a\u003e","type":"authors"},{"authors":["markn="],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1561939200,"objectID":"82a5029d7d9b639f505e157787bf1d3d","permalink":"/author/a-hrefhttp/markneumann.xyz-target_blank-relnoopener-noreferrermark-neumann/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/markneumann.xyz-target_blank-relnoopener-noreferrermark-neumann/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://markneumann.xyz' target=\"_blank\" rel=\"noopener noreferrer\"\u003eMark Neumann\u003c/a\u003e","type":"authors"},{"authors":["sameer="],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1561939200,"objectID":"6e07ef181703393f8b4b03a1b035f665","permalink":"/author/a-hrefhttp/sameersingh.org-target_blank-relnoopener-noreferrersameer-singh/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/sameersingh.org-target_blank-relnoopener-noreferrersameer-singh/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://sameersingh.org' target=\"_blank\" rel=\"noopener noreferrer\"\u003eSameer Singh\u003c/a\u003e","type":"authors"},{"authors":["jessestar"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1561939200,"objectID":"edf9fbb542bfe92b197fa1b0108104e9","permalink":"/author/a-hrefhttp/www.cs.cmu.edu/~jessed/-target_blank-relnoopener-noreferrerjesse-dodge/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/www.cs.cmu.edu/~jessed/-target_blank-relnoopener-noreferrerjesse-dodge/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://www.cs.cmu.edu/~jessed/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eJesse Dodge\u003c/a\u003e*","type":"authors"},{"authors":["rob="],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1561939200,"objectID":"1e69b33059cc9342e33da675bf1c651a","permalink":"/author/a-hrefhttps/rloganiv.github.io-target_blank-relnoopener-noreferrerrobert-logan/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/rloganiv.github.io-target_blank-relnoopener-noreferrerrobert-logan/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://rloganiv.github.io' target=\"_blank\" rel=\"noopener noreferrer\"\u003eRobert Logan\u003c/a\u003e","type":"authors"},{"authors":["mattp="],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1561939200,"objectID":"6f2b7dd59575de35d40e0a23533cc5ea","permalink":"/author/a-hrefhttps/scholar.google.com/citationsuserk5ncpzwaaaajhlen-target_blank-relnoopener-noreferrermatthew-e.-peters/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/scholar.google.com/citationsuserk5ncpzwaaaajhlen-target_blank-relnoopener-noreferrermatthew-e.-peters/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://scholar.google.com/citations?user=K5nCPZwAAAAJ\u0026hl=en' target=\"_blank\" rel=\"noopener noreferrer\"\u003eMatthew E. Peters\u003c/a\u003e","type":"authors"},{"authors":["vidur="],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1561939200,"objectID":"d0262167f06a23b14f0fd7fc2a1d9257","permalink":"/author/a-hrefhttps/scholar.google.com/citationsusernobf_heaaaaj-target_blank-relnoopener-noreferrervidur-joshi/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/scholar.google.com/citationsusernobf_heaaaaj-target_blank-relnoopener-noreferrervidur-joshi/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://scholar.google.com/citations?user=NobF_hEAAAAJ' target=\"_blank\" rel=\"noopener noreferrer\"\u003eVidur Joshi\u003c/a\u003e","type":"authors"},{"authors":["nelson"],"categories":null,"content":"","date":1554076800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1554076800,"objectID":"7184fd1ca11da8b5027f7b05f2b44d86","permalink":"/author/a-hrefhttps/cs.stanford.edu/~nfliu/-target_blank-relnoopener-noreferrernelson-f.-liu/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/cs.stanford.edu/~nfliu/-target_blank-relnoopener-noreferrernelson-f.-liu/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://cs.stanford.edu/~nfliu/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eNelson F. Liu\u003c/a\u003e","type":"authors"},{"authors":["rowan"],"categories":null,"content":"","date":1533081600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1533081600,"objectID":"d3750635a3b8746ef003d5679b039abb","permalink":"/author/a-hrefhttp/rowanzellers.com/-target_blank-relnoopener-noreferrerrowan-zellers/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/rowanzellers.com/-target_blank-relnoopener-noreferrerrowan-zellers/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://rowanzellers.com/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eRowan Zellers\u003c/a\u003e","type":"authors"},{"authors":["sam"],"categories":null,"content":"","date":1533081600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1533081600,"objectID":"e4e71fbcd126139d55a6bddb1c8f65b1","permalink":"/author/a-hrefhttp/samthomson.com/-target_blank-relnoopener-noreferrersam-thomson/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/samthomson.com/-target_blank-relnoopener-noreferrersam-thomson/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://samthomson.com/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eSam Thomson\u003c/a\u003e","type":"authors"},{"authors":["yonatan"],"categories":null,"content":"","date":1533081600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1533081600,"objectID":"464fa4fe4322425e77d3928c6b48ed77","permalink":"/author/a-hrefhttp/yonatanbisk.com/-target_blank-relnoopener-noreferreryonatan-bisk/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/yonatanbisk.com/-target_blank-relnoopener-noreferreryonatan-bisk/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://yonatanbisk.com/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eYonatan Bisk\u003c/a\u003e","type":"authors"},{"authors":["samstar"],"categories":null,"content":"","date":1525132800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1525132800,"objectID":"06cde61cbe9f1c0c7d3a6c111cd6dab7","permalink":"/author/a-hrefhttp/samthomson.com/-target_blank-relnoopener-noreferrersam-thomson/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/samthomson.com/-target_blank-relnoopener-noreferrersam-thomson/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://samthomson.com/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eSam Thomson\u003c/a\u003e*","type":"authors"},{"authors":["chenhao"],"categories":null,"content":"","date":1525132800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1525132800,"objectID":"ce8099c085eb4f38ff57db08388e4a12","permalink":"/author/a-hrefhttps/chenhaot.com/-target_blank-relnoopener-noreferrerchenhao-tan/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/chenhaot.com/-target_blank-relnoopener-noreferrerchenhao-tan/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://chenhaot.com/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eChenhao Tan\u003c/a\u003e","type":"authors"},{"authors":["omer"],"categories":null,"content":"","date":1525132800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1525132800,"objectID":"bc4bcb920a0d31cd4b7f0eff18fd6517","permalink":"/author/a-hrefhttps/levyomer.wordpress.com/-target_blank-relnoopener-noreferreromer-levy/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/levyomer.wordpress.com/-target_blank-relnoopener-noreferreromer-levy/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://levyomer.wordpress.com/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eOmer Levy\u003c/a\u003e","type":"authors"},{"authors":["bhavana"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1522540800,"objectID":"3a640323d6060c987b5c6cf2096872aa","permalink":"/author/a-hrefhttp/allenai.org/team/bhavanad/-target_blank-relnoopener-noreferrerbhavana-dalvi/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/allenai.org/team/bhavanad/-target_blank-relnoopener-noreferrerbhavana-dalvi/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://allenai.org/team/bhavanad/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eBhavana Dalvi\u003c/a\u003e","type":"authors"},{"authors":["dongyeop"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1522540800,"objectID":"2b09f815c215a56588b131014a9ed27a","permalink":"/author/a-hrefhttp/www.cs.cmu.edu/~dongyeok/-target_blank-relnoopener-noreferrerdongyeop-kang/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/www.cs.cmu.edu/~dongyeok/-target_blank-relnoopener-noreferrerdongyeop-kang/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://www.cs.cmu.edu/~dongyeok/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eDongyeop Kang\u003c/a\u003e","type":"authors"},{"authors":["ed"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1522540800,"objectID":"839353db5a8f9e9e14bbaddf22173466","permalink":"/author/a-hrefhttp/www.cs.cmu.edu/~hovy/-target_blank-relnoopener-noreferrereduard-hovy/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/www.cs.cmu.edu/~hovy/-target_blank-relnoopener-noreferrereduard-hovy/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://www.cs.cmu.edu/~hovy/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eEduard Hovy\u003c/a\u003e","type":"authors"},{"authors":["swabhastar"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1522540800,"objectID":"b5d4592c8b6c16e2f1ecfff660cfc067","permalink":"/author/a-hrefhttp/www.cs.cmu.edu/~sswayamd/-target_blank-relnoopener-noreferrerswabha-swayamdipta/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/www.cs.cmu.edu/~sswayamd/-target_blank-relnoopener-noreferrerswabha-swayamdipta/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://www.cs.cmu.edu/~sswayamd/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eSwabha Swayamdipta\u003c/a\u003e*","type":"authors"},{"authors":["suchinstar"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1522540800,"objectID":"fa34c25f6acfb0b83697c1a85ebf65b6","permalink":"/author/a-hrefhttps/suchin.io/-target_blank-relnoopener-noreferrersuchin-gururangan/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/suchin.io/-target_blank-relnoopener-noreferrersuchin-gururangan/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://suchin.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eSuchin Gururangan\u003c/a\u003e*","type":"authors"},{"authors":["waleed"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1522540800,"objectID":"9c68b4bb6737210793b2c8d1024633c2","permalink":"/author/a-hrefhttps/wammar.github.io/-target_blank-relnoopener-noreferrerwaleed-ammar/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/wammar.github.io/-target_blank-relnoopener-noreferrerwaleed-ammar/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://wammar.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eWaleed Ammar\u003c/a\u003e","type":"authors"},{"authors":["samb"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1522540800,"objectID":"318404fcdcf96a6ffb449c2ac0ab0500","permalink":"/author/a-hrefhttps/www.nyu.edu/projects/bowman/-target_blank-relnoopener-noreferrersam-bowman/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/www.nyu.edu/projects/bowman/-target_blank-relnoopener-noreferrersam-bowman/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://www.nyu.edu/projects/bowman/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eSam Bowman\u003c/a\u003e","type":"authors"},{"authors":["sebastian"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1522540800,"objectID":"d68d186b5437c0054f698afb12d818b3","permalink":"/author/sebastian-kohlmeier/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/sebastian-kohlmeier/","section":"authors","summary":"","tags":null,"title":"Sebastian Kohlmeier","type":"authors"},{"authors":["maarten"],"categories":null,"content":"","date":1485907200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1485907200,"objectID":"09aa5d76f9c7c34a3cb468bb08cf92f9","permalink":"/author/a-hrefhttp/homes.cs.washington.edu/~msap/-target_blank-relnoopener-noreferrermaarten-sap/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/homes.cs.washington.edu/~msap/-target_blank-relnoopener-noreferrermaarten-sap/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://homes.cs.washington.edu/~msap/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eMaarten Sap\u003c/a\u003e","type":"authors"},{"authors":["roi"],"categories":null,"content":"","date":1485907200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1485907200,"objectID":"53b2c6a46ac812374e24a6a8bac0ec65","permalink":"/author/a-hrefhttp/ie.technion.ac.il/~roiri/-target_blank-relnoopener-noreferrerroi-reichart/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/ie.technion.ac.il/~roiri/-target_blank-relnoopener-noreferrerroi-reichart/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://ie.technion.ac.il/~roiri/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eRoi Reichart\u003c/a\u003e","type":"authors"},{"authors":["annak"],"categories":null,"content":"","date":1485907200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1485907200,"objectID":"8e70e0f286c4d498511c6e2df1dde685","permalink":"/author/a-hrefhttp/www.cl.cam.ac.uk/~alk23/-target_blank-relnoopener-noreferreranna-korhonen/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/www.cl.cam.ac.uk/~alk23/-target_blank-relnoopener-noreferreranna-korhonen/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://www.cl.cam.ac.uk/~alk23/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eAnna Korhonen\u003c/a\u003e","type":"authors"},{"authors":["ari"],"categories":null,"content":"","date":1485907200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1485907200,"objectID":"aa7925a00cffabb64a80be8f9b2b7a95","permalink":"/author/a-hrefhttp/www.cs.huji.ac.il/~arir/-target_blank-relnoopener-noreferrerari-rappoport/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/www.cs.huji.ac.il/~arir/-target_blank-relnoopener-noreferrerari-rappoport/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://www.cs.huji.ac.il/~arir/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eAri Rappoport\u003c/a\u003e","type":"authors"},{"authors":["yannis"],"categories":null,"content":"","date":1485907200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1485907200,"objectID":"1f98fb23db447d97730d8a349b745bdb","permalink":"/author/a-hrefhttp/www.ikonstas.net/-target_blank-relnoopener-noreferreryannis-konstas/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/www.ikonstas.net/-target_blank-relnoopener-noreferreryannis-konstas/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://www.ikonstas.net/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eYannis Konstas\u003c/a\u003e","type":"authors"},{"authors":["ivan"],"categories":null,"content":"","date":1485907200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1485907200,"objectID":"d083a01ef83d2c90e2ff0e0436ad5c23","permalink":"/author/a-hrefhttps/sites.google.com/site/ivanvulic/-targetblankivan-vulic/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/sites.google.com/site/ivanvulic/-targetblankivan-vulic/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://sites.google.com/site/ivanvulic/' target='blank'\u003eIvan Vuli\u003c/a\u003e","type":"authors"},{"authors":["lizilles"],"categories":null,"content":"","date":1485907200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1485907200,"objectID":"4ed9a47c39a25ac68ec93dc12ad71a2d","permalink":"/author/li-zilles/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/li-zilles/","section":"authors","summary":"","tags":null,"title":"Li Zilles","type":"authors"},{"authors":["dana"],"categories":null,"content":"","date":1435708800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1435708800,"objectID":"d62f511fb676ebed5efc144c0efe502b","permalink":"/author/dana-rubinstein/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/dana-rubinstein/","section":"authors","summary":"","tags":null,"title":"Dana Rubinstein","type":"authors"},{"authors":["effi"],"categories":null,"content":"","date":1435708800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1435708800,"objectID":"5211333d848210d45e42b0d089d2edb4","permalink":"/author/effi-levi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/effi-levi/","section":"authors","summary":"","tags":null,"title":"Effi Levi","type":"authors"},{"authors":["koppel"],"categories":null,"content":"","date":1372636800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1372636800,"objectID":"ccc402d91e3e8dd0c6118f5b88a0727f","permalink":"/author/a-hrefhttp/u.cs.biu.ac.il/~koppel/-target_blank-relnoopener-noreferrermoshe-koppel/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/u.cs.biu.ac.il/~koppel/-target_blank-relnoopener-noreferrermoshe-koppel/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://u.cs.biu.ac.il/~koppel/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eMoshe Koppel\u003c/a\u003e","type":"authors"},{"authors":["orent"],"categories":null,"content":"","date":1372636800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1372636800,"objectID":"67b1ba841b9254d03f776cee66088505","permalink":"/author/a-hrefhttp/www.ise.bgu.ac.il/orentsur/-target_blank-relnoopener-noreferreroren-tsur/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/www.ise.bgu.ac.il/orentsur/-target_blank-relnoopener-noreferreroren-tsur/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://www.ise.bgu.ac.il/OrenTsur/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eOren Tsur\u003c/a\u003e","type":"authors"},{"authors":["omri"],"categories":null,"content":"","date":1341100800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1341100800,"objectID":"9c2eae572b3ce680465db88ea3063b64","permalink":"/author/a-hrefhttp/www.cs.huji.ac.il/~oabend/-target_blank-relnoopener-noreferreromri-abend/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/www.cs.huji.ac.il/~oabend/-target_blank-relnoopener-noreferreromri-abend/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://www.cs.huji.ac.il/~oabend/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eOmri Abend\u003c/a\u003e","type":"authors"},{"authors":["lab"],"categories":null,"content":"Roy Schwartz's lab at the School of Computer Science and Engineering at the The Hebrew University of Jerusalem studies Natural Language Processing (NLP). Our research is driven towards making text understanding technology widely accessibleto doctors, to teachers, to researchers or even to curious teenagers. To be broadly adopted, NLP technology needs to not only be accurate, but also reliable; models should provide explanations for their outputs; and the methods we use to evaluate them need to be convincing. Our lab also studies methods to make NLP technology more efficient and green, in order to decrease the environmental impact of the field, as well as lower the cost of AI research in order to broaden participation in it.  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"fe1f51c425d0e91980707c007b293282","permalink":"/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"Roy Schwartz's lab at the School of Computer Science and Engineering at the The Hebrew University of Jerusalem studies Natural Language Processing (NLP). Our research is driven towards making text understanding technology widely accessibleto doctors, to teachers, to researchers or even to curious teenagers.","tags":null,"title":"","type":"authors"},{"authors":["home"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ad5e25f4be46d7ea52b4406d1eccc2e1","permalink":"/author/a-hrefhttp/www.cs.huji.ac.il/~roys02/-target_blank-relnoopener-noreferrer/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttp/www.cs.huji.ac.il/~roys02/-target_blank-relnoopener-noreferrer/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='http://www.cs.huji.ac.il/~roys02/' target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003c/a\u003e","type":"authors"},{"authors":["gabis"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6316f613fc7d879429c05fee6af329a5","permalink":"/author/a-hrefhttps/gabrielstanovsky.github.io/-target_blank-relnoopener-noreferrergabriel-stanovsky/a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/a-hrefhttps/gabrielstanovsky.github.io/-target_blank-relnoopener-noreferrergabriel-stanovsky/a/","section":"authors","summary":"","tags":null,"title":"\u003ca href='https://gabrielstanovsky.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eGabriel Stanovsky\u003c/a\u003e","type":"authors"},{"authors":["admin"],"categories":null,"content":"Bio Roy Schwartz is an associate professor at the School of Computer Science and Engineering at The Hebrew University of Jerusalem. Prior to that Roy was a postdoc (2016-2019) and then a research scientist (2019-2020) at the AllenNLP team at the Allen institute for AI and at the School of Computer Science and Engineering at The University of Washington, where he was working with Noah A. Smith. Roy completed his Ph.D. in 2016 at the School of Computer Science and Engineering at The Hebrew University of Jerusalem, where he worked with Ari Rappoport. In 2011, he graduated his masters degree (magna cum laude) in computer science, under the supervision of Prof. Ari Rappoport. Prior to that, he studied computer science and cognitive sciences at the Hebrew University, and completed his B.Sc. (magna cum laude) in 2008. He was a member of the Amirim program for outstanding undergraduate students. In 2004-2005, he was a software engineer at Check Point Ltd.  Teaching  Introduction to Machine Learning (24/25, 23/24) Advanced Natural Language Processing (24/25, 23/24, 22/23) Efficient Natural Language Processing: Reading Papers through Role Playing (24/25, 22/23, 21/22). Please read this document before signing up for this class! Seminar on Natural Language Processing (20/21) Advanced Practical Machine Learning (20/21) Object Oriented Programming; Israeli Council of Higher Education Program for Online Digital Learning (2018\u0026ndash;2021) Guest talk on distributional semantics at UW NLP course (spring 2017, slides, video) UW-NLP RNN Reading Group (spring 2017) UW-NLP Discourse Reading Group (winter 2017) Lecturer of the huji coursera online version of Introduction to Object Oriented Programming Lecturer of Introduction to Object Oriented Programming (13/14, 11/12 [Ranked #1 in student survey!], 10/11, 09/10) Lecturer of Computer Laboratory in Data Structures (12/13)  Lecturer of Introduction to Programming in the Perl Language (07/08)  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/roy-schwartz/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/roy-schwartz/","section":"authors","summary":"Bio Roy Schwartz is an associate professor at the School of Computer Science and Engineering at The Hebrew University of Jerusalem. Prior to that Roy was a postdoc (2016-2019) and then a research scientist (2019-2020) at the AllenNLP team at the Allen institute for AI and at the School of Computer Science and Engineering at The University of Washington, where he was working with Noah A.","tags":null,"title":"Roy Schwartz","type":"authors"},{"authors":["roy_hebrew"],"categories":null,"content":"                     (Natural Language Processing).           \u0026ndash; , , ,   .     ,       ,   ;      ,      .         ,  ,        ,              .   \u0026ndash;             .      (2016-2019)    (2019-2020)           ,       .        2016           .  2011         ,      .  ,            .       . -2004-2005        .             ( )     ,  2023 ( -1:00:30)         ( )         ,  2023 ( -1:29:36)               ,  2023          ,  2023          ,  2019    -Ynet       ,  2019        ,  2020           -   , 2023             , 2023                  ,  2021 (  41:40)  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"1349cb72a98b0b8fed74f5e5780d4186","permalink":"/author/%D7%A8%D7%95%D7%A2%D7%99-%D7%A9%D7%95%D7%95%D7%A8%D7%A5/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%D7%A8%D7%95%D7%A2%D7%99-%D7%A9%D7%95%D7%95%D7%A8%D7%A5/","section":"authors","summary":"                     (Natural Language Processing).           \u0026ndash; , , ,   .","tags":null,"title":" ","type":"authors"},{"authors":["Tamer Ghattas","Michael Hassid","\u003cb\u003eRoy Schwartz\u003c/b\u003e"],"categories":null,"content":"","date":1740614400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1740614400,"objectID":"59981aa28c4a4d25d0ebc306f76b9ffb","permalink":"/publication/on_pruning_state-space/","publishdate":"2025-02-27T00:00:00Z","relpermalink":"/publication/on_pruning_state-space/","section":"publication","summary":"Recent work proposed state-space models (SSMs) as an efficient alternative to transformer-based LLMs. Can these models be pruned to further reduce their computation costs? We adapt several pruning methods to the SSM structure, and apply them to four SSM-based LLMs across multiple tasks. We find that such models are quite robust to some pruning methods (e.g. WANDA), while using other methods lead to fast performance degradation.","tags":["greenai"],"title":"On Pruning State-Space LLMs","type":"publication"},{"authors":["Nadav Timor","Jonathan Mamou","Daniel Korat","Moshe Berchansky","Oren Pereg","Gaurav Jain","\u003cb\u003eRoy Schwartz\u003c/b\u003e","Moshe Wasserblat","David Harel"],"categories":null,"content":"","date":1739664000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1739664000,"objectID":"d3c5c0f7c4e341497ad28746d5648d3a","permalink":"/publication/accelerating_llm_inference/","publishdate":"2025-02-16T00:00:00Z","relpermalink":"/publication/accelerating_llm_inference/","section":"publication","summary":"Accelerating the inference of large language models (LLMs) is a critical challenge in generative AI. Speculative decoding (SD) methods offer substantial efficiency gains by generating multiple tokens using a single target forward pass. However, existing SD approaches require the drafter and target models to share the same vocabulary, thus limiting the pool of possible drafters, often necessitating the training of a drafter from scratch. We present three new SD methods that remove this shared-vocabulary constraint. All three methods preserve the target distribution (i.e., they are lossless) and work with off-the-shelf models without requiring additional training or modifications. Empirically, on summarization, programming, and long-context tasks, our algorithms achieve significant speedups over standard autoregressive decoding. By enabling any off-the-shelf model to serve as drafter and requiring no retraining, this work substantially broadens the applicability of the SD framework in practice.","tags":["greenai"],"title":"Accelerating LLM Inference with Lossless Speculative Decoding Algorithms for Heterogeneous Vocabularies","type":"publication"},{"authors":["Guy Kaplan","Matanel Oren","Yuval Reif","\u003cb\u003eRoy Schwartz\u003c/b\u003e"],"categories":null,"content":"","date":1737504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504000,"objectID":"3d538f85e82a282bb4242f2ba0c47987","permalink":"/publication/token2word/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/publication/token2word/","section":"publication","summary":"Natural language is composed of words, but modern LLMs process sub-words as input. A natural question raised by this discrepancy is whether LLMs encode words internally, and if so how. We present evidence that LLMs engage in an intrinsic detokenization process, where sub-word sequences are combined into coherent word representations. Our experiments show that this process takes place primarily within the early and middle layers of the model. They also show that it is robust to non-morphemic splits, typos and perhaps importantly\u0026mdash;to out-of-vocabulary words: when feeding the inner representation of such words to the model as input vectors, it can 'understand' them despite never seeing them during training. Our findings suggest that LLMs maintain a latent vocabulary beyond the tokenizer's scope. These insights provide a practical, finetuning-free application for expanding the vocabulary of pre-trained models. By enabling the addition of new vocabulary words, we reduce input length and inference iterations, which reduces both space and model latency, with little to no loss in model accuracy.","tags":["understanding_models"],"title":"From Tokens to Words: on the Inner Lexicon of LLMs","type":"publication"},{"authors":["Amit Ben Artzy","\u003cb\u003eRoy Schwartz\u003c/b\u003e"],"categories":null,"content":"","date":1725753600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1725753600,"objectID":"677579bf1db36a6f709efb89a0da4196","permalink":"/publication/attend_first/","publishdate":"2024-09-08T00:00:00Z","relpermalink":"/publication/attend_first/","section":"publication","summary":"In decoder-based LLMs, the representation of a given layer serves two purposes: as input to the next layer during the computation of the current token; and as input to the attention mechanism of future tokens. In this work, we show that the importance of the latter role might be overestimated. To show that, we start by manipulating the representations of previous tokens; e.g. by replacing the hidden states at some layer k with random vectors. Our experimenting with four LLMs and four tasks show that this operation often leads to small to negligible drop in performance. Importantly, this happens if the manipulation occurs in the top part of the model\u0026mdash;k is in the final 30-50% of the layers. In contrast, doing the same manipulation in earlier layers might lead to chance level performance. We continue by switching the hidden state of certain tokens with hidden states of other tokens from another prompt; e.g., replacing the word \"Italy\" with \"France\" in \"What is the capital of Italy?\". We find that when applying this switch in the top 1\u0026frasl;3 of the model, the model ignores it (answering \"Rome\"). However if we apply it before, the model conforms to the switch (\"Paris\"). Our results hint at a two stage process in transformer-based LLMs: the first part gathers input from previous tokens, while the second mainly processes that information internally.","tags":["understanding_models"],"title":"Attend First, Consolidate Later: On the Importance of Attention in Different LLM Layers","type":"publication"},{"authors":["Ilia Kuznetsov","Osama Mohammed Afzal","Koen Dercksen","Nils Dycke","Alexander Goldberg","\u003ca href='https://tomhoper.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eTom Hope\u003c/a\u003e","Dirk Hovy","Jonathan K. Kummerfeld","Anne Lauscher","Kevin Leyton-Brown","Sheng Lu","Mausam","Margot Mieskes","Aurlie Nvol","Danish Pruthi","Lizhen Qu","\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='http://homes.cs.washington.edu/~nasmith/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eNoah A. Smith\u003c/a\u003e","Thamar Solorio","Jingyan Wang","Xiaodan Zhu","Anna Rogers","Nihar B. Shah","\u003ca href='https://www.informatik.tu-darmstadt.de/ukp/ukp_home/head_ukp/index.en.jsp' target=\"_blank\" rel=\"noopener noreferrer\"\u003eIryna Gurevych\u003c/a\u003e"],"categories":null,"content":"","date":1715558400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1715558400,"objectID":"d930989d8d32bbe9afced792bb654b8e","permalink":"/publication/what_can_natural/","publishdate":"2024-05-13T00:00:00Z","relpermalink":"/publication/what_can_natural/","section":"publication","summary":"The number of scientific articles produced every year is growing rapidly. Providing quality control over them is crucial for scientists and, ultimately, for the public good. In modern science, this process is largely delegated to peer review -- a distributed procedure in which each submission is evaluated by several independent experts in the field. Peer review is widely used, yet it is hard, time-consuming, and prone to error. Since the artifacts involved in peer review -- manuscripts, reviews, discussions -- are largely text-based, Natural Language Processing has great potential to improve reviewing. As the emergence of large language models (LLMs) has enabled NLP assistance for many new tasks, the discussion on machine-assisted peer review is picking up the pace. Yet, where exactly is help needed, where can NLP help, and where should it stand aside? The goal of our paper is to provide a foundation for the future efforts in NLP for peer-reviewing assistance. We discuss peer review as a general process, exemplified by reviewing at AI conferences. We detail each step of the process from manuscript submission to camera-ready revision, and discuss the associated challenges and opportunities for NLP assistance, illustrated by existing work. We then turn to the big challenges in NLP for peer review as a whole, including data acquisition and licensing, operationalization and experimentation, and ethical issues. To help consolidate community efforts, we create a companion repository that aggregates key datasets pertaining to peer review. Finally, we issue a detailed call for action for the scientific community, NLP and AI researchers, policymakers, and funding bodies to help bring the research in NLP for peer review forward. We hope that our work will help set the agenda for research in machine-assisted scientific quality control in the age of AI, within the NLP community and beyond.","tags":null,"title":"What Can Natural Language Processing Do for Peer Review?","type":"publication"},{"authors":["\u003ca href='https://scholar.google.co.il/citations?user=96pR-j0AAAAJ\u0026hl=en' target=\"_blank\" rel=\"noopener noreferrer\"\u003eJonathan Mamou\u003c/a\u003e","\u003ca href='https://www.intel.com/content/www/us/en/artificial-intelligence/bios/oren-pereg.html' target=\"_blank\" rel=\"noopener noreferrer\"\u003eOren Pereg\u003c/a\u003e","Daniel Korat","Moshe Berchansky","Nadav Timor","\u003ca href='https://www.linkedin.com/in/moshe-wasserblat-8977632/?originalSubdomain=il' target=\"_blank\" rel=\"noopener noreferrer\"\u003eMoshe Wasserblat\u003c/a\u003e","\u003cb\u003eRoy Schwartz\u003c/b\u003e"],"categories":null,"content":"","date":1715126400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1715126400,"objectID":"0c686123fdc9eaba5e80ae270ee97668","permalink":"/publication/accelerating_speculative/","publishdate":"2024-05-08T00:00:00Z","relpermalink":"/publication/accelerating_speculative/","section":"publication","summary":"Speculative decoding is a promising method for reducing the inference latency of large language models. The effectiveness of the method depends on the speculation length (SL) - the number of tokens generated by the draft model at each iteration. The vast majority of speculative decoding approaches use the same SL for all iterations. In this work, we show that this practice is suboptimal. We introduce DISCO, a DynamIc SpeCulation length Optimization method that uses a classifier to dynamically adjust the SL at each iteration, while provably preserving the decoding quality. Experiments with four benchmarks demonstrate average speedup gains of 10.3% relative to our best baselines.","tags":["greenai"],"title":"Accelerating Speculative Decoding using Dynamic Speculation Length","type":"publication"},{"authors":["Michael Hassid*","Tal Remez*","Jonas Gehring","\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='https://www.cs.huji.ac.il/~adiyoss/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eYossi Adi\u003c/a\u003e"],"categories":null,"content":"","date":1712016000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1712016000,"objectID":"1d530ee9e5f5d2ddbc53ca599da0002f","permalink":"/publication/larger_better/","publishdate":"2024-04-02T00:00:00Z","relpermalink":"/publication/larger_better/","section":"publication","summary":"It is a common belief that large language models (LLMs) are better than smaller-sized ones. However, larger models also require significantly more time and compute during inference. This begs the question: what happens when both models operate under the same budget? (e.g., compute, run-time). To address this question, we analyze code generation LLMs of various sizes and make comparisons such as running a 70B model once vs. generating five outputs from a 13B model and selecting one. Our findings reveal that, in a standard unit-test setup, the repeated use of smaller models can yield consistent improvements, with gains of up to 15% across five tasks. On the other hand, in scenarios where unit-tests are unavailable, a ranking-based selection of candidates from the smaller model falls short of the performance of a single output from larger ones. Our results highlight the potential of using smaller models instead of larger ones, and the importance of studying approaches for ranking LLM outputs.","tags":["greenai","multimodality"],"title":"The Larger the Better? Improved LLM Code-Generation via Budget Reallocation","type":"publication"},{"authors":["Yuval Reif","\u003cb\u003eRoy Schwartz\u003c/b\u003e"],"categories":null,"content":"","date":1710288000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1710288000,"objectID":"de9e426447d9fe9d24f16afb5427f473","permalink":"/publication/label_bias/","publishdate":"2024-03-13T00:00:00Z","relpermalink":"/publication/label_bias/","section":"publication","summary":"Large language models (LLMs) have shown remarkable adaptability to diverse tasks, by leveraging context prompts containing instructions, or minimal input-output examples. However, recent work revealed they also exhibit label bias---an undesirable preference toward predicting certain answers over others. Still, detecting and measuring this bias reliably and at scale has remained relatively unexplored. In this study, we evaluate different approaches to quantifying label bias in a model's predictions, conducting a comprehensive investigation across 279 classification tasks and ten LLMs. Our investigation reveals substantial label bias in models both before and after debiasing attempts, as well as highlights the importance of outcomes-based evaluation metrics, which were not previously used in this regard. We further propose a novel label bias calibration method tailored for few-shot prompting, which outperforms recent calibration approaches for both improving performance and mitigating label bias. Our results emphasize that label bias in the predictions of LLMs remains a barrier to their reliability.","tags":["improved_evaluation"],"title":"Beyond Performance: Quantifying and Mitigating Label Bias in LLMs","type":"publication"},{"authors":["Matanel Oren*","Michael Hassid*","Nir Yarden","\u003ca href='https://www.cs.huji.ac.il/~adiyoss/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eYossi Adi\u003c/a\u003e","\u003cb\u003eRoy Schwartz\u003c/b\u003e"],"categories":null,"content":"","date":1705017600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705017600,"objectID":"1d1c041ee9fc6278880e0d2083105804","permalink":"/publication/tova/","publishdate":"2024-01-12T00:00:00Z","relpermalink":"/publication/tova/","section":"publication","summary":"Transformers are considered conceptually different from the previous generation of state-of-the-art NLP models\u0026mdash;recurrent neural networks (RNNs). In this work, we demonstrate that decoder-only transformers can in fact be conceptualized as unbounded multi-state RNNs\u0026mdash;an RNN variant with unlimited hidden state size. We further show that transformers can be converted into bounded multi-state RNNs by fixing the size of their hidden state, effectively compressing their key-value cache. We introduce a novel, training-free compression policy\u0026mdash;Token Omission Via Attention (TOVA). Our experiments with four long range tasks and several LLMs show that TOVA outperforms several baseline compression policies. Particularly, our results are nearly on par with the full model, using in some cases only 1\u0026frasl;8 of the original cache size, which translates to 4.8X higher throughput. Our results shed light on the connection between transformers and RNNs, and help mitigate one of LLMs' most painful computational bottlenecks - the size of their key-value cache. We publicly release our code.","tags":["greenai","understanding_models"],"title":"Transformers are Multi-State RNNs","type":"publication"},{"authors":["Michael Hassid*","Tal Remez*","Tu Anh Nguyen","Itai Gat","Alexis Conneau","Felix Kreuk","Jade Copet","Alexandre Defossez","Gabriel Synnaeve","Emmanuel Dupoux","\u003cb\u003eRoy Schwartz\u003c/b\u003e*","\u003ca href='https://www.cs.huji.ac.il/~adiyoss/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eYossi Adi\u003c/a\u003e*"],"categories":null,"content":"","date":1695340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695340800,"objectID":"a22a79f9f1a2478a00461aeb8cbf187c","permalink":"/publication/twist/","publishdate":"2023-09-22T00:00:00Z","relpermalink":"/publication/twist/","section":"publication","summary":"Speech language models (SpeechLMs) process and generate acoustic data only, without textual supervision. In this work, we propose TWIST, a method for training SpeechLMs using a warm-start from a pretrained textual language models. We show using both automatic and human evaluations that TWIST outperforms a cold-start SpeechLM across the board. We empirically analyze the effect of different model design choices such as the speech tokenizer, the pretrained textual model, and the dataset size. We find that model and dataset scale both play an important role in constructing better-performing SpeechLMs. Based on our observations, we present the largest (to the best of our knowledge) SpeechLM both in terms of number of parameters and training data. We additionally introduce two spoken versions of the StoryCloze textual benchmark to further improve model evaluation and advance future research in the field. Speech samples can be found on our website: this https URL.","tags":["multimodality"],"title":"Textually Pretrained Speech Language Models","type":"publication"},{"authors":null,"categories":null,"content":"","date":1690848000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690848000,"objectID":"4f0361d9144b7e260c435edb9cad1415","permalink":"/talk/role_of_data/","publishdate":"2023-08-01T00:00:00Z","relpermalink":"/talk/role_of_data/","section":"talk","summary":"Google Deepmind, NLP seminar","tags":null,"title":"The Role of Data in Building Robust Models","type":"talk"},{"authors":["Nitzan Bitton Guetta*","\u003ca href='https://yonatanbitton.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eYonatan Bitton\u003c/a\u003e*","Jack Hassel","Ludwig Schmidt","Yuval Elovici","\u003ca href='https://gabrielstanovsky.github.io' target=\"_blank\" rel=\"noopener noreferrer\"\u003eGabi Stanovsky\u003c/a\u003e","\u003cb\u003eRoy Schwartz\u003c/b\u003e"],"categories":null,"content":"","date":1689292800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689292800,"objectID":"7b42a7fcc48940a1539beb7e6ef4fdc0","permalink":"/publication/whoops/","publishdate":"2023-03-10T00:00:00Z","relpermalink":"/publication/whoops/","section":"publication","summary":"Weird, unusual, and uncanny images pique the curiosity of observers because they challenge commonsense. For example, an image released during the 2022 world cup depicts the famous soccer stars Lionel Messi and Cristiano Ronaldo playing chess, which playfully violates our expectation that their competition should occur on the football field. Humans can easily recognize and interpret these unconventional images, but can AI models do the same? We introduce WHOOPS!, a new dataset and benchmark for visual commonsense. The dataset is comprised of purposefully commonsense-defying images created by designers using publicly-available image generation tools like Midjourney. We consider several tasks posed over the dataset. In addition to image captioning, cross-modal matching, and visual question answering, we introduce a difficult explanation generation task, where models must identify and explain why a given image is unusual. Our results show that state-of-the-art models such as GPT3 and BLIP2 still lag behind human performance on WHOOPS!. We hope our dataset will inspire the development of AI models with stronger visual commonsense reasoning abilities.","tags":["multimodality"],"title":"Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images","type":"publication"},{"authors":["Netta Madvil","\u003ca href='https://yonatanbitton.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eYonatan Bitton\u003c/a\u003e","\u003cb\u003eRoy Schwartz\u003c/b\u003e"],"categories":null,"content":"","date":1689033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689033600,"objectID":"0ed222a15bd4a57c22347179b465163f","permalink":"/publication/read_it/","publishdate":"2023-07-11T00:00:00Z","relpermalink":"/publication/read_it/","section":"publication","summary":"The prevalence of large-scale multimodal datasets presents unique challenges in assessing dataset quality. We propose a two-step method to analyze multimodal datasets, which leverages a small seed of human annotation to map each multimodal instance to the modalities required to process it. Our method sheds light on the importance of different modalities in datasets, as well as the relationship between them. We apply our approach to TVQA, a video question-answering dataset, and discover that most questions can be answered using a single modality, without a substantial bias towards any specific modality. Moreover, we find that more than 70\\% of the questions are solvable using several different single-modality strategies, e.g., by either looking at the video or listening to the audio, highlighting the limited integration of multiple modalities in TVQA. We leverage our annotation and analyze the MERLOT Reserve, finding that it struggles with image-based questions compared to text and audio, but also with auditory speaker identification. Based on our observations, we introduce a new test set that necessitates multiple modalities, observing a dramatic drop in model performance. Our methodology provides valuable insights into multimodal datasets and highlights the need for the development of more robust models.","tags":["multimodality"],"title":"Read, Look or Listen? What's Needed for Solving a Multimodal Dataset","type":"publication"},{"authors":["Ji-Ung Lee","Haritz Puerto","Betty van Aken","Yuki Arase","Jessica Zosa Forde","Leon Derczynski","Andreas Rckl","\u003ca href='https://www.informatik.tu-darmstadt.de/ukp/ukp_home/head_ukp/index.en.jsp' target=\"_blank\" rel=\"noopener noreferrer\"\u003eIryna Gurevych\u003c/a\u003e","\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='https://strubell.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eEmma Strubell\u003c/a\u003e","\u003ca href='http://www.cs.cmu.edu/~jessed/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eJesse Dodge\u003c/a\u003e"],"categories":null,"content":"","date":1687996800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687996800,"objectID":"69bf31312b2100933d80417bd9d2e0ae","permalink":"/publication/surveying_disparities/","publishdate":"2023-06-29T00:00:00Z","relpermalink":"/publication/surveying_disparities/","section":"publication","summary":"Many recent improvements in NLP stem from the development and use of large pre-trained language models (PLMs) with billions of parameters. Large model sizes makes computational cost one of the main limiting factors for training and evaluating such models; and has raised severe concerns about the sustainability, reproducibility, and inclusiveness for researching PLMs. These concerns are often based on personal experiences and observations. However, there had not been any large-scale surveys that investigate them. In this work, we provide a first attempt to quantify these concerns regarding three topics, namely, environmental impact, equity, and impact on peer reviewing. By conducting a survey with 312 participants from the NLP community, we capture existing (dis)parities between different and within groups with respect to seniority, academia, and industry; and their impact on the peer reviewing process. For each topic, we provide an analysis and devise recommendations to mitigate found disparities, some of which already successfully implemented. Finally, we discuss additional concerns raised by many participants in free-text responses.","tags":["greenai"],"title":"Surveying (Dis)Parities and Concerns of Compute Hungry NLP Research","type":"publication"},{"authors":["Daniel Rotem","Michael Hassid","Jonathan Mamou","\u003cb\u003eRoy Schwartz\u003c/b\u003e"],"categories":null,"content":"","date":1684540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684540800,"objectID":"e58fe9b03826bac767d5ca1eb7481fc0","permalink":"/publication/sweet/","publishdate":"2023-05-20T00:00:00Z","relpermalink":"/publication/sweet/","section":"publication","summary":"Adaptive inference is a simple method for reducing inference costs. The method works by maintaining multiple classifiers of different capacities, and allocating resources to each test instance according to its difficulty. In this work, we compare the two main approaches for adaptive inference, Early-Exit and Multi-Model, when training data is limited. First, we observe that for models with the same architecture and size, individual Multi-Model classifiers outperform their Early-Exit counterparts by an average of 2.3%. We show that this gap is caused by Early-Exit classifiers sharing model parameters during training, resulting in conflicting gradient updates of model weights. We find that despite this gap, Early-Exit still provides a better speed-accuracy trade-off due to the overhead of the Multi-Model approach. To address these issues, we propose SWEET (Separating Weights for Early-Exit Transformers) an Early-Exit fine-tuning method that assigns each classifier its own set of unique model weights, not updated by other classifiers. We compare SWEET's speed-accuracy curve to standard Early-Exit and Multi-Model baselines and find that it outperforms both methods at fast speeds while maintaining comparable scores to Early-Exit at slow speeds. Moreover, SWEET individual classifiers outperform Early-Exit ones by 1.1% on average. SWEET enjoys the benefits of both methods, paving the way for further reduction of inference costs in NLP.","tags":["greenai"],"title":"Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings","type":"publication"},{"authors":["Yuval Reif","\u003cb\u003eRoy Schwartz\u003c/b\u003e"],"categories":null,"content":"","date":1683072000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683072000,"objectID":"7edf39929a3d82c1968e402c821bd8de","permalink":"/publication/fight_bias/","publishdate":"2023-05-03T00:00:00Z","relpermalink":"/publication/fight_bias/","section":"publication","summary":"NLP models often rely on superficial cues known as dataset biases to achieve impressive performance, and can fail on examples where these biases do not hold. Recent work sought to develop robust, unbiased models by filtering biased examples from training sets. In this work, we argue that such filtering can obscure the true capabilities of models to overcome biases, which might never be removed in full from the dataset. We suggest that in order to drive the development of models robust to subtle biases, dataset biases should be amplified in the training set. We introduce an evaluation framework defined by a bias-amplified training set and an anti-biased test set, both automatically extracted from existing datasets. Experiments across three notions of bias, four datasets and two models show that our framework is substantially more challenging for models than the original data splits, and even more challenging than hand-crafted challenge sets. Our evaluation framework can use any existing dataset, even those considered obsolete, to test model robustness. We hope our work will guide the development of robust models that do not rely on superficial biases and correlations. To this end, we publicly release our code and data.","tags":["improved_evaluation"],"title":"Fighting Bias with Bias: Promoting Model Robustness by Amplifying Dataset Biases","type":"publication"},{"authors":["Aviad Sar-Shalom","\u003cb\u003eRoy Schwartz\u003c/b\u003e"],"categories":null,"content":"","date":1682985600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682985600,"objectID":"c06f624aa14beb28a6b0979c82417710","permalink":"/publication/curating_datasets/","publishdate":"2023-05-02T00:00:00Z","relpermalink":"/publication/curating_datasets/","section":"publication","summary":"The landscape of NLP research is dominated by large-scale models training on colossal datasets, relying on data quantity rather than quality. As an alternative to this landscape, we propose a method for weighing the relative importance of examples in a dataset based on their Example Training dynamics (swayamdipta et al., 2020)---a set of metrics computed during training. We propose a new way of computing the ETD of a dataset, and show that they can be used to improve performance in both in-distribution and out-of-distribution testing. We show that ETD can be transferable, i.e., they can be computed once and used for training different models, effectively reducing their computation cost. Finally, we suggest an active learning approach for computing ETD during training rather than as a preprocessing step---an approach that is not as effective, but dramatically reduces the extra computational costs.","tags":["improved_evaluation"],"title":"Curating Datasets for Better Performance with Example Training Dynamics","type":"publication"},{"authors":null,"categories":null,"content":"","date":1680048000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1680048000,"objectID":"7590a9e9267095f5ab9f7ba07eac1d49","permalink":"/talk/spurious_correlations/","publishdate":"2023-03-29T00:00:00Z","relpermalink":"/talk/spurious_correlations/","section":"talk","summary":"University of Pennsylvania, CLunch computational linguistics seminar","tags":null,"title":"Spurious Correlations: Challenges, Solutions, and Opportunities","type":"talk"},{"authors":null,"categories":null,"content":"","date":1677628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677628800,"objectID":"f582d6148313863e66c2c65bef36af19","permalink":"/talk/green_ai/","publishdate":"2023-03-01T00:00:00Z","relpermalink":"/talk/green_ai/","section":"talk","summary":"Healthcare NLP Summit 2023\nEPFL IC Colloquium\nInternational Society for Computational Biology (ISMB/ECCB 2021); Computational Biology going Green Session\nEuropean Broadcasting Union 2021 Sustainability Summit\nThe transdisciplinary 2021 research convention for artificial intelligence (KI-CAMP)\nMicrosoft; Machine Learning Seminar\nLancaster\u0026#39;s Data Science Lunchtime Seminar","tags":null,"title":"Green AI","type":"talk"},{"authors":["Judit Acs","Endre Hamerlik","\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='http://homes.cs.washington.edu/~nasmith/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eNoah A. Smith\u003c/a\u003e","Andras Kornai"],"categories":null,"content":"","date":1677628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677628800,"objectID":"3e006e5cdc29cd54d03dc7550ade62f2","permalink":"/publication/morphosyntactic_probing/","publishdate":"2023-05-25T00:00:00Z","relpermalink":"/publication/morphosyntactic_probing/","section":"publication","summary":"We introduce an extensive dataset for multilingual probing of morphological information in language models (247 tasks across 42 languages from 10 families), each consisting of a sentence with a target word and a morphological tag as the desired label, derived from the Universal Dependencies treebanks. We find that pre-trained Transformer models (mBERT and XLM-RoBERTa) learn features that attain strong performance across these tasks. We then apply two methods to locate, for each probing task, where the disambiguating information resides in the input. The first is a new perturbation method that \"masks\" various parts of context; the second is the classical method of Shapley values. The most intriguing finding that emerges is a strong tendency for the preceding context to hold more information relevant to the prediction than the following context.","tags":["understanding_models"],"title":"Morphosyntactic Probing of Multilingual BERT Models","type":"publication"},{"authors":["\u003ca href='https://yonatanbitton.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eYonatan Bitton\u003c/a\u003e","Ron Yosef","Eli Strugo","\u003ca href='http://www.hyadatalab.com/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eDafna Shahaf\u003c/a\u003e","\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='https://gabrielstanovsky.github.io' target=\"_blank\" rel=\"noopener noreferrer\"\u003eGabi Stanovsky\u003c/a\u003e"],"categories":null,"content":"","date":1668816000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668816000,"objectID":"760e19b315094757ca595b6e1466a57e","permalink":"/publication/vasr/","publishdate":"2022-11-19T00:00:00Z","relpermalink":"/publication/vasr/","section":"publication","summary":"A core process in human cognition is analogical mapping: the ability to identify a similar relational structure between different situations. We introduce a novel task, Visual Analogies of Situation Recognition, adapting the classical word-analogy task into the visual domain. Given a triplet of images, the task is to select an image candidate B' that completes the analogy (A to A' is like B to what?). Unlike previous work on visual analogy that focused on simple image transformations, we tackle complex analogies requiring understanding of scenes.\nWe leverage situation recognition annotations and the CLIP model to generate a large set of 500k candidate analogies. Crowdsourced annotations for a sample of the data indicate that humans agree with the dataset label ~80% of the time (chance level 25%). Furthermore, we use human annotations to create a gold-standard dataset of 3,820 validated analogies. Our experiments demonstrate that state-of-the-art models do well when distractors are chosen randomly (~86%), but struggle with carefully chosen distractors (~53%, compared to 90% human accuracy). We hope our dataset will encourage the development of new analogy-making models.","tags":["multimodality"],"title":"VASR: Visual Analogies of Situation Recognition","type":"publication"},{"authors":null,"categories":null,"content":"","date":1667779200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667779200,"objectID":"8b3b5f3b3f08c6a13954b069ab1be888","permalink":"/talk/dont_balance/","publishdate":"2022-11-07T00:00:00Z","relpermalink":"/talk/dont_balance/","section":"talk","summary":"\nTechnion, Computational Data Science Seminar\nTel Aviv University, NLP Seminar","tags":null,"title":"On the Limitations of Dataset Balancing: The Lost Battle Against Spurious Correlations","type":"talk"},{"authors":["Michael Hassid","\u003ca href='http://homes.cs.washington.edu/~hapeng/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eHao Peng\u003c/a\u003e","Daniel Rotem","\u003ca href='https://homes.cs.washington.edu/~jkasai/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eJungo Kasai\u003c/a\u003e","Ivan Montero","\u003ca href='http://homes.cs.washington.edu/~nasmith/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eNoah A. Smith\u003c/a\u003e","\u003cb\u003eRoy Schwartz\u003c/b\u003e"],"categories":null,"content":"","date":1665014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665014400,"objectID":"fa211fe381606cdbc5b49ee3cef49781","permalink":"/publication/how_much_does_attention/","publishdate":"2022-10-06T00:00:00Z","relpermalink":"/publication/how_much_does_attention/","section":"publication","summary":"The attention mechanism is considered the backbone of the widely-used Transformer architecture. It contextualizes the input by computing input-specific attention matrices. We find that this mechanism, while powerful and elegant, is not as important as typically thought for pretrained language models. We introduce PAPA, a new probing method that replaces the input-dependent attention matrices with constant ones\u0026#8212;the average attention weights over multiple inputs. We use PAPA to analyze several established pretrained Transformers on six downstream tasks. We find that without any input-dependent attention, all models achieve competitive performance\u0026#8212;an average relative drop of only 8% from the probing baseline. Further, little or no performance drop is observed when replacing half of the input-dependent attention matrices with constant (input-independent) ones. Interestingly, we show that better-performing models lose more from applying our method than weaker models, suggesting that the utilization of the input-dependent attention mechanism might be a factor in their success. Our results motivate research on simpler alternatives to input-dependent attention, as well as on methods for better utilization of this mechanism in the Transformer architecture.","tags":["understanding_models"],"title":"How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers","type":"publication"},{"authors":["Marcos Treviso*","Tianchu Ji*","Ji-Ung Lee*","Betty van Aken","Qingqing Cao","Manuel R. Ciosici","Michael Hassid","Kenneth Heafield","Sara Hooker","Pedro H. Martins","Andr F. T. Martins","Peter Milder","Colin Raffel","Jessica Forde","Edwin Simpson","Noam Slonim","\u003ca href='http://www.cs.cmu.edu/~jessed/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eJesse Dodge\u003c/a\u003e","\u003ca href='https://strubell.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eEmma Strubell\u003c/a\u003e","Niranjan Balasubramanian","Leon Derczynski","\u003ca href='https://www.informatik.tu-darmstadt.de/ukp/ukp_home/head_ukp/index.en.jsp' target=\"_blank\" rel=\"noopener noreferrer\"\u003eIryna Gurevych\u003c/a\u003e","\u003cb\u003eRoy Schwartz\u003c/b\u003e"],"categories":null,"content":"","date":1661990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661990400,"objectID":"ae1cb436482799838717f4f17cd73a4a","permalink":"/publication/efficient_methods/","publishdate":"2022-09-01T00:00:00Z","relpermalink":"/publication/efficient_methods/","section":"publication","summary":"Recent work in natural language processing (NLP) has yielded appealing results from scaling model parameters and training data; however, using only scale to improve performance means that resource consumption also grows. Such resources include data, time, storage, or energy, all of which are naturally limited and unevenly distributed. This motivates research into efficient methods that require fewer resources to achieve similar results. This survey synthesizes and relates current methods and findings in efficient NLP. We aim to provide both guidance for conducting NLP under limited resources, and point towards promising research directions for developing more efficient methods.","tags":["greenai"],"title":"Efficient Methods for Natural Language Processing: A Survey","type":"publication"},{"authors":["\u003ca href='https://yonatanbitton.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eYonatan Bitton\u003c/a\u003e*","Nitzan Bitton Guetta*","Ron Yosef","Yuval Elovici","\u003ca href='https://www.cs.unc.edu/~mbansal/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eMohit Bansal\u003c/a\u003e","\u003ca href='https://gabrielstanovsky.github.io' target=\"_blank\" rel=\"noopener noreferrer\"\u003eGabi Stanovsky\u003c/a\u003e","\u003cb\u003eRoy Schwartz\u003c/b\u003e"],"categories":null,"content":"","date":1658793600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658793600,"objectID":"dbe80be988b8964ca3f4e371e5c32526","permalink":"/publication/winogavil/","publishdate":"2022-07-26T00:00:00Z","relpermalink":"/publication/winogavil/","section":"publication","summary":"While vision-and-language models perform well on tasks such as visual question answering, they struggle when it comes to basic human commonsense reasoning skills. In this work, we introduce WinoGAViL: an online game to collect vision-and-language associations, (e.g., werewolves to a full moon), used as a dynamic benchmark to evaluate state-of-the-art models. Inspired by the popular card game Codenames, a spymaster gives a textual cue related to several visual candidates, and another player has to identify them. Human players are rewarded for creating associations that are challenging for a rival AI model but still solvable by other human players. We use the game to collect 3.5K instances, finding that they are intuitive for humans (90% Jaccard index) but challenging for state-of-the-art AI models, where the best model (ViLT) achieves a score of 52%, succeeding mostly where the cue is visually salient. Our analysis as well as the feedback we collect from players indicate that the collected associations require diverse reasoning skills, including general knowledge, common sense, abstraction, and more. We release the dataset, the code and the interactive game, aiming to allow future data collection that can be used to develop models with better association abilities. ","tags":["multimodality"],"title":"WinoGAViL: Gamified Association Benchmark to Challenge Vision-and-Language Models","type":"publication"},{"authors":["Yarden Tal","Inbal Magar","\u003cb\u003eRoy Schwartz\u003c/b\u003e"],"categories":null,"content":"","date":1651708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651708800,"objectID":"7401a2992aa394c24efad5ba3a36297c","permalink":"/publication/size_bias/","publishdate":"2022-05-05T00:00:00Z","relpermalink":"/publication/size_bias/","section":"publication","summary":"The size of pretrained models is increasing, and so does their performance on a variety of NLP tasks. However, as their memorization capacity grows, they might pick up more social biases. In this work, we examine the connection between model size and its gender bias (specifically, occupational gender bias). We measure bias in three masked language model families (RoBERTa, DeBERTa, and T5) in two setups: directly using prompt based method, and using a downstream task (Winogender). We find on the one hand that larger models receive higher bias scores on the former task, but when evaluated on the latter, they make fewer gender errors. To examine these potentially conflicting results, we carefully investigate the behavior of the different models on Winogender. We find that while larger models outperform smaller ones, the probability that their mistakes are caused by gender bias is higher. Moreover, we find that the proportion of stereotypical errors compared to anti-stereotypical ones grows with the model size. Our findings highlight the potential risks that can arise from increasing model size.","tags":null,"title":"Fewer Errors, but More Stereotypes? The Effect of Model Size on Gender Bias","type":"publication"},{"authors":["\u003ca href='https://scholar.google.co.il/citations?user=96pR-j0AAAAJ\u0026hl=en' target=\"_blank\" rel=\"noopener noreferrer\"\u003eJonathan Mamou\u003c/a\u003e","\u003ca href='https://www.intel.com/content/www/us/en/artificial-intelligence/bios/oren-pereg.html' target=\"_blank\" rel=\"noopener noreferrer\"\u003eOren Pereg\u003c/a\u003e","\u003ca href='https://www.linkedin.com/in/moshe-wasserblat-8977632/?originalSubdomain=il' target=\"_blank\" rel=\"noopener noreferrer\"\u003eMoshe Wasserblat\u003c/a\u003e","\u003cb\u003eRoy Schwartz\u003c/b\u003e"],"categories":null,"content":"","date":1649894400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649894400,"objectID":"56d4e95b2a2d9a646503c5ab907208af","permalink":"/publication/tangobert/","publishdate":"2022-12-10T00:00:00Z","relpermalink":"/publication/tangobert/","section":"publication","summary":"The remarkable success of large transformer-based models such as BERT, RoBERTa and XLNet in many NLP tasks comes with a large increase in monetary and environmental cost due to their high computational load and energy consumption. In order to reduce this computational load in inference time, we present TangoBERT, a cascaded model architecture in which instances are first processed by an efficient but less accurate first tier model, and only part of those instances are additionally processed by a less efficient but more accurate second tier model. The decision of whether to apply the second tier model is based on a confidence score produced by the first tier model. Our simple method has several appealing practical advantages compared to standard cascading approaches based on multi-layered transformer models. First, it enables higher speedup gains (average lower latency). Second, it takes advantage of batch size optimization for cascading, which increases the relative inference cost reductions. We report TangoBERT inference CPU speedup on four text classification GLUE tasks and on one reading comprehension task. Experimental results show that TangoBERT outperforms efficient early exit baseline models; on the the SST-2 task, it achieves an accuracy of 93.9% with a CPU speedup of 8.2x.","tags":["greenai"],"title":"TangoBERT: Reducing Inference Cost by using Cascaded Architecture","type":"publication"},{"authors":["\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='https://gabrielstanovsky.github.io' target=\"_blank\" rel=\"noopener noreferrer\"\u003eGabi Stanovsky\u003c/a\u003e"],"categories":null,"content":"","date":1649289600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649289600,"objectID":"3d760afa85c26a1858dcbf4ec261339d","permalink":"/publication/dont_balance/","publishdate":"2022-04-04T00:00:00Z","relpermalink":"/publication/dont_balance/","section":"publication","summary":"Recent work has shown that deep learning models in NLP are highly sensitive to low-level correlations between simple features and specific output labels, leading to overfitting and lack of generalization. To mitigate this problem, a common practice is to balance datasets by adding new instances or by filtering out \"easy\" instances (Sakaguchi et al., 2020), culminating in a recent proposal to eliminate single-word correlations altogether (Gardner et al., 2021). In this opinion paper, we identify that despite these efforts, increasingly-powerful models keep exploiting ever-smaller spurious correlations, and as a result even balancing all single-word features is insufficient for mitigating all of these correlations. In parallel, a truly balanced dataset may be bound to \"throw the baby out with the bathwater\" and miss important signal encoding common sense and world knowledge. We highlight several alternatives to dataset balancing, focusing on enhancing datasets with richer contexts, allowing models to abstain and interact with users, and turning from large-scale fine-tuning to zero- or few-shot setups.","tags":["improved_evaluation"],"title":"On the Limitations of Dataset Balancing: The Lost Battle Against Spurious Correlations","type":"publication"},{"authors":["\u003ca href='http://www.cs.cmu.edu/~jessed/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eJesse Dodge\u003c/a\u003e","Taylor Prewitt","Remi Tachet des Combes","Erika Odmark","\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='https://strubell.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eEmma Strubell\u003c/a\u003e","\u003ca href='https://www.sashaluccioni.com/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eSasha Luccioni\u003c/a\u003e","\u003ca href='http://homes.cs.washington.edu/~nasmith/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eNoah A. Smith\u003c/a\u003e","Nicole DeCario","Will Buchanan"],"categories":null,"content":"","date":1649203200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649203200,"objectID":"6d7806fab551301e3c36d2735e4206df","permalink":"/publication/carbon/","publishdate":"2022-04-06T00:00:00Z","relpermalink":"/publication/carbon/","section":"publication","summary":"The advent of cloud computing has provided people around the world with unprecedented access to computational power and enabled rapid growth in technologies such as machine learning, the computational demands of which come with a high energy cost and a commensurate increase in carbon footprint. As a result, recent scholarship has called for better estimates of the impact of AI on greenhouse gas emissions. However, data scientists today do not have easy or reliable access to measurements of this information, which precludes consideration of how to reduce the costs (computational, electricity, environmental) associated with machine learning workloads. We argue that cloud providers presenting information about software carbon intensity to users is a fundamental stepping stone towards minimizing emissions.\nIn this paper, we provide a framework for measuring software carbon intensity, and propose to measure operational carbon emissions by using location-based and time-specific marginal emissions data per energy unit. We provide measurements of operational software carbon intensity for a set of modern models covering natural language processing (NLP) and computer vision applications, including four sizes of DenseNet models trained on MNIST, pretraining and finetuning of BERT-small, pretraining of a 6.1 billion parameter language model, and five sizes of Vision Transformer. We confirm previous results that the geographic region of the data center plays a significant role in the carbon intensity for a given cloud instance. We also present new results showing that the time of day has meaningful impact on operational software carbon intensity. We then evaluate a suite of approaches for reducing emissions in the cloud: using cloud instances in different geographic regions, using cloud instances at different times of day, and dynamically pausing cloud instances when the marginal carbon intensity is above a certain threshold. We find that choosing an appropriate region can have the largest impact, but emissions can be reduced by the other methods as well. Finally, we conclude with recommendations for how machine learning practitioners can use software carbon intensity information to reduce environmental impact.","tags":["greenai"],"title":"Measuring the Carbon Intensity of AI in Cloud instances","type":"publication"},{"authors":["Inbal Magar","\u003cb\u003eRoy Schwartz\u003c/b\u003e"],"categories":null,"content":"","date":1646438400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646438400,"objectID":"4f424a29b774d4d1ea69dbdfd9c9962a","permalink":"/publication/contamination/","publishdate":"2022-03-05T00:00:00Z","relpermalink":"/publication/contamination/","section":"publication","summary":"Pretrained language models are typically trained on massive web-based datasets, which are often contaminated with downstream test sets. It is not clear to what extent models exploit the contaminated data for downstream tasks. We present a principled method to study this question. We pretrain BERT models on joint corpora of Wikipedia and labeled downstream datasets, and fine-tune them on the relevant task. Comparing performance between samples seen and unseen during pretraining enables us to define and quantify levels of memorization and exploitation. Experiments with two models and three downstream tasks show that exploitation exists in some cases, but in others the models memorize the contaminated data, but do not exploit it. We show that these two measures are affected by different factors such as the number of duplications of the contaminated data and the model size. Our results highlight the importance of analyzing massive web-scale datasets to verify that progress in NLP is obtained by better language understanding and not better data exploitation.","tags":["improved_evaluation"],"title":"Data Contamination: From Memorization to Exploitation","type":"publication"},{"authors":null,"categories":null,"content":"","date":1636502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636502400,"objectID":"599b0f752a248c2442522146ced51984","permalink":"/talk/efficient_data/","publishdate":"2021-11-10T00:00:00Z","relpermalink":"/talk/efficient_data/","section":"talk","summary":"SustainNLP 2021 Workshop, Invited Talk\nTechnion, Computational Data Science Seminar","tags":null,"title":"Not all Textual Instances are Alike: Efficient NLP by Better Understanding of our Data","type":"talk"},{"authors":["\u003ca href='http://homes.cs.washington.edu/~hapeng/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eHao Peng\u003c/a\u003e","\u003ca href='https://homes.cs.washington.edu/~jkasai/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eJungo Kasai\u003c/a\u003e","\u003ca href='https://nik0spapp.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eNikolaos Pappas\u003c/a\u003e","\u003ca href='https://dyogatama.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eDani Yogatama\u003c/a\u003e","\u003ca href='https://zhaofengwu.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eZhaofeng Wu\u003c/a\u003e","\u003ca href='https://ikekonglp.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eLingpeng Kong\u003c/a\u003e","\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='http://homes.cs.washington.edu/~nasmith/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eNoah A. Smith\u003c/a\u003e"],"categories":null,"content":"","date":1633910400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633910400,"objectID":"d86e89a0933c4cc32cec47a8458a6779","permalink":"/publication/abc/","publishdate":"2021-10-11T00:00:00Z","relpermalink":"/publication/abc/","section":"publication","summary":"Transformer architectures have achieved stateof-the-art results on a variety of natural language processing (NLP) tasks. However, their attention mechanism comes with a quadratic complexity in sequence lengths, making the computational overhead prohibitive, especially for long sequences. Attention context can be seen as a random-access memory with each token taking a slot. Under this perspective, the memory size grows linearly with the sequence length, and so does the overhead of reading from it. One way to improve the efficiency is to bound the memory size. We show that disparate approaches can be subsumed into one abstraction, attention with bounded-memory control (ABC), and they vary in their organization of the memory. ABC reveals new, unexplored possibilities. First, it connects several efficient attention variants that would otherwise seem apart. Second, this abstraction gives new insightsan established approach (Wang et al., 2020b) previously thought to not be applicable in causal attention, actually is. Last, we present a new instance of ABC, which draws inspiration from existing ABC approaches, but replaces their heuristic memory-organizing functions with a learned, contextualized one. Our experiments on language modeling, machine translation, and masked language model finetuning show that our approach outperforms previous efficient attention models; compared to the strong transformer baselines, it significantly improves the inference time and space efficiency with no or negligible accuracy loss.","tags":["greenai","understanding_models"],"title":"ABC: Attention with Bounded-memory Control","type":"publication"},{"authors":["\u003ca href='https://lambdaviking.com' target=\"_blank\" rel=\"noopener noreferrer\"\u003eWill Merrill\u003c/a\u003e","\u003ca href='https://vkramanuj.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eVivek Ramanujan\u003c/a\u003e","\u003ca href='https://u.cs.biu.ac.il/~yogo/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eYoav Goldberg\u003c/a\u003e","\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='http://homes.cs.washington.edu/~nasmith/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eNoah A. Smith\u003c/a\u003e"],"categories":null,"content":"","date":1631145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631145600,"objectID":"d5671ee32709b141661e097f671933cd","permalink":"/publication/norm_growth/","publishdate":"2021-09-09T00:00:00Z","relpermalink":"/publication/norm_growth/","section":"publication","summary":"The capacity of neural networks like the widely adopted transformer is known to be very high. Evidence is emerging that they learn successfully due to inductive bias in the training routine, typically a variant of gradient descent (GD). To better understand this bias, we study the tendency for transformer parameters to grow in magnitude (l2 norm) during training, and its implications for the emergent representations within self attention layers. Empirically, we document norm growth in the training of transformer language models, including T5 during its pretraining. As the parameters grow in magnitude, we prove that the network approximates a discretized network with saturated activation functions. Such \"saturated\" networks are known to have a reduced capacity compared to the full network family that can be described in terms of formal languages and automata. Our results suggest saturation is a new characterization of an inductive bias implicit in GD of particular interest for NLP. We leverage the emergent discrete structure in a saturated transformer to analyze the role of different attention heads, finding that some focus locally on a small number of positions, while other heads compute global averages, allowing counting. We believe understanding the interplay between these two capabilities may shed further light on the structure of computation within large transformers.","tags":["understanding_models"],"title":"Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent","type":"publication"},{"authors":["\u003ca href='http://www.cs.cmu.edu/~jessed/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eJesse Dodge\u003c/a\u003e","\u003ca href='https://suchin.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eSuchin Gururangan\u003c/a\u003e","\u003ca href='http://web.stanford.edu/~dcard/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eDallas Card\u003c/a\u003e","\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='http://homes.cs.washington.edu/~nasmith/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eNoah A. Smith\u003c/a\u003e"],"categories":null,"content":"","date":1631145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631145600,"objectID":"2fa9de56cfc7b4dc1fe7883cd3b78b37","permalink":"/publication/showyourwork2/","publishdate":"2021-09-09T00:00:00Z","relpermalink":"/publication/showyourwork2/","section":"publication","summary":"NLP is often supported by experimental results, and improved reporting of such results can lead to better understanding and more reproducible science. In this paper we analyze three statistical estimators for expected validation performance, a tool used for reporting performance (e.g., accuracy) as a function of computational budget (e.g., number of hyperparameter tuning experiments). Where previous work analyzing such estimators focused on the bias, we also examine the variance and mean squared error (MSE). In both synthetic and realistic scenarios, we evaluate three estimators and find the unbiased estimator has the highest variance, and the estimator with the smallest variance has the largest bias; the estimator with the smallest MSE strikes a balance between bias and variance, displaying a classic bias-variance tradeoff. We use expected validation performance to compare between different models, and analyze how frequently each estimator leads to drawing incorrect conclusions about which of two models performs best. We find that the two biased estimators lead to the fewest incorrect conclusions, which hints at the importance of minimizing variance and MSE.","tags":["greenai"],"title":"Expected Validation Performanceand Estimation of a Random Variable's Maximum","type":"publication"},{"authors":["\u003ca href='https://yonatanbitton.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eYonatan Bitton\u003c/a\u003e","\u003ca href='https://gabrielstanovsky.github.io' target=\"_blank\" rel=\"noopener noreferrer\"\u003eGabi Stanovsky\u003c/a\u003e","\u003ca href='https://www.cs.bgu.ac.il/~elhadad/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eMichael Elhadad\u003c/a\u003e","\u003cb\u003eRoy Schwartz\u003c/b\u003e"],"categories":null,"content":"","date":1630800000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630800000,"objectID":"52999c4a70323766abaf8abe45ca2d90","permalink":"/publication/vl_mlm/","publishdate":"2021-09-05T00:00:00Z","relpermalink":"/publication/vl_mlm/","section":"publication","summary":"Masked language modeling (MLM) is one of the key sub-tasks in vision-language pretraining. In the cross-modal setting, tokens in the sentence are masked at random, and the model predicts the masked tokens given the image and the text. In this paper, we observe several key disadvantages of MLM in this setting. First, as captions tend to be short, in a third of the sentences no token is sampled. Second, the majority of masked tokens are stop-words and punctuation, leading to under-utilization of the image. We investigate a range of alternative masking strategies specific to the cross-modal setting that address these shortcomings, aiming for better fusion of text and image in the learned representation. When pre-training the LXMERT model, our alternative masking strategies consistently improve over the original masking strategy on three downstream tasks, especially in low resource settings. Further, our pre-training approach substantially outperforms the baseline model on a prompt-based probing task designed to elicit image objects. These results and our analysis indicate that our method allows for better utilization of the training data.","tags":["greenai","multimodality"],"title":"Data Efficient Masked Language Modeling for Vision and Language","type":"publication"},{"authors":["\u003ca href='https://lambdaviking.com' target=\"_blank\" rel=\"noopener noreferrer\"\u003eWill Merrill\u003c/a\u003e","\u003ca href='https://u.cs.biu.ac.il/~yogo/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eYoav Goldberg\u003c/a\u003e","\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='http://homes.cs.washington.edu/~nasmith/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eNoah A. Smith\u003c/a\u003e"],"categories":null,"content":"","date":1619049600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619049600,"objectID":"907e7814dfe27d48bced3a7e5ad46e51","permalink":"/publication/gpt300/","publishdate":"2021-04-22T00:00:00Z","relpermalink":"/publication/gpt300/","section":"publication","summary":"Language models trained on billions of tokens have recently led to unprecedented results on many NLP tasks. This success raises the question of whether, in principle, a system can ever \"understand\" raw text without access to some form of grounding. We formally investigate the abilities of ungrounded systems to acquire meaning. Our analysis focuses on the role of \"assertions\": contexts within raw text that provide indirect clues about underlying semantics. We study whether assertions enable a system to emulate representations preserving semantic relations like equivalence. We find that assertions enable semantic emulation if all expressions in the language are referentially transparent. However, if the language uses non-transparent patterns like variable binding, we show that emulation can become an uncomputable problem. Finally, we discuss differences between our formal model and natural language, exploring how our results generalize to a modal setting and other semantic relations. Together, our results suggest that assertions in code or language do not provide sufficient signal to fully emulate semantic representations. We formalize ways in which ungrounded language models appear to be fundamentally limited in their ability to \"understand\".","tags":["understanding_models"],"title":"Provable Limitations of Acquiring Meaning from Ungrounded Form: What will Future Language Models Understand?","type":"publication"},{"authors":["\u003ca href='https://tomhoper.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eTom Hope\u003c/a\u003e*","\u003ca href='https://aidaamini.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eAida Amini\u003c/a\u003e*","David Wadden","Madeleine van Zuylen","Sravanthi Parasa","\u003ca href='http://erichorvitz.com/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eEric Horvitz\u003c/a\u003e","\u003ca href='https://www.cs.washington.edu/people/faculty/weld' target=\"_blank\" rel=\"noopener noreferrer\"\u003eDan Weld\u003c/a\u003e","\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='https://homes.cs.washington.edu/~hannaneh/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eHannaneh Hajishirzi\u003c/a\u003e"],"categories":null,"content":"","date":1615507200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615507200,"objectID":"5898c87fc36fdb5a7f94308ca27e91da","permalink":"/publication/covid_mechanisms/","publishdate":"2021-03-17T00:00:00Z","relpermalink":"/publication/covid_mechanisms/","section":"publication","summary":"The urgency of mitigating COVID-19 has spawned a large and diverse body of scientific literature that is challenging for researchers to navigate. This explosion of information has stimulated interest in automated tools to help identify useful knowledge. We have pursued the use of methods for extracting diverse forms of mechanism relations from the natural language of scientific papers. We seek to identify concepts in COVID-19 and related literature which represent activities, functions, associations and causal relations, ranging from cellular processes to economic impacts. We formulate a broad, coarse-grained schema targeting mechanism relations between open, free-form entities. Our approach strikes a balance between expressivity and breadth that supports generalization across diverse concepts. We curate a dataset of scientific papers annotated according to our novel schema. Using an information extraction model trained on this new corpus, we construct a knowledge base (KB) of 2M mechanism relations, which we make publicly available. Our model is able to extract relations at an F1 at least twice that of baselines such as open IE or related scientific IE systems. We conduct experiments examining the ability of our system to retrieve relevant information on viral mechanisms of action, and on applications of AI to COVID-19 research. In both cases, our system identifies relevant information from our automatically-constructed knowledge base with high precision.","tags":null,"title":"Extracting a Knowledge Base of Mechanisms from COVID-19 Papers","type":"publication"},{"authors":["\u003ca href='https://yonatanbitton.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eYonatan Bitton\u003c/a\u003e","\u003ca href='https://gabrielstanovsky.github.io' target=\"_blank\" rel=\"noopener noreferrer\"\u003eGabi Stanovsky\u003c/a\u003e","\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='https://www.cs.bgu.ac.il/~elhadad/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eMichael Elhadad\u003c/a\u003e"],"categories":null,"content":"","date":1615334400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615334400,"objectID":"a5dc045d8f694908a6da561de31f9227","permalink":"/publication/contrast_sets/","publishdate":"2021-03-10T00:00:00Z","relpermalink":"/publication/contrast_sets/","section":"publication","summary":"Recent works have shown that supervised models often exploit data artifacts to achieve good test scores while their performance severely degrades on samples outside their training distribution. Contrast sets (Gardneret al., 2020) quantify this phenomenon by perturbing test samples in a minimal way such that the output label is modified. While most contrast sets were created manually, requiring intensive annotation effort, we present a novel method which leverages rich semantic input representation to automatically generate contrast sets for the visual question answering task. Our method computes the answer of perturbed questions, thus vastly reducing annotation cost and enabling thorough evaluation of models' performance on various semantic aspects (e.g., spatial or relational reasoning). We demonstrate the effectiveness of our approach on the GQA dataset and its semantic scene graph image representation. We find that, despite GQA's compositionality and carefully balanced label distribution, two high-performing models drop 13-17% in accuracy compared to the original test set. Finally, we show that our automatic perturbation can be applied to the training set to mitigate the degradation in performance, opening the door to more robust models.","tags":["improved_evaluation","multimodality"],"title":"Automatic Generation of Contrast Sets from Scene Graphs: Probing the Compositional Consistency of GQA","type":"publication"},{"authors":["\u003ca href='http://homes.cs.washington.edu/~hapeng/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eHao Peng\u003c/a\u003e","\u003ca href='https://nik0spapp.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eNikolaos Pappas\u003c/a\u003e","\u003ca href='https://dyogatama.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eDani Yogatama\u003c/a\u003e","\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='http://homes.cs.washington.edu/~nasmith/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eNoah A. Smith\u003c/a\u003e","\u003ca href='https://ikekonglp.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eLingpeng Kong\u003c/a\u003e"],"categories":null,"content":"","date":1610409600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610409600,"objectID":"1624994691445fdf1365206fe66a7812","permalink":"/publication/rfa/","publishdate":"2020-05-04T00:00:00Z","relpermalink":"/publication/rfa/","section":"publication","summary":"Transformers are state-of-the-art models for a variety of sequence modeling tasks. At their core is an attention function which models pairwise interactions between the inputs at every timestep.  While attention is powerful, it doesnot scale efficiently to long sequences due to its quadratic time and space complexity in the sequence length.   We propose  RFA,  a linear time and space attention that uses random feature methods to approximate the softmax function, and explore its applications  in  transformers.   RFA offers  a  straightforward  way  of  learning  with recency bias through an optional gating mechanism and can be used as a drop-in replacement for conventional softmax attention. Experiments on language modeling and machine translation demonstrate that RFA achieves similar or better performance compared to strong transformer baselines.  In the machine translation experiment, RFA decodes twice as fast as a vanilla transformer. Compared to existing efficient transformer variants, RFA is competitive in terms of both accuracy and efficiency on three long text classification datasets.   Our analysis shows that RFA's efficiency gains are especially notable on long sequences, suggesting that RFA will be particularly useful in tasks that require working with large inputs, fast decoding speed, or low memory footprints.","tags":["greenai","understanding_models"],"title":"Random Feature Attention","type":"publication"},{"authors":null,"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"bbd95ffd24c789dc5276ef0fa897188e","permalink":"/talk/green_nlp/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/talk/green_nlp/","section":"talk","summary":"Berkeley; Natural Language Processing Group Seminar\nStanford; Natural Language Processing Group Seminar\nGoogle Brain; Natural Language Processing Group Seminar\nHebrew University of Jerusalem; Machine Learning Seminar\nIntel Israel; Natural Language Processing Group Seminar","tags":null,"title":"Green NLP","type":"talk"},{"authors":["\u003ca href='http://www.cs.cmu.edu/~sswayamd/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eSwabha Swayamdipta\u003c/a\u003e","\u003cb\u003eRoy Schwartz\u003c/b\u003e","Nicholas Lourie","\u003ca href='https://yizhong-wang.com/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eYizhong Wang\u003c/a\u003e","\u003ca href='https://homes.cs.washington.edu/~hannaneh/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eHannaneh Hajishirzi\u003c/a\u003e","\u003ca href='http://homes.cs.washington.edu/~nasmith/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eNoah A. Smith\u003c/a\u003e","\u003ca href='http://homes.cs.washington.edu/~yejin/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eYejin Choi\u003c/a\u003e"],"categories":null,"content":"","date":1600041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600041600,"objectID":"983549b2dfb87b817ebfe9e61790d7a9","permalink":"/publication/datamaps/","publishdate":"2020-09-01T00:00:00Z","relpermalink":"/publication/datamaps/","section":"publication","summary":"Large datasets have become commonplace in NLP research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce \"Data Maps\"---a model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: the behavior of the model on individual instances during training (training dynamics) for building data maps. This yields two intuitive measures for each example---the model's confidence in the true class, and the variability of this confidence across epochs, in a single run of training. Experiments on four datasets show that these model-dependent measures reveal three distinct regions in the data map, each with pronounced characteristics. First, our data maps show the presence of \"ambiguous\" regions with respect to the model, which contribute the most towards out-of-distribution generalization. Second, the most populous regions in the data are \"easy to learn\" for the model, and play an important role in model optimization. Finally, data maps uncover a region with instances that the model finds \"hard to learn\"; these often correspond to labeling errors. Our results indicate that a shift in focus from quantity to quality of data could lead to robust models and improved out-of-distribution generalization.","tags":["improved_evaluation","greenai"],"title":"Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics","type":"publication"},{"authors":["\u003ca href='https://aidaamini.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eAida Amini\u003c/a\u003e*","\u003ca href='https://tomhoper.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eTom Hope\u003c/a\u003e*","David Wadden","\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='https://homes.cs.washington.edu/~hannaneh/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eHannaneh Hajishirzi\u003c/a\u003e"],"categories":null,"content":"","date":1592524800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592524800,"objectID":"7d8d2e0743e6cb2f57cff95a8137fd4e","permalink":"/publication/covid_mechanisms_ws/","publishdate":"2020-06-19T00:00:00Z","relpermalink":"/publication/covid_mechanisms_ws/","section":"publication","summary":"The COVID-19 pandemic has sparked an influx of research by scientists worldwide, leading to a rapidly evolving corpus of interdisciplinary papers. At the time of this writing the COVID-19 Open Research Dataset (CORD-19) has amassed over 128K relevant papers, both historical and cutting-edge. In this emergency scenario, there is a need for automatic information extraction (IE) to provide scientists with structured knowledge, and to accelerate exploration and discovery.\nIn this work we extract relations capturing a broad notion of mechanisms in CORD-19 papers \u0026ndash; Spanning a range of mechanisms as diverse as psychological intervention techniques, computational algorithms, and molecular mechanisms of viral cell entry. This unified view of natural and artificial mechanisms can help generalize across the CORD-19 corpus and is designed to help scale the study of the many different types of processes, activities and functions described in the dataset.\nWe collect a set of annotations from domain experts for direct mechanisms (operations and functions explicitly described in the text) and indirect mechanisms (observed effects and interactions without an explicit description of a direct functional relation). For example, descriptions of the mechanism by which the SARS-CoV-2 virus binds to cells, or of a diagnostic procedure based on computer vision \u0026ndash; are considered direct mechanisms. Conversely, descriptions of indirect mechanisms can for example be of observed links between COVID-19 and certain symptoms, with no explicit mention of the functional process leading from the disease to the symptoms. This distinction between direct and indirect relations is inspired by a review of biomedical and scientific ontologies (e.g., direct and indirect regulation of proteins by chemicals).\nWe allow annotators to select free-form text spans as the arguments in our mechanism relations; this is in contrast to many existing datasets of annotated scientific relations which are often entity-centric (e.g., protein-chemical interactions). We do so in order to capture the complexity and diversity of the many concepts and ideas described in the corpus, in a scalable approach. To address the challenging nature of the annotation task with multiple \"correct\" annotations of complex and diverse spans, we conduct a multi-round annotation process with final adjudication by a domain expert experienced in bioNLP annotations.\nOur annotations are used in combination with existing datasets from different domains to train a relation extraction model, using a mapping schema for previously introduced scientific datasets,selecting only direct and indirect mechanisms (e.g.,DIRECT UP-REGULATION in the chemprot dataset) and unifying relation labels into our typology using a domain expert. Our results indicate we outperform baselines including openIE and SRL, and also supervised models trained on related science IE datasets in the biomedical and computer science domains. We use a biomedical language model that we fine-tune to capture semantic similarity, build a graph of similar mechanisms and induce concepts by finding cliques. To support search over our KB, we use the same language model for retrieving relations similar to the query. To help boost community efforts we release our curated data and models as well as a large-scale knowledge graph of extracted mechanisms.","tags":null,"title":"Extracting a knowledge base of mechanisms from COVID-19 papers","type":"publication"},{"authors":["\u003ca href='https://lambdaviking.com' target=\"_blank\" rel=\"noopener noreferrer\"\u003eWill Merrill\u003c/a\u003e","\u003ca href='https://sgailw.cswp.cs.technion.ac.il' target=\"_blank\" rel=\"noopener noreferrer\"\u003eGail Weiss\u003c/a\u003e","\u003ca href='https://u.cs.biu.ac.il/~yogo/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eYoav Goldberg\u003c/a\u003e","\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='http://homes.cs.washington.edu/~nasmith/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eNoah A. Smith\u003c/a\u003e","\u003ca href='http://www.cs.technion.ac.il/~yahave/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eEran Yahav\u003c/a\u003e"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"d1aecc8e2f36bd4b4d385f0754f9971b","permalink":"/publication/rnn_hierarchy/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/rnn_hierarchy/","section":"publication","summary":"We develop a formal hierarchy of the expressive capacity of RNN architectures. The hierarchy is based around two formal properties: space complexity, which is a measure of the RNN's memory, and rational recurrence, defined as whether the recurrent update can be described by a weighted finite-state machine. We place several RNN variants within this hierarchy. For example, we prove that the LSTM is not rational, which formally separates it from the related QRNN (Bradbury et al., 2016). We also show how the expressive capacity of these models is expanded by stacking multiple layers or composing them with different pooling functions.  Our results build on the theory of 'saturated' RNNs (Merrill, 2019). While formally extending these findings to unsaturated RNNs is left to future work, we hypothesize that the practical learnable capacity of unsaturated RNNs obeys a similar hierarchy. Experimental findings from training unsaturated networks on formal languages support this conjecture.","tags":["understanding_models"],"title":"A Formal Hierarchy of RNN Architectures","type":"publication"},{"authors":["\u003ca href='http://homes.cs.washington.edu/~hapeng/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eHao Peng\u003c/a\u003e","\u003cb\u003eRoy Schwartz\u003c/b\u003e","Dianqi Li","\u003ca href='http://homes.cs.washington.edu/~nasmith/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eNoah A. Smith\u003c/a\u003e"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"68b89e8a4be4f02b881e0c079a4c279d","permalink":"/publication/mae/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/mae/","section":"publication","summary":"Multi-head attentive neural architectures have achieved state-of-the-art results on a variety of natural language processing tasks. Evidence has shown that they are overparameterized; attention heads can be pruned without significant performance loss. In this work, we instead 'reallocate' them-the model learns to activate different heads on different inputs. Drawing connections between multi-head attention and mixture of experts, we propose the mixture of attentive experts model (MAE). MAE is trained using a block coordinate descent algorithm that alternates between updating (1) the responsibilities of the experts and (2) their parameters. Experiments on machine translation and language modeling show that MAE outperforms strong baselines on both tasks. Particularly, on the WMT14 English to German translation dataset, MAE improves over 'transformer-base' by 0.8 BLEU, with a comparable number of parameters. Our analysis shows that our model learns to specialize different experts to different inputs.","tags":null,"title":"A Mixture of \u003ci\u003eh-1\u003c/i\u003e Heads is Better than \u003ci\u003eh\u003c/i\u003e Heads","type":"publication"},{"authors":["\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='https://gabrielstanovsky.github.io' target=\"_blank\" rel=\"noopener noreferrer\"\u003eGabi Stanovsky\u003c/a\u003e","\u003ca href='http://www.cs.cmu.edu/~sswayamd/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eSwabha Swayamdipta\u003c/a\u003e","\u003ca href='http://www.cs.cmu.edu/~jessed/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eJesse Dodge\u003c/a\u003e","\u003ca href='http://homes.cs.washington.edu/~nasmith/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eNoah A. Smith\u003c/a\u003e"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"09980fdceb6f0763d23789d3185f905c","permalink":"/publication/sledgehammer/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/sledgehammer/","section":"publication","summary":"As NLP models become larger, executing a trained model requires significant computational resources incurring monetary and environmental costs. To better respect a given inference budget, we propose a modification to contextual representation fine-tuning which, during inference, allows for an early (and fast) 'exit' from neural network calculations for simple instances, and late (and accurate) exit for hard instances. To achieve this, we add classifiers to different layers of BERT and use their calibrated confidence scores to make early exit decisions. We test our proposed modification on five different datasets in two tasks: three text classification datasets and two natural language inference benchmarks. Our method presents a favorable speed/accuracy tradeoff in almost all cases, producing models which are up to five times faster than the state of the art, while preserving their accuracy. Our method also requires almost no additional training resources (in either time or parameters) compared to the baseline BERT model. Finally, our method alleviates the need for costly retraining of multiple models at different levels of efficiency; we allow users to control the inference speed/accuracy tradeoff using a single trained model, by setting a single variable at inference time. We publicly release our code.","tags":["greenai"],"title":"The Right Tool for the Job: Matching Model and Instance Complexities","type":"publication"},{"authors":["\u003ca href='http://www.cs.cmu.edu/~jessed/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eJesse Dodge\u003c/a\u003e","\u003ca href='http://gabrielilharco.com' target=\"_blank\" rel=\"noopener noreferrer\"\u003eGabriel Ilharco\u003c/a\u003e","\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='https://homes.cs.washington.edu/~ali/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eAli Farhadi\u003c/a\u003e","\u003ca href='https://homes.cs.washington.edu/~hannaneh/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eHannaneh Hajishirzi\u003c/a\u003e","\u003ca href='http://homes.cs.washington.edu/~nasmith/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eNoah A. Smith\u003c/a\u003e"],"categories":null,"content":"","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580515200,"objectID":"e20b7646b10e3955a01b8e92a2233a27","permalink":"/publication/finetune/","publishdate":"2020-02-01T00:00:00Z","relpermalink":"/publication/finetune/","section":"publication","summary":"Fine-tuning pretrained contextual word embedding models to supervised downstream tasks has become commonplace in natural language processing. This process, however, is often brittle: even with the same hyperparameter values, distinct random seeds can lead to substantially different results. To better understand this phenomenon, we experiment with four datasets from the GLUE benchmark, fine-tuning BERT hundreds of times on each while varying only the random seeds. We find substantial performance increases compared to previously reported results, and we quantify how the performance of the best-found model varies as a function of the number of fine-tuning trials. Further, we examine two factors influenced by the choice of random seed: weight initialization and training data order. We find that both contribute comparably to the variance of out-of-sample performance, and that some weight initializations perform well across all tasks explored. On small datasets, we observe that many fine-tuning trials diverge part of the way through training, and we offer best practices for practitioners to stop training less promising runs early. We publicly release all of our experimental data, including training and validation scores for 2,100 trials, to encourage further analysis of training dynamics during fine-tuning.","tags":["greenai"],"title":"Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping","type":"publication"},{"authors":["\u003cb\u003eRoy Schwartz\u003c/b\u003e*","\u003ca href='http://www.cs.cmu.edu/~jessed/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eJesse Dodge\u003c/a\u003e*","\u003ca href='http://homes.cs.washington.edu/~nasmith/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eNoah A. Smith\u003c/a\u003e","\u003ca href='http://allenai.org/team/orene/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eOren Etzioni\u003c/a\u003e"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"a34fddc456f6c9c067029157009f758d","permalink":"/publication/greenai/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/publication/greenai/","section":"publication","summary":"The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018 [2]. These computations have a surprisingly large carbon footprint [38]. Ironically, deep learning was inspired by the human brain, which is remarkably energy efficient. Moreover, the financial cost of the computations can make it difficult for academics, students, and researchers, in particular those from emerging economies, to engage in deep learning research.\nThis position paper advocates a practical solution by making efficiency an evaluation criterion for research alongside accuracy and related measures. In addition, we propose reporting the financial cost or 'price tag' of developing, training, and running models to provide baselines for the investigation of increasingly efficient methods. Our goal is to make AI both greener and more inclusive---enabling any inspired undergraduate with a laptop to write high-quality research papers. Green AI is an emerging focus at the Allen Institute for AI.","tags":["greenai"],"title":"Green AI","type":"publication"},{"authors":["\u003ca href='https://scholar.google.com/citations?user=K5nCPZwAAAAJ\u0026hl=en' target=\"_blank\" rel=\"noopener noreferrer\"\u003eMatthew E. Peters\u003c/a\u003e","\u003ca href='http://markneumann.xyz' target=\"_blank\" rel=\"noopener noreferrer\"\u003eMark Neumann\u003c/a\u003e","\u003ca href='https://rloganiv.github.io' target=\"_blank\" rel=\"noopener noreferrer\"\u003eRobert Logan\u003c/a\u003e","\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='https://scholar.google.com/citations?user=NobF_hEAAAAJ' target=\"_blank\" rel=\"noopener noreferrer\"\u003eVidur Joshi\u003c/a\u003e","\u003ca href='http://sameersingh.org' target=\"_blank\" rel=\"noopener noreferrer\"\u003eSameer Singh\u003c/a\u003e","\u003ca href='http://homes.cs.washington.edu/~nasmith/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eNoah A. Smith\u003c/a\u003e"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"5241c7fc13c8a5c9e0a46ab2ae255360","permalink":"/publication/kermit/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/kermit/","section":"publication","summary":"Contextual word representations, typically trained on unstructured, unlabeled text, do not contain any explicit grounding to real world entities and are often unable to remember facts about those entities. We propose a general method to embed multiple knowledge bases(KBs) into large scale models, and there by enhance their representations with structured, human-curated knowledge. For each KB, we first use an integrated entity linker to retrieve relevant entity embeddings, then update contextual word representations via a form of word-to-entity attention. In contrast to previous approaches, the entity linkers and self-supervised language modeling objective are jointly trained end-to-end in a multitask setting that combines a small amount of entity linking supervision with a large amount of raw text. After integrating WordNet and a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation. KnowBerts runtime is comparable to BERTsand it scales to large KBs.","tags":["word_representations"],"title":"Knowledge Enhanced Contextual Word Representations","type":"publication"},{"authors":["\u003ca href='http://homes.cs.washington.edu/~hapeng/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eHao Peng\u003c/a\u003e","\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='http://homes.cs.washington.edu/~nasmith/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eNoah A. Smith\u003c/a\u003e"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"2e5dcdc75a12ddd89f89063f1e760e1f","permalink":"/publication/palm/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/palm/","section":"publication","summary":"We present PaLM, a hybrid parser and neural language model. Building on an RNN language model, PaLM adds an attention layer over text spans in the left context. An unsupervised constituency parser can be derived from its attention weights, using a greedy decoding algorithm. We evaluate PaLM on language modeling, and empirically show that it outperforms strong baselines. If syntactic annotations are available, the attention component can be trained in a supervised manner, providing syntactically-informed representations of the context, and further improving language modeling performance.","tags":["understanding_models"],"title":"PaLM: A Hybrid Parser and Language Model","type":"publication"},{"authors":["\u003ca href='http://www.cs.cmu.edu/~jessed/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eJesse Dodge\u003c/a\u003e","\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='http://homes.cs.washington.edu/~hapeng/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eHao Peng\u003c/a\u003e","\u003ca href='http://homes.cs.washington.edu/~nasmith/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eNoah A. Smith\u003c/a\u003e"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"bc1eea8e54580e8d974265f69ea149db","permalink":"/publication/sparsifying/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/sparsifying/","section":"publication","summary":"Neural models for NLP typically use large numbers of parameters to reach state-of-the- art performance, which can lead to excessive memory usage and increased runtime. We present a structure learning method for learning sparse, parameter-efficient NLP models. Our method applies group lasso to rational RNNs (Peng et al., 2018), a family of models that is closely connected to weighted finite-state automata (WFSAs). We take advantage of rational RNNs natural grouping of the weights, so the group lasso penalty directly removes WFSA states, substantially reducing the number of parameters in the model. Our experiments on a number of sentiment analysis datasets, using both GloVe and BERT embeddings, show that our approach learns neural structures which have fewer parameters without sacrificing performance relative to parameter-rich baselines. Our method also highlights the interpretable properties of rational RNNs. We show that sparsifying such models makes them easier to visualize, and we present models that rely exclusively on as few as three WFSAs after pruning more than 90% of the weights. We publicly release our code.","tags":["greenai","understanding_models"],"title":"RNN Architecture Learning with Sparse Regularization ","type":"publication"},{"authors":["\u003ca href='http://www.cs.cmu.edu/~jessed/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eJesse Dodge\u003c/a\u003e","\u003ca href='https://suchin.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eSuchin Gururangan\u003c/a\u003e","\u003ca href='http://web.stanford.edu/~dcard/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eDallas Card\u003c/a\u003e","\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='http://homes.cs.washington.edu/~nasmith/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eNoah A. Smith\u003c/a\u003e"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"37be21390a243324f4cc1fb45554898a","permalink":"/publication/showyourwork/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/showyourwork/","section":"publication","summary":"Research in natural language processing proceeds, in part, by demonstrating that new models achieve superior performance (e.g., accuracy) on held-out test data, compared to previous results. In this paper, we demonstrate that test-set performance scores alone are insufficient for drawing accurate conclusions about which model performs best. We argue for reporting additional details, especially performance on validation data obtained during model development. We present a novel technique for doing so: expected validation performance of the best-found model as a function of computation budget (i.e., the number of hyperparameter search trials or the overall training time). Using our approach, we find multiple recent model comparisons where authors would have reached a different conclusion if they had used more (or less) computation. Our approach also allows us to estimate the amount of computation required to obtain a given accuracy; applying it to several recently published results yields massive variation across papers, from hours to weeks. We conclude with a set of best practices for reporting experimental results which allow for robust future comparisons, and provide code to allow researchers to use our technique.","tags":["greenai"],"title":"Show Your Work: Improved Reporting of Experimental Results ","type":"publication"},{"authors":["\u003ca href='https://cs.stanford.edu/~nfliu/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eNelson F. Liu\u003c/a\u003e","\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='http://homes.cs.washington.edu/~nasmith/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eNoah A. Smith\u003c/a\u003e"],"categories":null,"content":"","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554076800,"objectID":"458917737964753b4534a68d01b748ad","permalink":"/publication/inoculation/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/publication/inoculation/","section":"publication","summary":"Several datasets have recently been constructed to expose brittleness in models trained on existing benchmarks. While model performance on these challenge datasets is significantly lower compared to the original benchmark, it is unclear what particular weaknesses they reveal. For example, a challenge dataset may be difficult because it targets phenomena that current models cannot capture, or because it simply exploits blind spots in a model's specific training set. We introduce inoculation by fine-tuning, a new analysis method for studying challenge datasets by exposing models (the metaphorical patient) to a small amount of data from the challenge dataset (a metaphorical pathogen) and assessing how well they can adapt. We apply our method to analyze the NLI 'stress tests' (Naik et al., 2018) and the Adversarial SQuAD dataset (Jia and Liang,2017). We show that after slight exposure, some of these datasets are no longer challenging, while others remain difficult. Our results indicate that failures on challenge datasets may lead to very different conclusions about models, training datasets, and the challenge datasets themselves.","tags":["improved_evaluation"],"title":"Inoculation by Fine-Tuning: A Method for Analyzing Challenge Datasets","type":"publication"},{"authors":null,"categories":null,"content":"","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"2080fb38cd10bc24f698ab039e3694bc","permalink":"/talk/towards_interpretable/","publishdate":"2018-12-01T00:00:00Z","relpermalink":"/talk/towards_interpretable/","section":"talk","summary":"Tel Aviv University; Computer Science and Electrical Engineering Colloquia\nWeizmann Institute; Machine Learning Seminar\nThe Hebrew University; Computer Science Colloquium\nTechnion; Computer Science, Electrical Engineering, and Industrial Engineering Colloquia","tags":null,"title":"Towards Interpretable Deep Learning for Natural Language Processing","type":"talk"},{"authors":["\u003ca href='http://homes.cs.washington.edu/~hapeng/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eHao Peng\u003c/a\u003e","\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='http://samthomson.com/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eSam Thomson\u003c/a\u003e","\u003ca href='http://homes.cs.washington.edu/~nasmith/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eNoah A. Smith\u003c/a\u003e"],"categories":null,"content":"","date":1533081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533081600,"objectID":"c037b7b164ba30b860de47f247a01c0c","permalink":"/publication/rr/","publishdate":"2018-10-31T00:00:00Z","relpermalink":"/publication/rr/","section":"publication","summary":"Despite the tremendous empirical success of neural models in natural language processing, many of them lack the strong intuitions that accompany classical machine learning approaches. Recently, connections have been shown between convolutional neural networks (CNNs) and weighted finite state automata (WFSAs), leading to new interpretations and insights. In this work, we show that some recurrent neural networks also share this connection to WFSAs. We characterize this connection formally, defining rational recurrences to be recurrent hidden state update functions that can be written as the Forward calculation of a finite set of WFSAs. We show that several recent neural models use rational recurrences. Our analysis provides a fresh view of these models and facilitates devising new neural architectures that draw inspiration from WFSAs. We present one such model, which performs better than two recent baselines on language modeling and text classification. Our results demonstrate that transferring intuitions from classical models like WFSAs can be an effective approach to designing and understanding neural models.","tags":["understanding_models"],"title":"Rational Recurrences","type":"publication"},{"authors":["\u003ca href='http://rowanzellers.com/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eRowan Zellers\u003c/a\u003e","\u003ca href='http://yonatanbisk.com/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eYonatan Bisk\u003c/a\u003e","\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='http://homes.cs.washington.edu/~yejin/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eYejin Choi\u003c/a\u003e"],"categories":null,"content":"","date":1533081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533081600,"objectID":"eb52c057620dbb0cd289f1918dce57c4","permalink":"/publication/swag/","publishdate":"2018-10-31T00:00:00Z","relpermalink":"/publication/swag/","section":"publication","summary":"Given a partial description like 'she opened the hood of the car', humans can reason about the situation and anticipate what might come next ('then, she examined the engine'). In this paper, we introduce the task of grounded commonsense inference, unifying natural language inference and commonsense reasoning.\nWe present SWAG, a new dataset with 113k multiple choice questions about a rich spectrum of grounded situations. To address the recurring challenges of the annotation artifacts and human biases found in many existing datasets, we propose Adversarial Filtering (AF), a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers, and using them to filter the data. To account for the aggressive adversarial filtering, we use state-of-the-art language models to massively oversample a diverse set of potential counterfactuals. Empirical results demonstrate that while humans can solve the resulting inference problems with high accuracy (88%), various competitive models struggle on our task. We provide comprehensive analysis that indicates significant opportunities for future research.","tags":["improved_evaluation"],"title":"SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference","type":"publication"},{"authors":["\u003ca href='https://cs.stanford.edu/~nfliu/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eNelson F. Liu\u003c/a\u003e","\u003ca href='https://levyomer.wordpress.com/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eOmer Levy\u003c/a\u003e","\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='https://chenhaot.com/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eChenhao Tan\u003c/a\u003e","\u003ca href='http://homes.cs.washington.edu/~nasmith/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eNoah A. Smith\u003c/a\u003e"],"categories":null,"content":"","date":1525132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525132800,"objectID":"6f1cac49532ac4818369595c415f0b2d","permalink":"/publication/lstm/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/publication/lstm/","section":"publication","summary":"While recurrent neural networks have found success in a variety of natural language processing applications, they are general models of sequential data. We investigate how the properties of natural language data affect an LSTMs ability to learn a nonlinguistic task: recalling elements from its input. We find that models trained on natural language data are able to recall tokens from much longer sequences than models trained on non-language sequential data. Furthermore, we show that the LSTM learns to solve the memorization task by explicitly using a subset of its neurons to count timesteps in the input. We hypothesize that the patterns and structure in natural language data enable LSTMs to learn by providing approximate ways of reducing loss, but understanding the effect of different training data on the learnability of LSTMs remains an open question.","tags":null,"title":"LSTMs Exploit Linguistic Attributes of Data","type":"publication"},{"authors":["\u003cb\u003eRoy Schwartz\u003c/b\u003e*","\u003ca href='http://samthomson.com/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eSam Thomson\u003c/a\u003e*","\u003ca href='http://homes.cs.washington.edu/~nasmith/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eNoah A. Smith\u003c/a\u003e"],"categories":null,"content":"","date":1525132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525132800,"objectID":"b29b498a1f0f35b3881ba70697daa94e","permalink":"/publication/sopa/","publishdate":"2018-07-15T00:00:00Z","relpermalink":"/publication/sopa/","section":"publication","summary":"Recurrent and convolutional neural networks comprise two distinct families of models that have proven to be useful for encoding natural language utterances. In this paper we present SoPa, a new model that aims to bridge these two approaches. SoPa combines neural representation learning with weighted finite-state automata (WFSAs) to learn a soft version of traditional surface patterns. We show that SoPa is an extension of a one-layer CNN, and that such CNNs are equivalent to a restricted version of SoPa, and accordingly, to a restricted form of WFSA. Empirically, on three text classification tasks, SoPa is comparable or better than both a BiLSTM (RNN) baseline and a CNN baseline, and is particularly useful in small data settings.","tags":["understanding_models"],"title":"SoPa: Bridging CNNs, RNNs, and Weighted Finite-State Machines","type":"publication"},{"authors":["\u003ca href='http://www.cs.cmu.edu/~dongyeok/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eDongyeop Kang\u003c/a\u003e","\u003ca href='https://wammar.github.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eWaleed Ammar\u003c/a\u003e","\u003ca href='http://allenai.org/team/bhavanad/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eBhavana Dalvi\u003c/a\u003e","Madeleine van Zuylen","Sebastian Kohlmeier","\u003ca href='http://www.cs.cmu.edu/~hovy/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eEduard Hovy\u003c/a\u003e","\u003cb\u003eRoy Schwartz\u003c/b\u003e"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522540800,"objectID":"e432881c7bb49b824679cd87674d65e8","permalink":"/publication/peeread/","publishdate":"2018-06-02T00:00:00Z","relpermalink":"/publication/peeread/","section":"publication","summary":"Peer reviewing is a central component in the scientific publishing process. We present the first public dataset of scientific peer reviews available for research purposes (PeerRead v1) providing an opportunity to study this important artifact. The dataset consists of 14K paper drafts and the corresponding accept/reject decisions in top-tier venues including ACL, NIPS and ICLR.  The dataset also includes 8.7K textual peer reviews written by experts for a subset of the papers. We describe the data collection process and report interesting observedphenomena in the peer reviews. We also propose two novel NLP tasks based on this dataset and provide simple baseline models. In the first task, we show that simple models can predict whether a paper is accepted with up to 21% error reduction compared to the majority baseline. In the second task, we predict the numerical scores of review aspects and show that simple models can outperform the mean baseline for aspects with high variance such as 'originality' and 'impact'.","tags":null,"title":"A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications","type":"publication"},{"authors":["\u003ca href='https://suchin.io/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eSuchin Gururangan\u003c/a\u003e*","\u003ca href='http://www.cs.cmu.edu/~sswayamd/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eSwabha Swayamdipta\u003c/a\u003e*","\u003ca href='https://levyomer.wordpress.com/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eOmer Levy\u003c/a\u003e","\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='https://www.nyu.edu/projects/bowman/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eSam Bowman\u003c/a\u003e","\u003ca href='http://homes.cs.washington.edu/~nasmith/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eNoah A. Smith\u003c/a\u003e"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522540800,"objectID":"0e1a342b11782311ddc5e4b260ff7a88","permalink":"/publication/artifacts/","publishdate":"2018-06-02T00:00:00Z","relpermalink":"/publication/artifacts/","section":"publication","summary":"Large-scale datasets for natural language inference are created by presenting crowd workers with a sentence (premise), and asking them to generate three new sentences (hypotheses) that it entails, contradicts, or is logically neutral with respect to. We show that, in a significant portion of such data, this protocol leaves clues that make it possible to identify the label by looking only at the hypothesis, without observing the premise. Specifically, we show that a simple text categorization model can correctly classify the hypothesis alone in about 67% of SNLI (Bowman et al., 2015) and 53% of MultiNLI (Williams et al., 2018). Our analysis reveals that specific linguistic phenomena such as negation and vagueness are highly correlated with certain inference classes. Our findings suggest that the success of natural language inference models to date has been overestimated, and that the task remains a hard open problem.","tags":["improved_evaluation"],"title":"Annotation Artifacts in Natural Language Inference Data","type":"publication"},{"authors":null,"categories":null,"content":"","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512086400,"objectID":"dd3f0ca395d5bb85ba9ad24f4215d910","permalink":"/talk/inductive_bias/","publishdate":"2017-12-01T00:00:00Z","relpermalink":"/talk/inductive_bias/","section":"talk","summary":"Google Research Tel Aviv; Machine Learning Seminar","tags":null,"title":"Inductive Bias of Deep Networks through Language Patterns","type":"talk"},{"authors":["\u003ca href='https://sites.google.com/site/ivanvulic/' target='blank'\u003eIvan Vuli\u003c/a\u003e","\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='http://www.cs.huji.ac.il/~arir/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eAri Rappoport\u003c/a\u003e","\u003ca href='http://ie.technion.ac.il/~roiri/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eRoi Reichart\u003c/a\u003e","\u003ca href='http://www.cl.cam.ac.uk/~alk23/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eAnna Korhonen\u003c/a\u003e"],"categories":null,"content":"","date":1485907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1485907200,"objectID":"139d8ad21d31bbe1c3529101948656ab","permalink":"/publication/context_config/","publishdate":"2017-07-01T00:00:00Z","relpermalink":"/publication/context_config/","section":"publication","summary":"This paper is concerned with identifying contexts useful for training word representation models for different word classes such as adjectives (A), verbs (V), and nouns (N). We introduce a simple yet effective framework for an automatic selection of class-specific context configurations. We construct a context configuration space based on universal dependency relations between words, and efficiently search this space with an adapted beam search algorithm. In word similarity tasks for each word class, we show that our framework is both effective and efficient. Particularly, it improves the Spearman's rho correlation with human scores on SimLex-999 over the best previously proposed class-specific contexts by 6 (A), 6 (V) and 5 (N) rho points. With our selected context configurations, we train on only 14% (A), 26.2% (V), and 33.6% (N) of all dependency-based contexts, resulting in a reduced training time. Our results generalise: we show that the configurations our algorithm learns for one English training setup outperform previously proposed context types in another training setup for English. Moreover, basing the configuration space on universal dependencies, it is possible to transfer the learned configurations to German and Italian. We also demonstrate improved per-class results over other context types in these two languages.","tags":["word_representations"],"title":"Automatic selection of context configurations for improved (and fast) class-specific word representations","type":"publication"},{"authors":["\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='http://homes.cs.washington.edu/~msap/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eMaarten Sap\u003c/a\u003e","\u003ca href='http://www.ikonstas.net/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eYannis Konstas\u003c/a\u003e","Li Zilles","\u003ca href='http://homes.cs.washington.edu/~yejin/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eYejin Choi\u003c/a\u003e","\u003ca href='http://homes.cs.washington.edu/~nasmith/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eNoah A. Smith\u003c/a\u003e"],"categories":null,"content":"","date":1485907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1485907200,"objectID":"578928aec6c1656fa51b778566bdeabf","permalink":"/publication/language_constraint/","publishdate":"2017-07-01T00:00:00Z","relpermalink":"/publication/language_constraint/","section":"publication","summary":"A writer's style depends not just on personal traits but also on her intent and mental state. In this paper, we show how variants of the same writing task can lead to measurable differences in writing style. We present a case study based on the story cloze task (Mostafazadeh et al., 2016a), where annotators were assigned similar writing tasks with different constraints: (1) writing an entire story, (2) adding a story ending for a given story context, and (3) adding an incoherent ending to a story. We show that a simple linear classifier informed by stylistic features is able to successfully distinguish among the three cases, without even looking at the story context. In addition, combining our stylistic features with language model predictions reaches state of the art performance on the story cloze challenge. Our results demonstrate that different task framings can dramatically affect the way people write.","tags":["improved_evaluation"],"title":"The Effect of Different Writing Tasks on Linguistic Style: A Case Study of the ROC Story Cloze Task","type":"publication"},{"authors":["\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='http://homes.cs.washington.edu/~msap/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eMaarten Sap\u003c/a\u003e","\u003ca href='http://www.ikonstas.net/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eYannis Konstas\u003c/a\u003e","Li Zilles","\u003ca href='http://homes.cs.washington.edu/~yejin/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eYejin Choi\u003c/a\u003e","\u003ca href='http://homes.cs.washington.edu/~nasmith/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eNoah A. Smith\u003c/a\u003e"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"16f511c2c1e2d28c9173df867d8d4aa6","permalink":"/publication/lsdsem_uw_nlp/","publishdate":"2017-04-01T00:00:00Z","relpermalink":"/publication/lsdsem_uw_nlp/","section":"publication","summary":"This paper describes University of Washington NLP's submission for the Linking Models of Lexical, Sentential and Discourse-level Semantics (LSDSem 2017) shared task\u0026mdash;the Story Cloze Task. Our system is a linear classifier with a variety of features, including both the scores of a neural language model and style features. We report 75.2% accuracy on the task. A further discussion of our results can be found in Schwartz et al. (2017).","tags":null,"title":"Story Cloze Task: UW NLP System","type":"publication"},{"authors":["\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='http://ie.technion.ac.il/~roiri/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eRoi Reichart\u003c/a\u003e","\u003ca href='http://www.cs.huji.ac.il/~arir/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eAri Rappoport\u003c/a\u003e"],"categories":null,"content":"","date":1467331200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1467331200,"objectID":"38a984ae3fcee16dc7bee710c452d723","permalink":"/publication/sp_sg/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/sp_sg/","section":"publication","summary":"State-of-the-art word embeddings, which are often trained on bag-of-words (BOW) contexts, provide a high quality representation of aspects of the semantics of nouns. However, their quality decreases substantially for the task of verb similarity prediction. In this paper we show that using symmetric pattern contexts (SPs, e.g., ``X and Y'') improves word2vec verb similarity performance by up to 15% and is also instrumental in adjective similarity prediction. The unsupervised SP contexts are even superior to a variety of dependency contexts extracted using a supervised dependency parser. Moreover, we observe that SPs and dependency coordination contexts (Coor) capture a similar type of information, and demonstrate that Coor contexts are superior to other dependency contexts including the set of all dependency contexts, although they are still inferior to SPs. Finally, there are substantially fewer SP contexts compared to alternative representations, leading to a massive reduction in training time. On an 8G words corpus and a 32 core machine, the SP model trains in 11 minutes, compared to 5 and 11 hours with BOW and all dependency contexts, respectively.","tags":["word_representations"],"title":"Symmetric Patterns and Coordinations: Fast and Enhanced Representations of Verbs and Adjectives","type":"publication"},{"authors":["\u003cb\u003eRoy Schwartz\u003c/b\u003e"],"categories":null,"content":"","date":1464739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464739200,"objectID":"0f40cf545adf79432be77d9037b7794b","permalink":"/publication/thesis/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/thesis/","section":"publication","summary":"","tags":["word_representations"],"title":"Pattern-based methods for Improved Lexical Semantics and Word Embeddings","type":"publication"},{"authors":null,"categories":null,"content":"","date":1454284800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1454284800,"objectID":"efb082ee0a992cc8565a3b08a0d42f23","permalink":"/talk/pattern-based_solutions/","publishdate":"2016-02-01T00:00:00Z","relpermalink":"/talk/pattern-based_solutions/","section":"talk","summary":"Intel Inc. Yakum; Research talk\nUniversity of Pennsylvania; CLunch computational linguistics seminar\nJohns Hopkins University; NLP seminar\nUniversity of Washington; NLP seminar","tags":null,"title":"Pattern-based Solutions to Limitations of Leading Word Embeddings","type":"talk"},{"authors":null,"categories":null,"content":"","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"f248d7679f3df615b94f5411aef8dc1c","permalink":"/talk/word_similarity/","publishdate":"2015-09-01T00:00:00Z","relpermalink":"/talk/word_similarity/","section":"talk","summary":"IBM Research Tel Aviv; Machine Learning and Data Mining Group Seminar","tags":null,"title":"Word Similarity via Symmetric Patterns","type":"talk"},{"authors":["Dana Rubinstein","Effi Levi","\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='http://www.cs.huji.ac.il/~arir/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eAri Rappoport\u003c/a\u003e"],"categories":null,"content":"","date":1435708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435708800,"objectID":"12f578dd53839cc61ab7c9b18ee14675","permalink":"/publication/semantics_from_text/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/semantics_from_text/","section":"publication","summary":"In recent years, distributional models (DMs) have shown great success in representing lexical semantics. In this work we show that the extent to which DMs represent semantic knowledge is highly dependent on the type of knowledge. We pose the task of predicting properties of concrete nouns in a supervised setting, and compare between learning taxonomic properties (e.g., animacy) and attributive properties (e.g., size, color). We employ four state-of-the-art DMs as sources of feature representation for this task, and show that they all yield poor results when tested on attributive properties, achieving no more than an average F-score of 0.37 in the binary property prediction task, compared to 0.73 on taxonomic properties. Our results suggest that the distributional hypothesis may not be equally applicable to all types of semantic information.","tags":["word_representations"],"title":"How Well Do Distributional Models Capture Different Types of Semantic Knowledge?","type":"publication"},{"authors":["\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='http://ie.technion.ac.il/~roiri/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eRoi Reichart\u003c/a\u003e","\u003ca href='http://www.cs.huji.ac.il/~arir/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eAri Rappoport\u003c/a\u003e"],"categories":null,"content":"","date":1435708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435708800,"objectID":"8e3bd2cdf0982208a5caab7cfdf9f0bf","permalink":"/publication/sp_embeddings/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/sp_embeddings/","section":"publication","summary":"We present a novel word level vector representation based on symmetric patterns (SPs). For this aim we automatically acquire SPs (e.g., 'X and Y') from a large corpus of plain text, and generate vectors where each coordinate represents the co-occurrence in SPs of the represented word with another word of the vocabulary. Our representation has three advantages over existing alternatives: First, being based on symmetric word relationships, it is highly suitable for word similarity prediction. Particularly, on the SimLex999 word similarity dataset, our model achieves a Spearman's \u0026#961; score of 0.517, compared to 0.462 of the state-of-the-art word2vec model. Interestingly, our model performs exceptionally well on verbs, outperforming state-of-the-art baselines by 20.2\u0026#x2012;41.5%. Second, pattern features can be adapted to the needs of a target NLP application. For example, we show that we can easily control whether the embeddings derived from SPs deem antonym pairs (e.g. (big,small)) as similar or dissimilar, an important distinction for tasks such as word classification and sentiment analysis. Finally, we show that a simple combination of the word similarity scores generated by our method and by word2vec results in a superior predictive power over that of each individual model, scoring as high as 0.563 in Spearman's \u0026#961; on SimLex999. This emphasizes the differences between the signals captured by each of the models.","tags":["word_representations"],"title":"Symmetric Pattern Based Word Embeddings for Improved Word Similarity Prediction","type":"publication"},{"authors":null,"categories":null,"content":"","date":1422748800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1422748800,"objectID":"0fc9851e88f6a985877336c58da6b7cb","permalink":"/talk/semantic_knowledge/","publishdate":"2015-02-01T00:00:00Z","relpermalink":"/talk/semantic_knowledge/","section":"talk","summary":"The Catalonia-Israel Symposium on Lexical Semantics and Grammatical Structure","tags":null,"title":"Semantic Knowledge Acquisition using Frequency Based Patterns","type":"talk"},{"authors":null,"categories":null,"content":"","date":1417392000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1417392000,"objectID":"efd0c4db9e70a844e3002aa090a475ea","permalink":"/talk/acquiring_semantic/","publishdate":"2014-12-01T00:00:00Z","relpermalink":"/talk/acquiring_semantic/","section":"talk","summary":"Hebrew University of Jerusalem; CS Learning Semanir","tags":null,"title":"Acquiring Semantic Knowledge using Patterns","type":"talk"},{"authors":["\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='http://ie.technion.ac.il/~roiri/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eRoi Reichart\u003c/a\u003e","\u003ca href='http://www.cs.huji.ac.il/~arir/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eAri Rappoport\u003c/a\u003e"],"categories":null,"content":"","date":1404172800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1404172800,"objectID":"d9563020ab84f7e55132ec38685a3b71","permalink":"/publication/coarse_grained/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/coarse_grained/","section":"publication","summary":"Classifying nouns into semantic categories (e.g., animals, food) is an important line of research in both cognitive science and natural language processing. We present a minimally supervised model for noun classification, which uses symmetric patterns (e.g., 'X and Y') and an iterative variant of the k-Nearest Neighbors algorithm. Unlike most previous works, we do not use a predefined set of symmetric patterns, but extract them automatically from plain text, in an unsupervised manner. We experiment with four semantic categories and show that symmetric patterns constitute much better classification features compared to leading word embedding methods. We further demonstrate that our simple k-Nearest Neighbors algorithm outperforms two state-of-the-art label propagation alternatives for this task. In experiments, our model obtains 82%-94% accuracy using as few as four labeled examples per category, emphasizing the effectiveness of simple search and representation techniques for this task.","tags":null,"title":"Minimally Supervised Classification to Semantic Categories using Automatically Acquired Symmetric Patterns","type":"publication"},{"authors":null,"categories":null,"content":"","date":1398902400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1398902400,"objectID":"948bd5778ebb1e6f2fc974738736383b","permalink":"/talk/identifying_authorships/","publishdate":"2014-05-01T00:00:00Z","relpermalink":"/talk/identifying_authorships/","section":"talk","summary":"Intel Inc. Haifa; ICRI-CI Retreat","tags":null,"title":"Identifying Authorships of very Short Texts using Flexible Patterns","type":"talk"},{"authors":null,"categories":null,"content":"","date":1380585600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1380585600,"objectID":"3047206d7b79a10b7743f23d4bcdd619","permalink":"/talk/semantic_representation/","publishdate":"2013-10-01T00:00:00Z","relpermalink":"/talk/semantic_representation/","section":"talk","summary":"Berkeley; Natural Language Processing Group Seminar\nStanford; Natural Language Processing Group Seminar\nUSC Information Sciences Institute; Natural Language Processing Group Seminar\nTwitter Inc.; Technological Talk\nIntel Inc. Santa Clara; Natural Language Processing Group Seminar\nIBM Research Tel Aviv; Machine Learning and Data Mining Group Seminar","tags":null,"title":"Semantic Representation using Flexible Patterns","type":"talk"},{"authors":["\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='http://www.ise.bgu.ac.il/OrenTsur/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eOren Tsur\u003c/a\u003e","\u003ca href='http://www.cs.huji.ac.il/~arir/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eAri Rappoport\u003c/a\u003e","\u003ca href='http://u.cs.biu.ac.il/~koppel/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eMoshe Koppel\u003c/a\u003e"],"categories":null,"content":"","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"e61824aa977036cb12cae2e0cc68ad76","permalink":"/publication/twitter_authorship/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/twitter_authorship/","section":"publication","summary":"Work on authorship attribution has traditionally focused on long texts. In this work, we tackle the question of whether the author of a very short text can be successfully identified. We use Twitter as an experimental testbed. We introduce the concept of an author's unique 'signature', and show that such signatures are typical of many authors when writing very short texts. We also present a new authorship attribution feature ('flexible patterns') and demonstrate a significant improvement over our baselines. Our results show that the author of a single tweet can be identified with good accuracy in an array of flavors of the authorship attribution task.","tags":null,"title":"Authorship Attribution of Micro-Messages","type":"publication"},{"authors":["\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='http://www.cs.huji.ac.il/~oabend/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eOmri Abend\u003c/a\u003e","\u003ca href='http://www.cs.huji.ac.il/~arir/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eAri Rappoport\u003c/a\u003e"],"categories":null,"content":"","date":1341100800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1341100800,"objectID":"9b25dda30b8c75cbba3971cccd528e1d","permalink":"/publication/learnable/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/learnable/","section":"publication","summary":"There is often more than one way to represent syntactic structures, even within a given formalism. Selecting one representation over another may affect parsing performance. Therefore, selecting between alternative syntactic representations (henceforth, syntactic selection) is an essential step in designing an annotation scheme. We present a methodology for syntactic selection and apply it to six central dependency structures. Our methodology compares pairs of annotation schemes that differ in the annotation of a single structure. It selects the more learnable scheme, namely the one that can be better learned using statistical parsers. We find that in three of the structures, one annotation is unequivocally better than the alternatives. Our results are consistent over various settings involving five parsers and two definitions of learnability. Furthermore, we show that the learnability gains incurred by our selections are both considerable (error reductions of up to 19.8%) and additive. The contribution of this work is in demonstrating that syntactic selection has a substantial and predictable effect on parsing performance, and showing that this effect can be effectively used in designing syntactic annotation schemes.","tags":null,"title":"Learnability-based Syntactic Annotation Design","type":"publication"},{"authors":["\u003cb\u003eRoy Schwartz\u003c/b\u003e","\u003ca href='http://www.cs.huji.ac.il/~oabend/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eOmri Abend\u003c/a\u003e","\u003ca href='http://ie.technion.ac.il/~roiri/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eRoi Reichart\u003c/a\u003e","\u003ca href='http://www.cs.huji.ac.il/~arir/' target=\"_blank\" rel=\"noopener noreferrer\"\u003eAri Rappoport\u003c/a\u003e"],"categories":null,"content":"","date":1309478400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1309478400,"objectID":"130d4fc50cfb78d36946926efa60f0bd","permalink":"/publication/ned/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/ned/","section":"publication","summary":"Dependency parsing is a central NLP task. In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations. We show that for three leading unsupervised parsers (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures. These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon.","tags":["improved_evaluation"],"title":"Neutralizing Linguistically Problematic Annotations in Unsupervised Dependency Parsing Evaluation","type":"publication"},{"authors":null,"categories":null,"content":"","date":1306886400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1306886400,"objectID":"12b9b21ce1c6bfc2bb3d349959d3e8d7","permalink":"/talk/neutralizing_linguistically/","publishdate":"2011-06-01T00:00:00Z","relpermalink":"/talk/neutralizing_linguistically/","section":"talk","summary":"The Israeli Seminar on Computational Linguistics (ISCOL 2011)","tags":null,"title":"Neutralizing Linguistically Problematic Annotations in Unsupervised Dependency Parsing Evaluation","type":"talk"},{"authors":null,"categories":null,"content":"       ,     .     (year1advisor@cs.huji.ac.il)     (  ).    ,            .     ,     ,     :\n    .           ( )            ,         .          (   ).       .      ,   .       ,        .      .               .          ,     .            . ,        1   1;        ;  IML   2  .    iml -communications    - (  -*).        .            .       - ,    (year1advisor@cs.huji.ac.il).            C,  5,  c503.       .          .           ,         .     ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6cfe504ee534801d5ba28035ee8caacb","permalink":"/advisory/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/advisory/","section":"","summary":"       ,     .     (year1advisor@cs.huji.ac.il)     (  ).    ,            .","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"        .        , , , ,           .      ,       .     (roy.schwartz1@mail.huji.ac.il)  .  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0a4ba8b5ca1d988c2465a7d545ab37f3","permalink":"/diversity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/diversity/","section":"","summary":"        .        , , , ,           .","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"                (Natural Language Processing).           - , , ,   .     ,       ,   ;      ,      .         ,  ,        ,              .   -             .      (2016-2019)    (2019-2020)           ,       .        2016           .  2011         ,      .  ,            .       . -2004-2005        .       -Ynet        ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d68fe639af49c099f90764d841b95c4a","permalink":"/hebrew/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/hebrew/","section":"","summary":"                (Natural Language Processing).           - , , ,   .","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"Many of the lab's papers, e.g., Green AI, Measuring the Carbon Intensity of AI in Cloud instances, Show Your Work, and SWAG, have been covered by various news sources. For Hebrew materials, see our Hebrew page. Below are podcast and recent interviews with Roy.  Interview with Roy on Bloomberg on AI Carbon footprint, March 2023 Interview with Roy on the Data Exchange podcast about Efficient NLP, December 2022 Interview with Roy on the Practical AI podcast about Green AI, March 2021 Interview with Roy on the NLP Highlights podcast about annotation artifacts, July 2017  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"aacee624b497bf6ad3be2d87231522d5","permalink":"/media/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/media/","section":"","summary":"Many of the lab's papers, e.g., Green AI, Measuring the Carbon Intensity of AI in Cloud instances, Show Your Work, and SWAG, have been covered by various news sources. For Hebrew materials, see our Hebrew page.","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0985a49259cc27ed0412019929cef063","permalink":"/project/biases/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/biases/","section":"project","summary":"We analyze the datasets on which NLP models are trained. Looking carefully into these datasets, we uncover limitations and biases in the data collection process as well as the evaluation process. Our findings indicate that the recent success of neural models on many NLP tasks has been overestimated, and pave the way for the development of more reliable methods of evaluation.","tags":null,"title":"Biases in Datasets","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d686074f00ba84d4757f75524bbc0c89","permalink":"/project/green_nlp/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/green_nlp/","section":"project","summary":"The computations required for deep learning research have been doubling every few months. These computations have a surprisingly large carbon footprint. Moreover, the financial cost of the computations can make it difficult for academics, students, and researchers, in particular those from emerging economies, to engage in deep learning research. Our lab studies tools to make NLP technology more efficient, and to enhance the reporting of computational budgets.","tags":null,"title":"Green NLP","type":"project"},{"authors":null,"categories":null,"content":"We are looking for curious and motivated students and postdocs from diverse backgrounds, who are interested in natural language processing. If you are looking for a postdoc, Ph.D., Masters, undergrad project or summer internship, please email Roy and attach a 2-page CV.\nStudents, please provide your undergrad and masters grade transcripts if available.\nPh.D. and postdoc candidates, please also briefly describe your research interests. ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fba6c149775da15c6a5b2adae893dd1c","permalink":"/joinus/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/joinus/","section":"","summary":"We are looking for curious and motivated students and postdocs from diverse backgrounds, who are interested in natural language processing. If you are looking for a postdoc, Ph.D., Masters, undergrad project or summer internship, please email Roy and attach a 2-page CV.","tags":null,"title":"Join Us!","type":"page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8c60b6875141d1b008d954773c22f1e8","permalink":"/project/multimodal/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/multimodal/","section":"project","summary":"Humans learn about the world using input from multiple modalities. Machines can also leverage other modalities in order to improve their textual understanding. Our lab studies methods for combining textual information with data from images, sounds, videos and others, with the goal of making them more robust and allowing them to generalize better.","tags":null,"title":"Multimodality","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"People","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3081f18a5769f6c057e102ad89f9a122","permalink":"/project/understanding_nlp/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/understanding_nlp/","section":"project","summary":"In recent years, deep learning became the leading machine learning technology in NLP. Despite its wide adoption in NLP, the theory of deep learning lags behind its empirical success, as many engineered systems are in commercial use without a solid scientific basis for their operation. Our research aims to bridge the gap between theory and practice. We devise mathematical theories that link deep neural models to classical NLP models, such as weighted finite-state automata.","tags":null,"title":"Understanding NLP","type":"project"}]