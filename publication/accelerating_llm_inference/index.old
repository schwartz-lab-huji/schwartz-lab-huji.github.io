---
title: "Accelerating LLM Inference with Lossless Speculative Decoding Algorithms for Heterogeneous Vocabularies"

authors:
- Nadav Timor
- Jonathan Mamou
- Daniel Korat
- Moshe Berchansky
- Oren Pereg
- Gaurav Jain
- me
- Moshe Wasserblat
- David Harel

publication_types: ["3"]
Date: 2025-02-16
publishDate: 2025-02-1600:00:00Z
publication: arXiv:2502.05202

abstract: "Accelerating the inference of large language models (LLMs) is a critical challenge in generative AI. Speculative decoding (SD) methods offer substantial efficiency gains by generating multiple tokens using a single target forward pass. However, existing SD approaches require the drafter and target models to share the same vocabulary, thus limiting the pool of possible drafters, often necessitating the training of a drafter from scratch. We present three new SD methods that remove this shared-vocabulary constraint. All three methods preserve the target distribution (i.e., they are lossless) and work with off-the-shelf models without requiring additional training or modifications. Empirically, on summarization, programming, and long-context tasks, our algorithms achieve significant speedups over standard autoregressive decoding. By enabling any off-the-shelf model to serve as drafter and requiring no retraining, this work substantially broadens the applicability of the SD framework in practice."

tags:
- greenai

links:
url_pdf: "https://www.arxiv.org/abs/2502.05202"
---
